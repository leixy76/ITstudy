### éƒ¨ç½²è®°å½•

[Prompt Templates | ğŸ¦œï¸ğŸ”— LangChain](https://python.langchain.com/docs/concepts/prompt_templates/)

åç«¯æ¥å£å¯åŠ¨ï¼š

python3.10 -m api.main

å‰ç«¯é¡µé¢å¯åŠ¨ï¼š

cd frontend
python3.10 main.py


åå¸¸ 1ï¼šè®¾äº† assist page é‡Œ RAG çš„ç³»ç»Ÿæç¤ºè¯ï¼Œæ•ˆæœåè€Œä¸å¥½ã€‚

system_template = '''
You are a helpful AI assistant. Reply in Simplified Chinese. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say you don't know. DO NOT try to make up an answer. If the question is not related to the context, politely respond that you are tuned to only answer questions that are related to the context. {context}  Question: {question} Helpful answer:
'''

æ³¨é‡Šäº†ï¼Œç›´æ¥ç”¨ä¸‹é¢çš„ï¼š

system_template = "You are a helpful AI assistant.... Reply in Simplified Chinese."

åå¸¸ 1ï¼šæŠŠç›¸å…³æ€§ä½çš„æ£€ç´¢æ•°æ®è¿‡æ»¤æ‰ï¼Œæ•ˆæœåè€Œä¸å¥½ã€‚

```py
                # æˆªå–å‰ similarity_top_k ä¸ªèŠ‚ç‚¹
                return sorted_nodes[:self.similarity_top_k]
``

å‰”é™¤çš„ä»£ç ï¼š

```py
                # æˆªå–å‰ similarity_top_k ä¸ªèŠ‚ç‚¹
                top_k_nodes = sorted_nodes[:self.similarity_top_k]
                
                # å‰”é™¤scoreå€¼ä½äº0.4çš„èŠ‚ç‚¹
                filtered_nodes = [node for node in top_k_nodes if node.score >= 0.4]
                
                return filtered_nodes
``



https://x.com/op7418/status/1890332159639998907

DeepSeek äº 2025+02-14 å‘æ–‡çš„ä¿¡æ¯ï¼š

Excited to see everyoneâ€™s enthusiasm for deploying DeepSeek-R1! Here are our recommended settings for the best experience:

â€¢ No system prompt
â€¢ Temperature: 0.6
â€¢ Official prompts for search & file upload:  http://bit.ly/4hyH8np
â€¢ Guidelines to mitigate model bypass thinking: http://bit.ly/4gJrhkF

The official DeepSeek deployment runs the same model as the open-source versionâ€”enjoy the full DeepSeek-R1 experience!


**response with \"\<think\>\n\" at the beginning of every output.**

system_template = "åœ¨æ¯æ¬¡è¾“å‡ºçš„å¼€å§‹éƒ½ä½¿ç”¨\"<think>\n\"ä½œä¸ºèµ·å§‹ã€‚"





ç¼“è§£ R1 æ¨¡å‹ç»•è¿‡æ€è€ƒçš„æ–¹æ³•ï¼š

æˆ‘ä»¬è§‚å¯Ÿåˆ° DeepSeek-R1 ç³»åˆ—æ¨¡å‹åœ¨å›åº”æŸäº›æŸ¥è¯¢æ—¶å€¾å‘äºç»•è¿‡æ€ç»´æ¨¡å¼ï¼ˆå³è¾“å‡º"<think>\n\n</think>"ï¼‰ï¼Œè¿™å¯èƒ½ä¼šå¯¹æ¨¡å‹çš„è¡¨ç°äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ 

ä¸ºäº†ç¡®ä¿æ¨¡å‹è¿›è¡Œå……åˆ†çš„æ¨ç†ï¼Œæˆ‘ä»¬å»ºè®®å¼ºåˆ¶æ¨¡å‹åœ¨æ¯æ¬¡è¾“å‡ºçš„å¼€å§‹éƒ½ä½¿ç”¨"<think>\n"ä½œä¸ºèµ·å§‹ã€‚


Deepseek R1 å®˜æ–¹æ–‡ä»¶ä¸Šä¼ æç¤ºè¯ï¼š

For file upload, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments. 
```
file_template = \
"""[file name]: {file_name}
[file content begin]
{file_content}
[file content end]
{question}"""
```


Deepseek R1 å®˜æ–¹æœç´¢æç¤ºè¯

  For Chinese query, we use the prompt:
```
search_answer_zh_template = \
'''# ä»¥ä¸‹å†…å®¹æ˜¯åŸºäºç”¨æˆ·å‘é€çš„æ¶ˆæ¯çš„æœç´¢ç»“æœ:
{search_results}
åœ¨æˆ‘ç»™ä½ çš„æœç´¢ç»“æœä¸­ï¼Œæ¯ä¸ªç»“æœéƒ½æ˜¯[webpage X begin]...[webpage X end]æ ¼å¼çš„ï¼ŒXä»£è¡¨æ¯ç¯‡æ–‡ç« çš„æ•°å­—ç´¢å¼•ã€‚è¯·åœ¨é€‚å½“çš„æƒ…å†µä¸‹åœ¨å¥å­æœ«å°¾å¼•ç”¨ä¸Šä¸‹æ–‡ã€‚è¯·æŒ‰ç…§å¼•ç”¨ç¼–å·[citation:X]çš„æ ¼å¼åœ¨ç­”æ¡ˆä¸­å¯¹åº”éƒ¨åˆ†å¼•ç”¨ä¸Šä¸‹æ–‡ã€‚å¦‚æœä¸€å¥è¯æºè‡ªå¤šä¸ªä¸Šä¸‹æ–‡ï¼Œè¯·åˆ—å‡ºæ‰€æœ‰ç›¸å…³çš„å¼•ç”¨ç¼–å·ï¼Œä¾‹å¦‚[citation:3][citation:5]ï¼Œåˆ‡è®°ä¸è¦å°†å¼•ç”¨é›†ä¸­åœ¨æœ€åè¿”å›å¼•ç”¨ç¼–å·ï¼Œè€Œæ˜¯åœ¨ç­”æ¡ˆå¯¹åº”éƒ¨åˆ†åˆ—å‡ºã€‚
åœ¨å›ç­”æ—¶ï¼Œè¯·æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š
- ä»Šå¤©æ˜¯{cur_date}ã€‚
- å¹¶éæœç´¢ç»“æœçš„æ‰€æœ‰å†…å®¹éƒ½ä¸ç”¨æˆ·çš„é—®é¢˜å¯†åˆ‡ç›¸å…³ï¼Œä½ éœ€è¦ç»“åˆé—®é¢˜ï¼Œå¯¹æœç´¢ç»“æœè¿›è¡Œç”„åˆ«ã€ç­›é€‰ã€‚
- å¯¹äºåˆ—ä¸¾ç±»çš„é—®é¢˜ï¼ˆå¦‚åˆ—ä¸¾æ‰€æœ‰èˆªç­ä¿¡æ¯ï¼‰ï¼Œå°½é‡å°†ç­”æ¡ˆæ§åˆ¶åœ¨10ä¸ªè¦ç‚¹ä»¥å†…ï¼Œå¹¶å‘Šè¯‰ç”¨æˆ·å¯ä»¥æŸ¥çœ‹æœç´¢æ¥æºã€è·å¾—å®Œæ•´ä¿¡æ¯ã€‚ä¼˜å…ˆæä¾›ä¿¡æ¯å®Œæ•´ã€æœ€ç›¸å…³çš„åˆ—ä¸¾é¡¹ï¼›å¦‚éå¿…è¦ï¼Œä¸è¦ä¸»åŠ¨å‘Šè¯‰ç”¨æˆ·æœç´¢ç»“æœæœªæä¾›çš„å†…å®¹ã€‚
- å¯¹äºåˆ›ä½œç±»çš„é—®é¢˜ï¼ˆå¦‚å†™è®ºæ–‡ï¼‰ï¼Œè¯·åŠ¡å¿…åœ¨æ­£æ–‡çš„æ®µè½ä¸­å¼•ç”¨å¯¹åº”çš„å‚è€ƒç¼–å·ï¼Œä¾‹å¦‚[citation:3][citation:5]ï¼Œä¸èƒ½åªåœ¨æ–‡ç« æœ«å°¾å¼•ç”¨ã€‚ä½ éœ€è¦è§£è¯»å¹¶æ¦‚æ‹¬ç”¨æˆ·çš„é¢˜ç›®è¦æ±‚ï¼Œé€‰æ‹©åˆé€‚çš„æ ¼å¼ï¼Œå……åˆ†åˆ©ç”¨æœç´¢ç»“æœå¹¶æŠ½å–é‡è¦ä¿¡æ¯ï¼Œç”Ÿæˆç¬¦åˆç”¨æˆ·è¦æ±‚ã€æå…·æ€æƒ³æ·±åº¦ã€å¯Œæœ‰åˆ›é€ åŠ›ä¸ä¸“ä¸šæ€§çš„ç­”æ¡ˆã€‚ä½ çš„åˆ›ä½œç¯‡å¹…éœ€è¦å°½å¯èƒ½å»¶é•¿ï¼Œå¯¹äºæ¯ä¸€ä¸ªè¦ç‚¹çš„è®ºè¿°è¦æ¨æµ‹ç”¨æˆ·çš„æ„å›¾ï¼Œç»™å‡ºå°½å¯èƒ½å¤šè§’åº¦çš„å›ç­”è¦ç‚¹ï¼Œä¸”åŠ¡å¿…ä¿¡æ¯é‡å¤§ã€è®ºè¿°è¯¦å°½ã€‚
- å¦‚æœå›ç­”å¾ˆé•¿ï¼Œè¯·å°½é‡ç»“æ„åŒ–ã€åˆ†æ®µè½æ€»ç»“ã€‚å¦‚æœéœ€è¦åˆ†ç‚¹ä½œç­”ï¼Œå°½é‡æ§åˆ¶åœ¨5ä¸ªç‚¹ä»¥å†…ï¼Œå¹¶åˆå¹¶ç›¸å…³çš„å†…å®¹ã€‚
- å¯¹äºå®¢è§‚ç±»çš„é—®ç­”ï¼Œå¦‚æœé—®é¢˜çš„ç­”æ¡ˆéå¸¸ç®€çŸ­ï¼Œå¯ä»¥é€‚å½“è¡¥å……ä¸€åˆ°ä¸¤å¥ç›¸å…³ä¿¡æ¯ï¼Œä»¥ä¸°å¯Œå†…å®¹ã€‚
- ä½ éœ€è¦æ ¹æ®ç”¨æˆ·è¦æ±‚å’Œå›ç­”å†…å®¹é€‰æ‹©åˆé€‚ã€ç¾è§‚çš„å›ç­”æ ¼å¼ï¼Œç¡®ä¿å¯è¯»æ€§å¼ºã€‚
- ä½ çš„å›ç­”åº”è¯¥ç»¼åˆå¤šä¸ªç›¸å…³ç½‘é¡µæ¥å›ç­”ï¼Œä¸èƒ½é‡å¤å¼•ç”¨ä¸€ä¸ªç½‘é¡µã€‚
- é™¤éç”¨æˆ·è¦æ±‚ï¼Œå¦åˆ™ä½ å›ç­”çš„è¯­è¨€éœ€è¦å’Œç”¨æˆ·æé—®çš„è¯­è¨€ä¿æŒä¸€è‡´ã€‚
\# ç”¨æˆ·æ¶ˆæ¯ä¸ºï¼š
{question}'''
```





#### 2025-02-24

1ã€å¢åŠ èŠå¤©å†å²è®°å½•ã€‚


å®ç°ä»¥ä¸‹åŠŸèƒ½ï¼š
```
1.æ¯æ¬¡å¯¹è¯æ—¶éƒ½ä¼šåŠ è½½å®Œæ•´çš„å†å²è®°å½•
2.å°†å†å²è®°å½•ä½œä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯å‘é€ç»™LLM
å°†æ–°çš„å¯¹è¯å†…å®¹è¿½åŠ åˆ°å†å²è®°å½•ä¸­å¹¶ä¿å­˜
è¿™æ ·å°±èƒ½å®ç°å¯¹è¯å†å²çš„è¿ç»­æ€§ï¼Œè®©LLMèƒ½å¤ŸåŸºäºä¹‹å‰çš„å¯¹è¯å†…å®¹è¿›è¡Œæ›´è¿è´¯çš„å›å¤ã€‚
```

ä»¥ä¸‹çš„å®ç°é€»è¾‘ä¾›å‚è€ƒï¼š
```
1. å®šä¹‰æ¶ˆæ¯ç±»å‹
é¦–å…ˆå®šä¹‰æ¶ˆæ¯ç±»å‹ï¼ŒåŒ…æ‹¬ç”¨æˆ·æ¶ˆæ¯å’ŒAIæ¶ˆæ¯ï¼š
2. å®ç°å†å²è®°å½•å­˜å‚¨
ä½¿ç”¨æµè§ˆå™¨çš„localStorageæ¥æŒä¹…åŒ–å­˜å‚¨å¯¹è¯å†å²ï¼š
3. å®ç°æ¶ˆæ¯æ ¼å¼åŒ–
å°†å†å²è®°å½•æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œç”¨äºå‘é€ç»™LLMï¼š
4. å®ç°å¯¹è¯ç®¡ç†
æ ¸å¿ƒçš„å¯¹è¯ç®¡ç†é€»è¾‘ï¼ŒåŒ…æ‹¬å‘é€æ¶ˆæ¯å’Œå¤„ç†å›å¤ï¼š
5. å®ç°ä¸Šä¸‹æ–‡çª—å£
ä¸ºäº†æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå¯ä»¥åªä¿ç•™æœ€è¿‘çš„Næ¡æ¶ˆæ¯ï¼š
åœ¨sendMessageå‡½æ•°ä¸­ï¼Œå¯ä»¥åœ¨ä¿å­˜å†å²è®°å½•å‰è°ƒç”¨trimHistoryï¼š
6.å®ç°æ¸…é™¤å†å²
æä¾›æ¸…é™¤å†å²è®°å½•çš„åŠŸèƒ½ï¼š
```


ç‚¹å‡»æŒ‰é’®`chatButton`æ—¶ï¼Œä¸è¦æ¸…ç©ºå½“å‰èŠå¤©æ¡†çš„å†…å®¹ã€‚æ¯æ¬¡èŠå¤©è¾“å‡ºçš„å†…å®¹ï¼Œéƒ½è¿½åŠ åˆ°èŠå¤©æ¡†å·²æœ‰çš„å†…å®¹ä¹‹å

#### 2025-02-21

1ã€å¤–æ˜¾æ€è€ƒè¿‡ç¨‹ã€‚

æŒ‰ DeepSeek å®˜æ–¹çš„æ¨èè®¾ç½®æ”¹é€ äº†æç¤ºè¯ã€‚

æ ¸å¿ƒç‚¹ 1ï¼šä¸è¦è®¾ç½®ç³»ç»Ÿæç¤ºè¯ã€‚

æ ¸å¿ƒç‚¹ 2ï¼šä¸ºäº†ç¡®ä¿æ¨¡å‹è¿›è¡Œå……åˆ†çš„æ¨ç†ï¼Œæˆ‘ä»¬å»ºè®®å¼ºåˆ¶æ¨¡å‹åœ¨æ¯æ¬¡è¾“å‡ºçš„å¼€å§‹éƒ½ä½¿ç”¨"<think>\n"ä½œä¸ºèµ·å§‹ã€‚

æ ¸å¿ƒç‚¹ 3ï¼štemperature å‚æ•°è®¾ç½®ä¸º 0.6ã€‚

å‘ç°è¾“å‡ºçš„ç»“æœé‡Œå¸¦æœ‰äº† <think> æ ‡ç­¾ï¼Œè¿™ä¸ªæ—¶å€™æ‰çŸ¥é“ä¹‹å‰è‡ªå·±æ‹¿åˆ°çš„ç»“æœå…¶å®åŒ…å«äº†æ€è€ƒè¿‡ç¨‹ï¼Œä¸€ç›´ä»¥ä¸ºè‡ªå·±æ²¡æ‹¿åˆ°æ€è€ƒè¿‡ç¨‹ã€‚

æœ€å¼€å§‹çš„æç¤ºè¯ï¼š

            prompt_template = ChatPromptTemplate([
                ("user", "Use the following pieces of context to answer the question at the end.\n{context}\nQuestion: {question}")
            ])


æ”¹äº†ä¸€ç‰ˆåçš„æç¤ºè¯ï¼š

            prompt_template = ChatPromptTemplate([
                ("user", "Use the following pieces of context to answer the question at the end.\n{context}\nQuestion: {question}")
            ])

æœ€ç»ˆçš„æç¤ºè¯ï¼š

            prompt_template = ChatPromptTemplate([
                ("user", "**response with \"\<think\>\n\" at the beginning of every output.**\nUse the following pieces of context to answer the question at the end.\n{context}\nQuestion: {question}")
            ])



#### 2025-02-19

ä½¿ç”¨ Flask è‡ªå¸¦çš„ debug æ¨¡å¼ï¼Œå®ƒä¹Ÿæœ‰è‡ªåŠ¨é‡è½½åŠŸèƒ½ï¼š

app.run(host='0.0.0.0', port=5001, debug=True)

1ã€å¢åŠ å‰ç«¯é¡µé¢ã€‚






#### 2025-02-18

1ã€è°ƒç”¨å¤§æ¨¡å‹å•ç‹¬å°è£…å‡ºæ¥ã€‚

```py
def chat_with_llm(question, context):
    full_response = ""
    system_template = '''
    You are a helpful AI assistant. Reply in Simplified Chinese. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say you don't know. DO NOT try to make up an answer. If the question is not related to the context, politely respond that you are tuned to only answer questions that are related to the context. {context}  Question: {question} Helpful answer:
    '''
    prompt_template = ChatPromptTemplate.from_messages(
        [("system", system_template), ("user", "{context}")]
    )
    prompt = prompt_template.invoke({"context": context, "question": question})
    response = model.stream(prompt)
    for chunk in response:
        print(chunk.content, end='', flush=True)
        full_response += chunk.content
    return full_response
``



```py
def chat_with_llm_pure(question, chat_record_file=None):
    full_response = ""
    response = model.stream(question)
    for chunk in response:
        print(chunk.content, end='', flush=True)
        full_response += chunk.content
    if chat_record_file:
        with open(chat_record_file, 'w', encoding='utf-8') as f:
            f.write(f"[question]:\n\n{question}\n\n[answer]:\n\n{full_response}")
    return full_response
``


1ã€ç”¨ FastAPI å°è£…æˆæ¥å£ã€‚

é¡¹ç›®çš„ç»“æ„è°ƒæ•´å¦‚ä¸‹ï¼š

project_root/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ retrieval.py
â”‚   â”œâ”€â”€ indexing.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ agent_rag.py
â”œâ”€â”€ eval/
â”‚   â””â”€â”€ utils_eval.py
â””â”€â”€ helper.py
â””â”€â”€ .env

01 ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨ Python è·¯å¾„ä¸­

ä¸ºäº†è®©ç»å¯¹å¯¼å…¥æ­£å¸¸å·¥ä½œï¼Œéœ€è¦ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨ Python çš„ `sys.path` ä¸­ã€‚å¯ä»¥åœ¨é¡¹ç›®å…¥å£æ–‡ä»¶ï¼ˆå¦‚ `main.py`ï¼‰ä¸­æ·»åŠ ä»¥ä¸‹ä»£ç ï¼š

```py
import sys
import os

 # å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° sys.path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
``

02 åˆ›å»º src/__init__.py

åœ¨ `src/` ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªç©ºçš„ `__init__.py` æ–‡ä»¶ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ª Python åŒ…ï¼š

03 å°†åŒ…çš„å¼•ç”¨ä¿®æ”¹ä¸ºç»å¯¹è·¯å¾„ã€‚æ¯”å¦‚ï¼š

ä¿®æ”¹ `src/api/main.py`

è¿è¡ŒåæŠ¥é”™ï¼š
```
python3.10 api/main.py
Traceback (most recent call last):
  File "/Users/Daglas/dalong.github/agent-rag/src/api/main.py", line 3, in <module>
    from src.retrieval import basic_query_from_documents, chat_with_llm_pure
ModuleNotFoundError: No module named 'src'
```

è¿™ä¸ªé”™è¯¯è¡¨æ˜ Python æ— æ³•æ‰¾åˆ° `src` æ¨¡å—ã€‚è¿™æ˜¯å› ä¸º Python çš„æ¨¡å—æœç´¢è·¯å¾„ï¼ˆ`sys.path`ï¼‰ä¸­æ²¡æœ‰åŒ…å«é¡¹ç›®çš„æ ¹ç›®å½•ã€‚è®©æˆ‘ä»¬ä¸€æ­¥æ­¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼š

æœ€ç»ˆé€‰çš„è§£å†³æ–¹æ¡ˆï¼š

è§£å†³æ–¹æ¡ˆ 1ï¼šä¿®æ”¹è¿è¡Œæ–¹å¼ï¼ˆæ¨èï¼‰

cd /Users/Daglas/dalong.github/agent-rag
python3.10 -m src.api.main

è·‘èµ·æ¥åï¼Œéšä¾¿åœ¨ä¸€ä¸ª shell é‡Œè¾“å…¥å³å¯ï¼š


curl -X POST "http://localhost:8001/query" \
-H "Content-Type: application/json" \
-d '{
    "question": "å¤§æ¨¡å‹é¢†åŸŸæœ‰å“ªäº›æœºä¼šï¼Œç‰¹åˆ«æ˜¯ DeepSeek ç”Ÿæ€ç›¸å…³çš„ã€‚è¯¦ç»†é˜è¿°ã€‚",
    "index_names": ["Yangzhiping"],
    "similarity_top_k": 12
}'


#### 2025-02-17

1ã€æ£€ç´¢æ—¶å¯ä»¥ä¸ç”¨ä¼ é€’ç»™ llama_index å¤§æ¨¡å‹ã€‚

æºä»£ç ï¼š

```py
Settings.llm = OpenAI(
    api_base="https://api.302.ai/v1",
    api_key=api_key,
    model_name="deepseek-v3-huoshan"
)
```

å› ä¸ºä¹‹å‰çœ‹è¯¾ç¨‹ã€Œ2025004Building-Agentic-RAG-with-Llamaindexã€ï¼Œæ˜¯å°†å¤§æ¨¡å‹ç›´æ¥åµŒå…¥åˆ° llama_index é‡Œçš„ï¼Œå‘ç°æ•ˆæœä¸å¥½ï¼Œåæ¥ç›´æ¥é‡æ–°è°ƒç”¨äº†å¤§æ¨¡å‹å°†æ£€ç´¢çš„çš„ç»“æœåˆå¹¶åˆ° Prompt é‡Œæé—®ã€‚

ä»Šå¤©æ‰å‘ç°ï¼Œå…¶å®æ ¹æœ¬æ— éœ€å°†å¤§æ¨¡å‹åµŒå…¥åˆ° llama_indexï¼Œllama_index åœ¨æ£€ç´¢çš„æ—¶å€™å‹æ ¹ä¸æ˜¯ç”¨çš„å¤§æ¨¡å‹ã€‚ä¸Šé¢çš„ä»£ç å¯ä»¥ç›´æ¥æ³¨é‡Šæ‰ã€‚

2ã€æ”¯æŒæ£€ç´¢å‘é‡æ•°æ®åº“ä¸­çš„å¤šä¸ª collectionsã€‚

```py
def basic_query_from_documents(question, index_names, similarity_top_k):
    try:
        client = weaviate.connect_to_local()
        
        if isinstance(index_names, str):
            index_names = [index_names]
            
        vector_indices = []
        for index_name in index_names:
            vector_store = WeaviateVectorStore(
                weaviate_client=client, 
                index_name=index_name
            )
            vector_index = VectorStoreIndex.from_vector_store(vector_store)
            vector_indices.append(vector_index)
        
        # åˆ›å»ºæ¯ä¸ªç´¢å¼•çš„æ£€ç´¢å™¨
        retrievers = [index.as_retriever(similarity_top_k=similarity_top_k) for index in vector_indices]
        
        # è‡ªå®šä¹‰å¤åˆæ£€ç´¢å™¨
        # ä¿®æ”¹åçš„ MultiIndexRetriever å®ç°
        class MultiIndexRetriever(BaseRetriever):
            def __init__(self, retrievers, similarity_top_k):
                super().__init__()
                self.retrievers = retrievers
                self.similarity_top_k = similarity_top_k  # å­˜å‚¨å…¨å±€ top_k å€¼

            def _retrieve(self, query, **kwargs):
                all_nodes = []
                # æ”¶é›†æ‰€æœ‰æ£€ç´¢å™¨çš„èŠ‚ç‚¹
                for retriever in self.retrievers:
                    retrieved_nodes = retriever.retrieve(query, **kwargs)
                    all_nodes.extend(retrieved_nodes)
                
                # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•°é™åºæ’åºï¼ˆåˆ†æ•°è¶Šé«˜è¶Šç›¸å…³ï¼‰
                sorted_nodes = sorted(all_nodes, key=lambda x: x.score, reverse=True)
                
                # æˆªå–å‰ similarity_top_k ä¸ªèŠ‚ç‚¹
                return sorted_nodes[:self.similarity_top_k]

        # åˆ›å»ºå¤åˆæ£€ç´¢å™¨æ—¶ä¼ å…¥ similarity_top_k å‚æ•°
        combined_retriever = MultiIndexRetriever(retrievers, similarity_top_k=similarity_top_k)  # å…³é”®ä¿®æ”¹

        # åˆ›å»ºæŸ¥è¯¢å¼•æ“æ—¶ä¸éœ€è¦å†è®¾ç½® similarity_top_kï¼ˆå·²åœ¨æ£€ç´¢å™¨å±‚å¤„ç†ï¼‰
        query_engine = RetrieverQueryEngine.from_args(combined_retriever)
        
        response = query_engine.query(question)
        context = "\n".join([n.text for n in response.source_nodes])
        source_datas = response.source_nodes
        
        print_data_sources(source_datas)
        print(f"Number of source nodes: {len(source_datas)}")
        
        chat_with_llm(question, context)

    except Exception as e:
        print(f"Error occurred: {str(e)}")
        raise
    finally:
        if 'client' in locals():
            client.close()
            print("\nWeaviate connection closed.")
```


#### 2025-02-14

1ã€å®ç°å‘é‡åŒ–å‰çš„åˆ†å‰²æ–‡æ¡£ã€‚

ç»ˆäºè§£å†³äº†è¿™ä¸ªå›°æ‰°å¥½å‡ å¤©çš„äº‹æƒ…ã€‚

ä¹‹å‰åœ¨è¯¾ç¨‹ã€Œ2025004Building-Agentic-RAG-with-Llamaindexã€ä¸­çœ‹åˆ° Llamaindex å‘é‡åŒ–ä¹‹å‰ç”¨ä¸“é—¨çš„åˆ†å‰²å™¨åˆ†å‰²æ–‡æ¡£çš„ã€‚

```py
    # load documents
    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()
    splitter = SentenceSplitter(chunk_size=1024)
    nodes = splitter.get_nodes_from_documents(documents)
    vector_index = VectorStoreIndex(nodes)
```

ä½†æ˜¯åˆ‡æ¢ä¸ºç”¨ weaviate åï¼Œè¿™ä¸ªå®ç°å§‹ç»ˆæ²¡æˆåŠŸï¼Œä¸€åº¦ä»¥ä¸ºå‘é‡æ•°æ®åº“ä¸æ”¯æŒè¿™ç§å½¢å¼ã€‚weaviate çš„å‘é‡åŒ–é‡‡ç”¨ä¸‹é¢çš„å½¢å¼å®ç°çš„ã€‚åªèƒ½æ•´ä¸ª documents å‘é‡åŒ–è€Œä¸æ˜¯åˆ‡å—å‘é‡åŒ–ã€‚è€Œä¸”è¿˜å¼„é”™ï¼šä»¥ä¸º batch_size æ˜¯ä¹‹å‰åˆ‡æ¢çš„ chunk_sizeï¼Œè¿™ä¸¤ä¸ªä¸æ˜¯ä¸€ä¸ªä¸œè¥¿ã€‚batch_size æ˜¯å¤„ç†çš„æ—¶å€™å¹¶å‘å¤„ç†åŒ…çš„æ•°é‡ã€‚

```py
def create_document_index(input_files, index_name):
    # è¿æ¥æœ¬åœ° Weaviate
    client = weaviate.connect_to_local()
    # åˆ›å»ºé›†åˆ
    documents = client.collections.create(name=index_name)
    print("documents collection has been created.")
    # load documents
    documents = SimpleDirectoryReader(input_files=input_files).load_data()

    custom_batch = client.batch.fixed_size(
        batch_size=123,
        concurrent_requests=3,
        consistency_level=ConsistencyLevel.ALL,
    )
    vector_store = WeaviateVectorStore(
        weaviate_client=client,
        index_name=index_name,
        # we pass our custom batch as a client_kwargs
        client_kwargs={"custom_batch": custom_batch},
    )

    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    index = VectorStoreIndex.from_documents(
        documents, storage_context=storage_context
    )

    print("All vector data has been written to Weaviate.")

    client.close()  # Free up resources
```

å…ˆåˆ‡å‰²æ–‡æ¡£å†å‘é‡åŒ–çš„å®ç°å¦‚ä¸‹ï¼š

```py
def create_document_index(input_files, index_name, chunk_size=1024, chunk_overlap=200):
    try:
        # è¿æ¥æœ¬åœ° Weaviate
        client = weaviate.connect_to_local()

        # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™åˆ é™¤
        if client.collections.exists(index_name):
            client.collections.delete(index_name)
            print(f"Existing collection {index_name} has been deleted.")
        
        # åˆ›å»ºé›†åˆ
        documents = client.collections.create(name=index_name)
        print("documents collection has been created.")

        # load documents
        documents = SimpleDirectoryReader(input_files=input_files).load_data()
        # è®¾ç½®æ–‡æ¡£åˆ†å‰²å™¨
        splitter = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        nodes = splitter.get_nodes_from_documents(documents)

        vector_store = WeaviateVectorStore(
            weaviate_client=client,
            index_name=index_name,
        )

        storage_context = StorageContext.from_defaults(vector_store=vector_store)

        index = VectorStoreIndex(
            nodes,
            storage_context=storage_context,
            show_progress=True
        )

        print("All vector data has been written to Weaviate.")

    except Exception as e:
        print(f"Error occurred: {str(e)}")
        raise
    finally:
        if 'client' in locals():
            client.close()  # Ensure client is always closed
            print("Weaviate connection closed.")
```


#### 2025-02-13

1ã€å¢åŠ å‘é‡æ•°æ®åº“ã€‚

[Locally hosted | Weaviate](https://weaviate.io/developers/weaviate/quickstart/local)

å¼€å§‹ä»…ä»…çœ‹çš„å®˜æ–¹æ–‡æ¡£ï¼Œèµ°äº†ä¸å°‘å¼¯è·¯ã€‚çŸ¥é“çœ‹äº† llama_index çš„å®˜æ–¹æ–‡æ¡£æ‰ç®—è§£å†³ã€‚

[Weaviate Vector Store - LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/vector_stores/WeaviateIndexDemo/)

æœ¬åœ°éƒ¨ç½²å‘é‡æ•°æ®åº“çš„åŸºæœ¬æ€è·¯ï¼š

01 docker é‡Œéƒ¨ç½²å‘é‡æ•°æ®åº“ã€‚

ç›´æ¥ç”¨å®˜ç½‘æä¾›çš„ `docker-compose.yml` æ–‡ä»¶ã€‚é…ç½®æ–‡ä»¶é‡Œéƒ¨ç½²çš„æ—¶å€™å¯ä»¥åœ¨ weaviate é‡Œé…å¥—æŒ‡å®šåµŒå…¥æ¨¡å‹å’Œç”Ÿæˆç”Ÿæˆæ¨¡å‹ï¼Œä¹Ÿå¯ä»¥å–æ¶ˆã€‚è¿™ä¸ªåœ°æ–¹ç»™è‡ªå·±æŒ–äº†å‘ã€‚ä»¥ä¸º weaviate é‡Œåªèƒ½åœ¨è¿™é‡Œé…ç½®å¤§æ¨¡å‹ã€‚è€Œ weaviate é…å¥—çš„å¤§æ¨¡å‹åªæ”¯æŒ openAIã€Claudeã€Google é‚£äº›ä¸»æµï¼Œæ²¡å›½å†…ä»¥åŠç¬¬ä¸‰æ–¹å¹³å°çš„ã€‚åé¢å¡åœ¨äº†è¿™é‡Œï¼Œä¸çŸ¥é“å¦‚ä½•è·Ÿä¹‹å‰ llama_index é‡Œè°ƒç”¨å¤§æ¨¡å‹è¿èµ·æ¥ã€‚

02 é¡¹ç›®æ•°æ®åº“é‡Œæ–°å»ºæ•°æ®é›†ã€‚

```py
def create_document_collection():
    # è¿æ¥æœ¬åœ° Weaviate
    client = weaviate.connect_to_local()
    
    # åˆ›å»ºé›†åˆ
    documents = client.collections.create(
        name="LlamaIndex",
        vectorizer_config=Configure.Vectorizer.text2vec_ollama(
            api_endpoint="http://host.docker.internal:11434",
            model="bge-m3:latest",
        ),
        generative_config=Configure.Generative.ollama(
            api_endpoint="http://host.docker.internal:11434",
            model="deepseek-r1:14b",
        )
    )

    client.close()  # Free up resources
```

è¿™é‡Œå…¶å®åµŒå…¥æ¨¡å‹å’Œç”Ÿæˆæ¨¡å‹ä¹Ÿä¸ç”¨æŒ‡å®šçš„ã€‚æ¨¡å‹éƒ½æ˜¯ç”¨çš„å¤–éƒ¨çš„ã€‚

03 å‘é‡åŒ–æ•°æ®å­˜å…¥æ•°æ®åº“ã€‚

è¿™å°±å¼€å§‹äº†å‘ç‚¹ï¼Œå¼€å§‹ç”¨çš„å®˜æ–¹æ€è·¯å‚¨å­˜çš„ã€‚

```py
def import_document_to_weaviate():
    # load documents
    documents = SimpleDirectoryReader(input_files=["../data/papers/metagpt.pdf"]).load_data()

    splitter = SentenceSplitter(chunk_size=1024)
    nodes = splitter.get_nodes_from_documents(documents)
    
    # è¿æ¥æœ¬åœ° Weaviate
    client = weaviate.connect_to_local()

    documents = client.collections.get("DocumentRAG")

    with documents.batch.dynamic() as batch:
        for node in nodes:
            text = node.get_content()
            metadata = node.get_metadata_str()
            batch.add_object({
                "content": text,
                "metadata": metadata,
            })
            if batch.number_errors > 10:
                print("Batch import stopped due to excessive errors.")
                break

    failed_objects = documents.batch.failed_objects
    if failed_objects:
        print(f"Number of failed imports: {len(failed_objects)}")
        print(f"First failed object: {failed_objects[0]}")

    client.close()  # Free up resources

    print("All vector data has been written to Weaviate.")
```

æ­£ç¡®çš„åº”è¯¥æ˜¯ç”¨ llama_index å®˜æ–¹æ–‡æ¡£æä¾›çš„æ–¹æ³•ï¼š

```py
def import_vector_document():
    # è¿æ¥æœ¬åœ° Weaviate
    client = weaviate.connect_to_local()

    # load documents
    documents = SimpleDirectoryReader(input_files=["../data/papers/metagpt.pdf"]).load_data()

    custom_batch = client.batch.fixed_size(
        batch_size=1024,
        concurrent_requests=4,
        consistency_level=ConsistencyLevel.ALL,
    )
    vector_store = WeaviateVectorStore(
        weaviate_client=client,
        index_name="LlamaIndex",
        # we pass our custom batch as a client_kwargs
        client_kwargs={"custom_batch": custom_batch},
    )

    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    index = VectorStoreIndex.from_documents(
        documents, storage_context=storage_context
    )

    print("All vector data has been written to Weaviate.")

    client.close()  # Free up resources
```

04 ä»å‘é‡æ•°æ®åº“ä¸­æŸ¥è¯¢ã€‚

è¿™é‡Œä½¿ç”¨ llama_index å®˜æ–¹æ–‡æ¡£æä¾›çš„æ–¹æ³•ï¼Œå°±å¯ä»¥è·Ÿè¯¾ç¨‹ã€Œ2025004Building-Agentic-RAG-with-Llamaindexã€é‡Œçš„å†…å®¹æ¥ä¸Šäº†ã€‚

```py
    # è¿æ¥æœ¬åœ° Weaviate
    client = weaviate.connect_to_local()

    vector_store = WeaviateVectorStore(
        weaviate_client=client, 
        index_name="LlamaIndex"
    )

    vector_index = VectorStoreIndex.from_vector_store(vector_store)

    vector_query_engine = vector_index.as_query_engine()

    vector_tool = QueryEngineTool.from_defaults(
        query_engine=vector_query_engine,
        description=(
            "Useful for retrieving specific context from the MetaGPT paper."
        ),
    )

    query_engine = RouterQueryEngine(
        selector=LLMSingleSelector.from_defaults(),
        query_engine_tools=[
            vector_tool,
        ],
        verbose=True
    )

    response = query_engine.query("What is the summary of the document?")
    print(str(response))
    print(len(response.source_nodes))

    response = query_engine.query("How do agents share information with other agents?")
    print(len(response.source_nodes))
    print(str(response))

    client.close()  # Free up resources
```




#### 2025-02-12

1ã€åµŒå…¥æ¨¡å‹æ›´æ¢ä¸ºæœ¬åœ°ã€‚

from llama_index.core.embeddings import resolve_embed_model

Settings.embed_model = resolve_embed_model("local:/Users/Daglas/dalong.modelsets/bge-m3")


---

o3-mini-high è”ç½‘æœçš„è§£å†³æ–¹æ¡ˆ

æ–¹æ³•äºŒï¼šä½¿ç”¨ LlamaIndex æä¾›çš„æœ¬åœ°æ¨¡å‹åŠ è½½æ–¹å¼
å¦‚æœæ‚¨çš„ bge-m3 æ¨¡å‹æ˜¯é€šè¿‡ Ollama éƒ¨ç½²åœ¨æœ¬åœ°ï¼Œå¹¶ä¸”éœ€è¦ä»¥â€œæœ¬åœ°åµŒå…¥æ¨¡å‹â€çš„æ–¹å¼åŠ è½½ï¼ˆå³ä¸èµ° OpenAIEmbedding ç±»ï¼‰ï¼Œå¯ä»¥é‡‡ç”¨ LlamaIndex ä¸­çš„ resolve_embed_model æ–¹æ³•æ¥åŠ è½½æœ¬åœ°æ¨¡å‹ã€‚ä¾‹å¦‚ï¼ˆæ³¨æ„ï¼šä¸‹é¢çš„è·¯å¾„ä»…ä¾›ç¤ºä¾‹ï¼Œæ‚¨éœ€è¦æ ¹æ®å®é™…éƒ¨ç½²æƒ…å†µè°ƒæ•´ï¼‰ï¼š

python
å¤åˆ¶
from llama_index.core.embeddings import resolve_embed_model

\# ä¾‹å¦‚ï¼Œæœ¬åœ°éƒ¨ç½²æ—¶æ‚¨å°† bge-m3 æ¨¡å‹å­˜æ”¾åœ¨æŸä¸ªç›®å½•ä¸‹ï¼Œ
\# æˆ–è€…é€šè¿‡ Ollama çš„å†…éƒ¨åè®®è°ƒç”¨ï¼Œæ„é€ å¯¹åº”çš„æ¨¡å‹å­—ç¬¦ä¸²ï¼ˆæ­¤å¤„â€œlocal:â€ä»…ä¸ºæ ‡è¯†ï¼Œå…·ä½“æ ¼å¼è¯·å‚è€ƒæ‚¨ä½¿ç”¨çš„ LlamaIndex ç‰ˆæœ¬æ–‡æ¡£ï¼‰
Settings.embed_model = resolve_embed_model("local:/path/to/your/bge-m3:latest")


2ã€å¢åŠ  Google çš„ gemini å¤§æ¨¡å‹ã€‚

from llama_index.llms.gemini import Gemini

os.environ["http_proxy"] = "http://127.0.0.1:7890"
os.environ["https_proxy"] = "http://127.0.0.1:7890"

api_key = get_api_key()

model_name = "models/gemini-2.0-flash-exp"
Settings.llm = Gemini(
    api_key=api_key,
    model_name=model_name
)

---

o3-mini-high è”ç½‘æœçš„è§£å†³æ–¹æ¡ˆ

åœ¨ llama_index ä¸­è°ƒç”¨ Google çš„ Gemini å¤§æ¨¡å‹ï¼Œä½ å¯ä»¥ä½¿ç”¨ llama_index å†…ç½®çš„ Gemini æ¥å£æ¥å®Œæˆã€‚åŸºæœ¬æµç¨‹å¦‚ä¸‹ï¼š

1. **å®‰è£…ç›¸å…³åŒ…**  
   ä¸ºäº†ä½¿ç”¨ Gemini æ¨¡å‹ï¼Œéœ€è¦å®‰è£…æ”¯æŒ Gemini çš„æ‰©å±•åŒ…ã€‚ä¾‹å¦‚ï¼š
   ```bash
   pip install llama-index-llms-gemini
   pip install llama-index-embeddings-gemini
   ```
   è¿™äº›åŒ…ä¼šæ‰©å±• llama_indexï¼Œä½¿å…¶æ”¯æŒè°ƒç”¨ Google çš„ Gemini å¤§æ¨¡å‹ã€‚

2. **è®¾ç½® Google API å¯†é’¥**  
   Gemini æ¨¡å‹éœ€è¦ä¸€ä¸ªæœ‰æ•ˆçš„ Google API å¯†é’¥ã€‚ä½ å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡æ¥é…ç½®å¯†é’¥ï¼Œä¾‹å¦‚ï¼š
   ```python
   import os
   os.environ["GOOGLE_API_KEY"] = "your_google_api_key"
   ```
   æˆ–è€…ä½¿ç”¨ getpass ç­‰æ–¹å¼å®‰å…¨åœ°è·å–å¯†é’¥ã€‚

3. **åˆå§‹åŒ– Gemini æ¨¡å‹ä¸åµŒå…¥æ¨¡å‹**  
   åœ¨ llama_index ä¸­ï¼Œé»˜è®¤ä½¿ç”¨ Settings å¯¹è±¡æ¥ç®¡ç†å…¨å±€çš„ LLM å’ŒåµŒå…¥æ¨¡å‹ã€‚ä½ å¯ä»¥å°† Gemini æ¨¡å‹å’Œ Gemini åµŒå…¥æ¨¡å‹åˆ†åˆ«è®¾ç½®ç»™ Settings.llm å’Œ Settings.embed_modelã€‚ä¾‹å¦‚ï¼š
   ```python
   from llama_index.llms.gemini import Gemini
   from llama_index.embeddings.gemini import GeminiEmbedding
   from llama_index import Settings

   # è®¾ç½® Gemini åµŒå…¥æ¨¡å‹ï¼ˆæ­¤å¤„æ¨¡å‹åç§°é€šå¸¸ä¸º "models/embedding-001"ï¼Œä½ ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©å…¶ä»–å‹å·ï¼‰
   Settings.embed_model = GeminiEmbedding(model_name="models/embedding-001", api_key=os.environ["GOOGLE_API_KEY"])
   
   # åˆå§‹åŒ– Gemini LLMï¼ˆå¦‚æœéœ€è¦æŒ‡å®šæ›´é«˜ç‰ˆæœ¬çš„ Geminiï¼Œæ¯”å¦‚ gemini-1.5-pro-latestï¼Œä¹Ÿå¯åœ¨åˆå§‹åŒ–æ—¶ä¼ å…¥ model_name å‚æ•°ï¼‰
   Settings.llm = Gemini(api_key=os.environ["GOOGLE_API_KEY"])
   ```
   è¿™æ ·å…¨å±€çš„ LLM ä¸åµŒå…¥æ¨¡å‹å°±é…ç½®å¥½äº†ï¼Œåç»­åœ¨æ„å»ºç´¢å¼•æˆ–æŸ¥è¯¢æ—¶ä¼šè‡ªåŠ¨ä½¿ç”¨è¿™ä¸¤ä¸ªè®¾ç½®ã€‚

4. **æ„å»ºç´¢å¼•å¹¶å‘èµ·æŸ¥è¯¢**  
   ä»¥ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹æ¥çœ‹ï¼Œå‡è®¾ä½ æœ‰ä¸€ä¸ªå­˜æ”¾æ–‡æ¡£çš„æ–‡ä»¶å¤¹ï¼ˆå¦‚ "data"ï¼‰ï¼Œä½ å¯ä»¥ç”¨ SimpleDirectoryReader åŠ è½½æ–‡æ¡£ï¼Œç„¶åæ„å»º VectorStoreIndex å¹¶ä½¿ç”¨ Gemini LLM è¿›è¡ŒæŸ¥è¯¢ï¼š
   ```python
   from llama_index import SimpleDirectoryReader, VectorStoreIndex

   # åŠ è½½æ–‡æ¡£
   documents = SimpleDirectoryReader("data").load_data()

   # ä»æ–‡æ¡£æ„å»ºç´¢å¼•ï¼ˆå†…éƒ¨ä¼šè°ƒç”¨å‰é¢è®¾ç½®å¥½çš„ Gemini åµŒå…¥æ¨¡å‹ï¼‰
   index = VectorStoreIndex.from_documents(documents)

   # æ„é€ æŸ¥è¯¢å¼•æ“å¹¶å‘èµ·æŸ¥è¯¢
   query_engine = index.as_query_engine()
   response = query_engine.query("è¯·é—®è¿™ä»½æ–‡æ¡£ä¸»è¦è®²äº†äº›ä»€ä¹ˆï¼Ÿ")
   print(response)
   ```
   è¿™æ ·ï¼Œllama_index ä¼šä½¿ç”¨ä½ è®¾ç½®çš„ Gemini æ¨¡å‹ç”Ÿæˆå“åº”ã€‚

ä»¥ä¸Šæ–¹æ³•å·²åœ¨å¤šä¸ªå®æˆ˜æ¡ˆä¾‹ä¸­ä½¿ç”¨ï¼Œä¾‹å¦‚åœ¨[LLMä¹‹RAGå®æˆ˜](îˆ€citeîˆ‚turn0search0îˆ)å’Œ GitHub ä¸Šç›¸å…³ç¤ºä¾‹é¡¹ç›®ä¸­éƒ½æœ‰è¯¦ç»†è¯´æ˜ã€‚æ³¨æ„ï¼Œå¦‚æœä½ éœ€è¦ä½¿ç”¨ Gemini çš„å…¶ä»–å‹å·ï¼ˆä¾‹å¦‚ gemini-1.5-pro-latestï¼‰ï¼Œå¯ä»¥åœ¨ Gemini åˆå§‹åŒ–æ—¶ä¼ å…¥ç›¸åº”å‚æ•°ï¼Œå¦‚ï¼š
```python
Settings.llm = Gemini(api_key=os.environ["GOOGLE_API_KEY"], model_name="models/gemini-1.5-pro-latest")
```

è¿™æ ·ï¼Œä½ å°±å¯ä»¥åœ¨ llama_index ä¸­è°ƒç”¨ Google çš„ Gemini å¤§æ¨¡å‹è¿›è¡Œæ–‡æ¡£ç´¢å¼•ã€æ£€ç´¢ä¸ç”Ÿæˆä»»åŠ¡äº†ã€‚


æŠ¥é”™ï¼š
```
google.api_core.exceptions.RetryError: Timeout of 60.0s exceeded, last exception: 503 failed to connect to all addresses; last error: UNAVAILABLE: ipv4:142.250.77.10:443: Failed to connect to remote host: Timeout occurred: FD shutdown
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1739367642.689011 19299647 init.cc:232] grpc_wait_for_shutdown_with_timeout() timed out.
```

è¡¥å……ä¿¡æ¯ï¼šæˆ‘çš„ mac ä¸Šå¼€äº†ä»£ç†ï¼Œchrome æµè§ˆå™¨ä¸­æ˜¯å¯ä»¥è®¿é—®Google çš„aistudioåå°çš„

å›å¤ï¼š

è¿™ä¸ªé”™è¯¯é€šå¸¸è¡¨ç¤ºåœ¨ä½¿ç”¨ Gemini API æ—¶ï¼ŒgRPC å®¢æˆ·ç«¯æ— æ³•é€šè¿‡ä»£ç†æˆåŠŸè¿æ¥åˆ° Google çš„æœåŠ¡å™¨ï¼Œå¯¼è‡´è¶…æ—¶ã€‚è™½ç„¶ Chrome æµè§ˆå™¨èƒ½æ­£å¸¸è®¿é—® aistudio åå°ï¼Œä½† gRPC è¯·æ±‚é»˜è®¤ä¸ä¼šè‡ªåŠ¨ä½¿ç”¨ä½ åœ¨ç³»ç»Ÿæˆ–æµè§ˆå™¨ä¸­é…ç½®çš„ä»£ç†ã€‚ä½ å¯ä»¥å°è¯•ä»¥ä¸‹å‡ ä¸ªæ–¹å‘è§£å†³é—®é¢˜ï¼š

1. **é…ç½® gRPC çš„ä»£ç†è®¾ç½®**  
   gRPC æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡é…ç½® HTTP ä»£ç†ã€‚ä½ å¯ä»¥è¯•è¯•è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆä¾‹å¦‚åœ¨ç»ˆç«¯æˆ–ä»£ç å¼€å¤´è®¾ç½®ï¼‰ï¼š
   - `http_proxy` å’Œ `https_proxy`ï¼šä¾‹å¦‚
     ```bash
     export http_proxy="http://127.0.0.1:ä½ çš„ä»£ç†ç«¯å£"
     export https_proxy="http://127.0.0.1:ä½ çš„ä»£ç†ç«¯å£"
     ```
     æˆ–è€…åœ¨ Python ä¸­ï¼š
     ```python
     import os
     os.environ["http_proxy"] = "http://127.0.0.1:ä½ çš„ä»£ç†ç«¯å£"
     os.environ["https_proxy"] = "http://127.0.0.1:ä½ çš„ä»£ç†ç«¯å£"
     ```
   è¿™æ · gRPC å¯èƒ½ä¼šéµå¾ªè¿™äº›ç¯å¢ƒå˜é‡ï¼Œå»ºç«‹è¿æ¥ã€‚

2. **æ£€æŸ¥ä»£ç†è®¾ç½®æ˜¯å¦é€‚ç”¨äº gRPC**  
   æœ‰æ—¶å€™ gRPC ä½¿ç”¨çš„è¿æ¥åº“å¯èƒ½ä¸å®Œå…¨æ”¯æŒ HTTP ä»£ç†ã€‚å¦‚æœä½ çš„ä»£ç†éœ€è¦è®¤è¯æˆ–è€…ä½¿ç”¨ç‰¹æ®Šåè®®ï¼Œå¯ä»¥å°è¯•æŸ¥é˜… [gRPC å®˜æ–¹æ–‡æ¡£](https://grpc.io/docs/guides/) æˆ–æœç´¢ â€œgrpc proxy configurationâ€ ç›¸å…³å†…å®¹ï¼Œç¡®ä¿ä»£ç†é…ç½®æ­£ç¡®ã€‚

3. **å¢åŠ è¶…æ—¶æ—¶é—´**  
   å¦‚æœç½‘ç»œçŠ¶å†µè¾ƒå·®ï¼Œä¹Ÿå¯ä»¥å°è¯•å»¶é•¿è¶…æ—¶æ—¶é—´ï¼Œä½†é€šå¸¸æ ¹æœ¬åŸå› åœ¨äºè¿æ¥é—®é¢˜è€Œä¸æ˜¯è¯·æ±‚è€—æ—¶ã€‚ä¾‹å¦‚å¯ä»¥æŸ¥é˜… Google API çš„å®¢æˆ·ç«¯é…ç½®ï¼Œçœ‹æ˜¯å¦æ”¯æŒè°ƒæ•´è¶…æ—¶å‚æ•°ï¼Œä¸è¿‡è¿™åªæ˜¯ç¼“è§£ï¼Œä¸èƒ½è§£å†³ä»£ç†æœªç”Ÿæ•ˆçš„é—®é¢˜ã€‚

4. **ä½¿ç”¨ VPN æˆ–å…¶ä»–ç½‘ç»œå·¥å…·**  
   å¦‚æœé…ç½®ä»£ç†ä¾ç„¶æ— æ³•ç”Ÿæ•ˆï¼Œå°è¯•ä¸´æ—¶å…³é—­ç³»ç»Ÿä»£ç†æˆ–è€…åˆ‡æ¢ VPN è¿æ¥ï¼Œçœ‹æ˜¯å¦èƒ½ç›´æ¥è¿æ¥ã€‚å¦‚æœèƒ½ç›´æ¥è¿æ¥ï¼Œè¯´æ˜é—®é¢˜ç¡®å®å‡ºåœ¨ä»£ç†è®¾ç½®ä¸Šã€‚ä¹Ÿå¯ä»¥è€ƒè™‘åœ¨ä»£ç ä¸­ä½¿ç”¨æ”¯æŒ gRPC ä»£ç†çš„ç¬¬ä¸‰æ–¹å·¥å…·ã€‚

æ€»ç»“ï¼šè¯·ç¡®ä¿ä½ åœ¨è¿è¡Œä»£ç çš„ç¯å¢ƒä¸­æ­£ç¡®è®¾ç½®äº† `http_proxy` å’Œ `https_proxy` ç¯å¢ƒå˜é‡ï¼Œæˆ–è€…ä½¿ç”¨å…¶ä»–æ”¯æŒ gRPC çš„ä»£ç†é…ç½®æ–¹æ³•ã€‚è¿™æ ·é€šå¸¸å¯ä»¥è§£å†³â€œTimeout of 60.0s exceededâ€¦ UNAVAILABLEâ€è¿™ç±»é”™è¯¯ã€‚

### é—®é¢˜è®°å½•

#### 01 è°ƒç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹æ—¶æŠ¥é”™

2025-02-12 

ImportError: llama-index-embeddings-huggingface package not found, please run pip install llama-index-embeddings-huggingface

è§£å†³æ–¹æ¡ˆï¼š

pip install llama-index-embeddings-huggingface

#### 02 ç¬¬ä¸‰æ–¹å¹³å°è°ƒç”¨ä¸äº† api

2025-02-12 

æ¯”å¦‚ 302ã€ç¡…åŸºæµåŠ¨ç­‰ã€‚

æ€»æ˜¯æŠ¥é”™è°ƒç”¨çš„ api è·‘åˆ° OpenAI çš„å®˜ç½‘ã€‚

æœ€åæ— æ„ä¸­å‘ç° OpenAI é‡Œä¼  url çš„å‚æ•°å†™é”™äº†ï¼Œæ‹·çš„ä¹‹å‰çš„æœ‰ä¸ª base_urlï¼Œè¿™è¾¹çš„å¯¹è±¡é‡Œå‚æ•°åº”è¯¥æ˜¯ api_baseã€‚

Settings.llm = OpenAI(
    api_base=base_url,
    api_key=api_key,
    model_name=model_name
)

#### 03 å‘é‡åŒ–çš„æ—¶å€™ä¸èƒ½ç”¨å¼‚æ­¥

2025-02-14

DeepSeek ç»™çš„å¦‚ä¸‹ä»£ç ï¼š

```py
        index = VectorStoreIndex(
            nodes,
            storage_context=storage_context,
            show_progress=True,
            use_async=True
        )
```

æŠ¥é”™ï¼š

llama_index.vector_stores.weaviate._exceptions.AsyncClientNotProvidedError: Async method called without WeaviateAsyncClient provided. Pass the async weaviate client to be used via `weaviate_client` to the constructor of WeaviateVectorStore.

ä¹‹å‰ä¸€ç›´æ²¡å¼„æ˜ç™½ï¼Œåæ¥å‘ç°æ˜¯ä¸æ”¯æŒå¼‚æ­¥ï¼Œçœ‹æŠ¥é”™ä¿¡æ¯ `weaviate_client` æ‰æ”¯æŒå¼‚æ­¥ï¼Œ`weaviate_client` åº”ç”¨æ˜¯æ•°æ®åº“åœ¨äº‘ç«¯ã€‚

å»æ‰å³å¯ã€‚

```py
        index = VectorStoreIndex(
            nodes,
            storage_context=storage_context,
            show_progress=True	#æ˜¾ç¤ºè¿›åº¦
        )
```




é€šè¿‡ä¸‹åˆ—æ–¹æ³•å°†æ–‡æ¡£çš„æ‘˜è¦å’Œæ–‡æ¡£å†…å®¹å‘é‡åŒ–åå­˜å…¥ weaviate å‘é‡æ•°æ®åº“ä¸­ã€‚

```py
    summary_index = SummaryIndex(nodes)
    vector_index = VectorStoreIndex(nodes)
```

ç”¨å¦‚ä¸‹æ–¹æ³•å¯ä»¥å°†æ–‡æ¡£å†…å®¹ä»å‘é‡æ•°æ®åº“ä¸­å–å‡ºç´¢å¼•ï¼Œé‚£ä¹ˆå¦‚ä½•å–å‡ºæ‘˜è¦çš„ç´¢å¼•ã€‚

```py
    vector_store = WeaviateVectorStore(
        weaviate_client=client, 
        index_name=index_name
    )
```

#### 04 llamaindexé€šè¿‡å¤§æ¨¡å‹å‘é‡æ£€ç´¢è·å¾—çš„ç­”æ¡ˆè·Ÿä½¿ç”¨å¤§æ¨¡å‹æé—®çš„è·å¾—çš„ç­”æ¡ˆæ˜¯ä¸¤å›äº‹

llamaindexé€šè¿‡å¤§æ¨¡å‹å‘é‡æ£€ç´¢è·å¾—çš„ç­”æ¡ˆè·Ÿä½¿ç”¨å¤§æ¨¡å‹æé—®çš„è·å¾—çš„ç­”æ¡ˆæ˜¯ä¸¤å›äº‹ï¼Œä¹‹å‰ä¸€ç›´ä»¥ä¸ºæ˜¯é€šè¿‡ä¸ªä¸œè¥¿ã€‚

æ¢å¥è¯è¯´ï¼šllamaindexé€šè¿‡å¤§æ¨¡å‹å‘é‡æ£€ç´¢ï¼Œå…¶ç›®çš„æ˜¯ä¸ºäº†è·å¾—æ£€ç´¢çš„ä¸Šä¸‹æ–‡ï¼Œè€Œä¸æ˜¯æœ€åçš„ç­”æ¡ˆã€‚æœ€åçš„ç­”æ¡ˆæ˜¯è¦ç”¨é¢å¤–çš„å¤§æ¨¡å‹å°†é—®é¢˜ä¸Šä¸‹æ–‡åˆå¹¶åå•ç‹¬æç»™å¤§æ¨¡å‹ã€‚

å¦‚ä½•å‘ç°é—®é¢˜çš„ï¼šæ¯æ¬¡æ£€ç´¢åæé—®è·å¾—çš„ç­”æ¡ˆå¥½ç®€å•ã€‚åŒæ ·çš„å‘é‡æ•°æ®åº“å’Œé—®é¢˜ï¼Œåœ¨ page assist é‡Œæé—®å°±å¾ˆè¯¦ç»†ï¼Œæœ€åç»ˆäºæ‰¾åˆ°é—®é¢˜äº†ã€‚

ä¹‹å‰çš„ä»£ç ï¼š

```py
def retrieval_from_documents_llamaindex(prompt, index_name, similarity_top_k):
    # è¿æ¥æœ¬åœ° Weaviate
    client = weaviate.connect_to_local()

    vector_store = WeaviateVectorStore(
        weaviate_client=client, 
        index_name=index_name
    )

    vector_index = VectorStoreIndex.from_vector_store(vector_store)
    query_engine = vector_index.as_query_engine(similarity_top_k=similarity_top_k)

    response = query_engine.query(prompt)
    print(str(response))
    # print(len(response.source_nodes))
    for n in response.source_nodes:
        print(n.metadata)
        print(n.text)
        print("----------------------------------------------------------")

    client.close()  # Free up resources
```


ä¿®æ”¹åçš„ä»£ç ï¼š

```py
def retrieval_from_documents(question, index_name, similarity_top_k):
    # è¿æ¥æœ¬åœ° Weaviate
    client = weaviate.connect_to_local()

    vector_store = WeaviateVectorStore(
        weaviate_client=client, 
        index_name=index_name
    )

    vector_index = VectorStoreIndex.from_vector_store(vector_store)
    query_engine = vector_index.as_query_engine(similarity_top_k=similarity_top_k)

    response = query_engine.query(question)
    context = "\n".join([n.text for n in response.source_nodes])

    prompt_template = ChatPromptTemplate.from_messages(
        [("system", system_template), ("user", "{context}")]
    )
    prompt = prompt_template.invoke({"context": context, "question": question})

    response = model.stream(prompt)
    for chunk in response:
        print(chunk.content, end='', flush=True)
    # print(response.content)

    client.close()  # Free up resources
```



### gemini çš„è°ƒç”¨

```py
os.environ["http_proxy"] = "http://127.0.0.1:7890"
os.environ["https_proxy"] = "http://127.0.0.1:7890"
api_key_google = get_api_key_google()
genai.configure(api_key=api_key_google, transport="rest")
gemini_model = genai.GenerativeModel(
    model_name = "gemini-2.0-flash-thinking-exp-01-21",
)
def chat_with_gemini(question, context):
    system_template = f"You are a helpful AI assistant. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say you don't know. DO NOT try to make up an answer. If the question is not related to the context, politely respond that you are tuned to only answer questions that are related to the context. Question: {question} context: {context}  Helpful answer:"
    response = gemini_model.generate_content(
                contents=system_template, 
                stream=True)
    for chunk in response:
        print(chunk.text, end="", flush=True)
    print(response.text)
```




