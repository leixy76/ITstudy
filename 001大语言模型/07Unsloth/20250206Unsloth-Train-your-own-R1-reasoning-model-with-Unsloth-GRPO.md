## 20250206Unsloth-Train-your-own-R1-reasoning-model-with-Unsloth-GRPO

[Train your own R1 reasoning model locally (GRPO)](https://unsloth.ai/blog/r1-reasoning)

[Blog](https://unsloth.ai/blog)

Feb 6, 2025

By Daniel & Michael

Today, we're excited to introduce reasoning in [Unsloth](https://github.com/unslothai/unsloth)! DeepSeekâ€™s R1 research revealed an â€œaha momentâ€ where R1-Zero autonomously learned to allocate more thinking time without human feedback by using Group Relative Policy Optimization (GRPO). 

ä»Šå¤©ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´ä»‹ç» [Unsloth](https://github.com/unslothai/unsloth) ä¸­çš„æ¨ç†åŠŸèƒ½ï¼DeepSeek çš„ R1 ç ”ç©¶æ­ç¤ºäº†ä¸€ä¸ªé‡è¦çš„å‘ç°ï¼šR1-Zero é€šè¿‡ä½¿ç”¨ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼ŒGroup Relative Policy Optimizationï¼‰ï¼Œåœ¨æ²¡æœ‰äººä¸ºå¹²é¢„çš„æƒ…å†µä¸‹ï¼Œè‡ªä¸»åœ°å­¦ä¼šäº†åˆ†é…æ›´å¤šçš„æ—¶é—´ç”¨äºæ€è€ƒã€‚

We've enhanced the entire GRPO process, making it use 80% less VRAM than Hugging Face + FA2. This allows you to reproduce R1-Zero's "aha moment" on just 7GB of VRAM using Qwen2.5 (1.5B). 

æˆ‘ä»¬å¯¹æ•´ä¸ª GRPO æµç¨‹è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä½¿å…¶ VRAMï¼ˆæ˜¾å­˜ï¼‰ä½¿ç”¨é‡ç›¸æ¯” Hugging Face + FA2 å‡å°‘äº† 80%ã€‚è¿™æ„å‘³ç€ï¼Œä½ ç°åœ¨åªéœ€ 7GB æ˜¾å­˜ï¼Œå¹¶ä½¿ç”¨ Qwen2.5ï¼ˆ1.5Bï¼‰æ¨¡å‹ï¼Œå°±èƒ½é‡ç° R1-Zero çš„ã€Œçµå…‰ä¹ç°ã€æ—¶åˆ»ã€‚

Try our free GRPO notebook: [Llama 3.1 (8B) on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)
For [our Guide](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl)and GRPO notebooks featuring other models like Phi-4, visit our [docs](https://docs.unsloth.ai/basics/reasoning-grpo)

ä¸å¦¨è¯•è¯•æˆ‘ä»¬å…è´¹çš„ GRPO notebookï¼š[Llama 3.1ï¼ˆ8Bï¼‰on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)
æƒ³è¦äº†è§£æ›´å¤šä¿¡æ¯ï¼Œä¾‹å¦‚ [æˆ‘ä»¬çš„æŒ‡å—](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl)ä»¥åŠåŒ…å« Phi-4 ç­‰å…¶ä»–æ¨¡å‹çš„ GRPO notebookï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„[æ–‡æ¡£](https://docs.unsloth.ai/basics/reasoning-grpo)

[Llama3.1\_(8B)-GRPO.ipynb - Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)

[Reasoning - GRPO & RL | Unsloth Documentation](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl)

P.S. thanks for the love on our R1 Dynamic [1.58-bit](https://unsloth.ai/blog/deepseekr1-dynamic)GGUF last week & don't forget to â­Star us: [github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)

P.S. æ„Ÿè°¢å„ä½ä¸Šå‘¨å¯¹ R1 Dynamic [1.58-bit](https://unsloth.ai/blog/deepseekr1-dynamic)GGUF çš„æ”¯æŒï¼Œè¯·ä¸è¦å¿˜è®° â­Star æˆ‘ä»¬çš„é¡¹ç›®ï¼š[github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)

### Main Details 

ä¸»è¦ç»†èŠ‚ï¼š

1 With 15GB VRAM, Unsloth allows you to transform any model up to 15B parameters like Llama 3.1 (8B), Phi-4 (14B), Mistral (7B) or Qwen2.5 (7B) into a reasoning model

ä»…éœ€ 15GB æ˜¾å­˜ï¼ˆVRAMï¼‰ï¼ŒUnsloth å°±èƒ½å°†ä»»ä½•é«˜è¾¾ 150 äº¿å‚æ•°çš„æ¨¡å‹ï¼Œä¾‹å¦‚ Llama 3.1ï¼ˆ8Bï¼‰ã€Phi-4ï¼ˆ14Bï¼‰ã€Mistralï¼ˆ7Bï¼‰æˆ– Qwen2.5ï¼ˆ7Bï¼‰ï¼Œè½¬åŒ–ä¸ºå¼ºå¤§çš„æ¨ç†æ¨¡å‹ã€‚

2 Minimum requirement: Just â€¯7GB VRAM is enough to train your own reasoning model locally.

æœ€ä½é…ç½®ï¼šåªéœ€ 7GB æ˜¾å­˜ï¼ˆVRAMï¼‰ï¼Œå³å¯åœ¨æœ¬åœ°è®­ç»ƒæ‚¨ä¸“å±çš„æ¨ç†æ¨¡å‹ã€‚

3 The incredible team at Tiny-Zero demonstrated that you could achieve your own "aha" moment with Qwen2.5 (1.5B) - but it required 2xA100 GPUs (160GB VRAM). Now, with Unsloth, you can achieve the same "aha" moment using just a single 7GB VRAM GPU

Tiny-Zero å›¢é˜Ÿçš„ç ”ç©¶è¡¨æ˜ï¼Œä½¿ç”¨ Qwen2.5ï¼ˆ1.5Bï¼‰å¯ä»¥å®ç°æ¨¡å‹èƒ½åŠ›çš„è·ƒå‡ã€‚è¿‡å»ï¼Œè¿™éœ€è¦ 2 å— A100 GPUï¼ˆ160GB æ˜¾å­˜ï¼ˆVRAM)ï¼‰ã€‚ç°åœ¨ï¼Œå€ŸåŠ© Unslothï¼Œä»…ç”¨ä¸€å— 7GB æ˜¾å­˜ï¼ˆVRAMï¼‰çš„ GPU å°±èƒ½å®ç°åŒæ ·çš„æ•ˆæœã€‚

4 Previously, GRPO was only supported for full fine-tuning, but we've made it work with QLoRA and LoRA

è¿‡å»ï¼ŒGRPO ä»…æ”¯æŒå®Œæ•´æ¨¡å‹çš„å¾®è°ƒï¼Œç°åœ¨æˆ‘ä»¬å·²å°†å…¶ä¸ QLoRA å’Œ LoRA æŠ€æœ¯ç›¸é€‚é…ã€‚

5 Please note, this isnâ€™t fine-tuning DeepSeekâ€™s R1 distilled models or using distilled data from R1 for tuning which Unsloth already supported. This is converting a standard model into a full-fledged reasoning model using GRPO.

è¯·æ³¨æ„ï¼Œè¿™ä¸å¾®è°ƒ DeepSeek çš„ R1 è’¸é¦æ¨¡å‹æˆ–ä½¿ç”¨ R1 è’¸é¦æ•°æ®è¿›è¡Œè®­ç»ƒä¸åŒï¼ŒUnsloth å·²ç»æ”¯æŒè¿™äº›åŠŸèƒ½ã€‚è¿™é‡Œçš„é‡ç‚¹æ˜¯ä½¿ç”¨ GRPO å°†æ™®é€šæ¨¡å‹è½¬åŒ–ä¸ºå…·æœ‰å®Œæ•´æ¨ç†èƒ½åŠ›çš„æ¨¡å‹ã€‚

6 Use cases for GRPO include: If you want to make a customized model with rewards (say for law, medicine etc.), then GRPO can help.If you have input and output data (like questions and answers), but do not have the chain of thought or reasoning process, GRPO can magically create the reasoning process for you! + much more

GRPO çš„åº”ç”¨åœºæ™¯åŒ…æ‹¬ï¼šå¦‚æœæ‚¨å¸Œæœ›åˆ›å»ºå…·å¤‡å¥–åŠ±æœºåˆ¶çš„å®šåˆ¶æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼Œåº”ç”¨äºæ³•å¾‹ã€åŒ»å­¦ç­‰é¢†åŸŸï¼‰ï¼ŒGRPO å°†å¤§æœ‰å¸®åŠ©ã€‚å¦‚æœæ‚¨æ‹¥æœ‰è¾“å…¥å’Œè¾“å‡ºæ•°æ®ï¼ˆæ¯”å¦‚é—®é¢˜å’Œç­”æ¡ˆï¼‰ï¼Œä½†ç¼ºä¹ä¸­é—´çš„æ¨ç†è¿‡ç¨‹ï¼ŒGRPO èƒ½å¤Ÿç¥å¥‡åœ°è¡¥å…¨æ¨ç†é“¾ï¼æ›´å¤šåº”ç”¨ç­‰ä½ æ¢ç´¢ã€‚

---

[X ä¸Šçš„ Jiayi Panï¼šâ€œWe reproduced DeepSeek R1-Zero in the CountDown game, and it just works Through RL, the 3B base LM develops self-verification and search abilities all on its own You can experience the Ahah moment yourself for &lt; $30 Code: https://t.co/B2IsN1PrXV Here's what we learned ğŸ§µ https://t.co/43BVYMmS8Xâ€ / X](https://x.com/jiayi_pirate/status/1882839370505621655)


We reproduced DeepSeek R1-Zero in the CountDown game, and it just works 

Through RL, the 3B base LM develops self-verification and search abilities all on its own 

You can experience the Ahah moment yourself for < $30 

Code:

[Jiayi-Pan/TinyZero: Clean, minimal, accessible reproduction of DeepSeek R1-Zero](https://github.com/Jiayi-Pan/TinyZero)

Here's what we learned ğŸ§µ

### GRPO + The "aha" moment 

GRPO +ã€Œå•Šå“ˆã€æ—¶åˆ» 

DeepSeekâ€™s researchers observed an "aha moment" when training R1-Zero with pure reinforcement learning (RL). The model learned to extend its thinking time by reevaluating its initial approach, without any human guidance or predefined instructions. 

DeepSeek ç ”ç©¶äººå‘˜åœ¨ä½¿ç”¨çº¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è®­ç»ƒ R1-Zero æ—¶ï¼Œè§‚å¯Ÿåˆ°äº†ä¸€ä¸ªã€Œå•Šå“ˆã€æ—¶åˆ»ã€‚R1-Zero æ¨¡å‹æ— éœ€ä»»ä½•äººå·¥æŒ‡å¯¼æˆ–é¢„è®¾æŒ‡ä»¤ï¼Œå°±å­¦ä¼šäº†é€šè¿‡é‡æ–°è¯„ä¼°åˆå§‹æ–¹æ¡ˆæ¥å»¶é•¿æ€è€ƒæ—¶é—´ã€‚

In a test example, even though we only trained Phi-4 with 100 steps using GRPO, the results are already clear. The model without GRPO does not have the thinking token, whilst the one trained with GRPO does and also has the correct answer. 

åœ¨ä¸€ä¸ªæµ‹è¯•ä¾‹å­ä¸­ï¼Œå³ä½¿æˆ‘ä»¬åªä½¿ç”¨ GRPO è®­ç»ƒäº† Phi-4 æ¨¡å‹ 100 æ­¥ï¼Œç»“æœå·²ç»éå¸¸æ˜æ˜¾ã€‚æœªä½¿ç”¨ GRPO çš„æ¨¡å‹æ²¡æœ‰ç”¨äºæ€è€ƒçš„ Tokenï¼Œè€Œä½¿ç”¨ GRPO è®­ç»ƒçš„æ¨¡å‹æ—¢æœ‰æ€è€ƒçš„ Tokenï¼Œè€Œä¸”ç­”æ¡ˆä¹Ÿæ˜¯æ­£ç¡®çš„ã€‚

This magic could be recreated through GRPO, a RL algorithm that optimizes responses efficiently without requiring a value function, unlike Proximal Policy Optimization (PPO) which relies on a value function. In our notebooks, we train a model with GRPO, aiming for it to develop its own self-verification and search abilities autonomously - creating a mini "aha moment". 

è¿™ç§èƒ½åŠ›å¯ä»¥é€šè¿‡ GRPO è¿™ç§å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç®—æ³•é‡ç°ã€‚ä¸ä¾èµ–ä»·å€¼å‡½æ•°çš„è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆProximal Policy Optimizationï¼ŒPPOï¼‰ä¸åŒï¼ŒGRPO æ— éœ€ä»·å€¼å‡½æ•°ï¼Œå°±èƒ½é«˜æ•ˆåœ°ä¼˜åŒ–å“åº”ã€‚åœ¨æˆ‘ä»¬çš„ Notebook ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ GRPO è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œç›®æ ‡æ˜¯è®©å®ƒè‡ªä¸»å‘å±•è‡ªæˆ‘éªŒè¯å’Œæœç´¢èƒ½åŠ›ï¼Œä»è€Œåˆ›é€ ä¸€ä¸ªå¾®å‹çš„ã€Œå•Šå“ˆæ—¶åˆ»ã€ã€‚

How it works: 

å·¥ä½œåŸç†å¦‚ä¸‹ï¼š

1 The model generates groups of responses.

æ¨¡å‹ç”Ÿæˆè‹¥å¹²ç»„å“åº”ã€‚

2 Each response is scored based on correctness or another metric created by some set reward function rather than an LLM reward model.

æ¯ä¸€ä¸ªå“åº”éƒ½åŸºäºæ­£ç¡®æ€§ï¼Œæˆ–è€…å…¶ä»–ç”±å¥–åŠ±å‡½æ•°è®¾å®šçš„æŒ‡æ ‡è¿›è¡Œè¯„åˆ†ï¼Œè¿™é‡Œä¸ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¥–åŠ±æ¨¡å‹ã€‚

3 The average score of the group is computed.

è®¡ç®—æ¯ç»„å“åº”çš„å¹³å‡åˆ†ã€‚

4 Each response's score is compared to the group average.

å°†æ¯ä¸ªå“åº”çš„å¾—åˆ†ä¸è¯¥ç»„çš„å¹³å‡åˆ†è¿›è¡Œæ¯”è¾ƒã€‚

5 The model is reinforced to favor higher-scoring responses.

æ¨¡å‹ä¼šå­¦ä¹ å¹¶å€¾å‘äºé€‰æ‹©å¾—åˆ†æ›´é«˜çš„å“åº”ã€‚

As an example, assume we want a model to solve: 
What is 1+1? >> Chain of thought/working out >> The answer is 2. 
What is 2+2? >> Chain of thought/working out >> The answer is 4. 

ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬å¸Œæœ›ä¸€ä¸ªæ¨¡å‹è§£å†³ä»¥ä¸‹é—®é¢˜ï¼š
1+1 æ˜¯å¤šå°‘ï¼Ÿ>> æ€ç»´é“¾ / è®¡ç®—æ­¥éª¤ >> ç­”æ¡ˆæ˜¯ 2ã€‚
2+2 æ˜¯å¤šå°‘ï¼Ÿ>> æ€ç»´é“¾ / è®¡ç®—æ­¥éª¤ >> ç­”æ¡ˆæ˜¯ 4ã€‚

Originally, one had to collect large swathes of data to fill the working out / chain of thought process. But GRPO (the algorithm DeepSeek uses) or other RL algorithms can steer the model to automatically exhibit reasoning capabilities and create the reasoning trace. Instead, we need to create good reward functions or verifiers. For example, if it gets the correct answer, give it a score of 1. If some words are mis-spelt, minus 0.1. And so on! We can provide many many functions to reward the process. 

æœ€åˆï¼Œäººä»¬å¿…é¡»æ”¶é›†å¤§é‡æ•°æ®æ¥å®Œå–„æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œä¹Ÿå°±æ˜¯æ€ç»´é“¾ï¼ˆchain of thoughtï¼‰ã€‚ä½†æ˜¯ GRPOï¼ˆDeepSeek ä½¿ç”¨çš„ç®—æ³•ï¼‰æˆ–å…¶ä»–å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼ŒRLï¼‰ç®—æ³•å¯ä»¥å¼•å¯¼æ¨¡å‹è‡ªåŠ¨å±•ç°æ¨ç†èƒ½åŠ›ï¼Œå¹¶ç”Ÿæˆæ¨ç†è¿‡ç¨‹çš„è½¨è¿¹ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬æ›´éœ€è¦åˆ›å»ºè‰¯å¥½çš„å¥–åŠ±å‡½æ•°æˆ–éªŒè¯å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ¨¡å‹ç»™å‡ºäº†æ­£ç¡®çš„ç­”æ¡ˆï¼Œå°±å¥–åŠ± 1 åˆ†ï¼›å¦‚æœæŸäº›å•è¯æ‹¼å†™é”™è¯¯ï¼Œåˆ™æ‰£ 0.1 åˆ†ã€‚æˆ‘ä»¬å¯ä»¥è®¾è®¡å¤šç§å¤šæ ·çš„å‡½æ•°æ¥å¥–åŠ±æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚

### GRPO in Unsloth

Unsloth ä¸­çš„ GRPO

If you're using GRPO with Unsloth locally, please "pip install diffusers" as well as it is a dependency. 

å¦‚æœåœ¨æœ¬åœ°ä½¿ç”¨ Unsloth è¿è¡Œ GRPOï¼Œè¯·å…ˆæ‰§è¡Œã€Œpip install diffusersã€å‘½ä»¤ï¼Œå› ä¸ºå®ƒæ˜¯å¿…è¦çš„ä¾èµ–ã€‚

Wait for at least 300 steps for the reward to actually increase and please use the latest version of vLLM. Keep in mind that our example on Colab was just trained in an hour so the results are subpar. In order to get good results, you will need to train for at least 12 hours (this is how GRPO works), but keep in mind this isn't compulsory as you can stop at anytime. 

è¯·è‡³å°‘ç­‰å¾… 300 æ­¥ï¼Œä»¥ä¾¿å¥–åŠ±å€¼èƒ½å¤ŸçœŸæ­£å¼€å§‹æå‡ã€‚åŒæ—¶ï¼Œè¯·ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ vLLMã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨ Colab ä¸Šæä¾›çš„ç¤ºä¾‹ä»…ä»…è®­ç»ƒäº†ä¸€ä¸ªå°æ—¶ï¼Œå› æ­¤å®éªŒç»“æœå¯èƒ½ä¸å¤Ÿç†æƒ³ã€‚ä¸ºäº†è·å¾—æ›´å¥½çš„å®éªŒç»“æœï¼Œæ‚¨éœ€è¦è‡³å°‘è®­ç»ƒ 12 ä¸ªå°æ—¶ï¼ˆè¿™æ˜¯ GRPO çš„å…¸å‹è®­ç»ƒæ—¶é•¿ï¼‰ã€‚å½“ç„¶ï¼Œè¿™å¹¶éå¼ºåˆ¶è¦æ±‚ï¼Œæ‚¨å¯ä»¥éšæ—¶æ ¹æ®å®é™…æƒ…å†µåœæ­¢è®­ç»ƒã€‚

It's [advised](https://x.com/jiayi_pirate/status/1882839487417561307/photo/1)to apply GRPO to a model at least 1.5B in parameters to correctly generate thinking tokens as smaller models may not. If youâ€™re using a base model, ensure you have a chat template. Training loss tracking for GRPO is now built directly into Unsloth, eliminating the need for external tools like wandb etc. 

å»ºè®®å°† GRPOï¼ˆGradient Ratio Policy Optimizationï¼‰åº”ç”¨äºå‚æ•°é‡è‡³å°‘è¾¾åˆ° 15 äº¿çš„æ¨¡å‹ï¼Œä»¥ä¾¿èƒ½å¤Ÿæ­£ç¡®ç”Ÿæˆã€Œæ€ç»´ Tokenã€â€”â€” å³æ¨¡å‹åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç”¨äºæ€è€ƒå’Œå†³ç­–çš„å†…éƒ¨è¡¨å¾ [advised](https://x.com/jiayi_pirate/status/1882839487417561307/photo/1)ã€‚å‚æ•°é‡è¾ƒå°çš„æ¨¡å‹å¯èƒ½æ— æ³•æœ‰æ•ˆç”Ÿæˆè¿™äº› Tokenã€‚å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯åŸºç¡€æ¨¡å‹ï¼Œè¯·ç¡®ä¿é…å¤‡äº†ã€ŒèŠå¤©æ¨¡æ¿ã€ï¼Œè¿™æ˜¯ä¸€ç§é¢„å®šä¹‰çš„å¯¹è¯ç»“æ„ï¼Œèƒ½å¤Ÿå¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´è¿è´¯ã€æ›´ç¬¦åˆè¯­å¢ƒçš„å›å¤ã€‚ç°åœ¨ï¼ŒGRPO çš„è®­ç»ƒæŸå¤±è·Ÿè¸ªåŠŸèƒ½å·²ç›´æ¥é›†æˆåˆ° Unsloth ä¸­ï¼Œæ— éœ€ä½¿ç”¨åƒ wandb è¿™æ ·çš„å¤–éƒ¨å·¥å…·ï¼ˆwandb æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„æœºå™¨å­¦ä¹ å®éªŒè·Ÿè¸ªå¹³å°ï¼‰ã€‚Unsloth é›†æˆè®­ç»ƒæŸå¤±è·Ÿè¸ªåŠŸèƒ½ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥æ›´æ–¹ä¾¿åœ°ç›‘æ§å’Œè°ƒè¯•è®­ç»ƒè¿‡ç¨‹ã€‚

In addition to adding GRPO support, we subsequently have support for Online DPO, PPO and RLOO as well! More details can be seen in [Keithâ€™s post](https://www.linkedin.com/feed/update/urn:li:activity:7290108099607097344/)and [blog](https://substack.com/home/post/p-154490380)that includes the Github fork on how he got Online DPO working. The initial draft of GRPO changes on Google Colab can be seen in [Joeyâ€™s tweet](https://x.com/shxf0072/status/1886085377146180091)as well! Both of their contributions allowed us to also make support for the other generation based RL methods. See below for a graph comparison of Unsloth's Online DPO VRAM consumption vs standard Hugging Face + FA2. 

é™¤äº†æ”¯æŒ GRPO ä¹‹å¤–ï¼ŒUnsloth è¿˜å¢åŠ äº†å¯¹åœ¨çº¿ DPOï¼ˆDirect Preference Optimizationï¼‰ã€PPOï¼ˆProximal Policy Optimizationï¼‰å’Œ RLOOï¼ˆReinforcement Learning from Online Observationsï¼‰çš„æ”¯æŒï¼æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è€ƒ [Keith's post](https://www.linkedin.com/feed/update/urn:li:activity:7290108099607097344/) å’Œ [blog](https://substack.com/home/post/p-154490380)ï¼Œå…¶ä¸­åŒ…å«äº†å…³äºä»–æ˜¯å¦‚ä½•å®ç° Online DPO çš„ Github åˆ†æ”¯ã€‚æ‚¨è¿˜å¯ä»¥åœ¨ [Joey's tweet](https://x.com/shxf0072/status/1886085377146180091) ä¸­æ‰¾åˆ° Google Colab ä¸Š GRPO æ›´æ”¹çš„åˆå§‹è‰æ¡ˆï¼ä»–ä»¬çš„è´¡çŒ®ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæ”¯æŒå…¶ä»–åŸºäºç”Ÿæˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•ï¼ˆå³é€šè¿‡ç”Ÿæˆå†…å®¹æ¥ä¼˜åŒ–ç­–ç•¥çš„ RL æ–¹æ³•ï¼‰ã€‚è¯·å‚é˜…ä¸‹å›¾ï¼Œäº†è§£ Unsloth çš„åœ¨çº¿ DPO æ˜¾å­˜ï¼ˆVRAMï¼‰æ¶ˆè€—ä¸æ ‡å‡† Hugging Face + FA2 çš„æ¯”è¾ƒã€‚

### Unsloth x vLLM # 

Unsloth x vLLM

#### 20x more throughput, 50% VRAM savings:

ååé‡æé«˜ 20 å€ï¼Œæ˜¾å­˜ï¼ˆVRAMï¼‰èŠ‚çœ 50%:

You can now use [vLLM](https://github.com/vllm-project/vllm/i) directly in your finetuning stack, which allows for much more throughput and allows you to finetune and do inference on the model at the same time! On 1x A100 40GB, expect **4000 tokens / s**or so with Unslothâ€™s dynamic 4bit quant of Llama 3.2 3B Instruct. On a 16GB Tesla T4 (free Colab GPU), you can get 300 tokens / s. 

ç°åœ¨ï¼Œæ‚¨å¯ä»¥ç›´æ¥åœ¨å¾®è°ƒå †æ ˆï¼ˆå³ç”¨äºæ¨¡å‹å¾®è°ƒçš„å·¥å…·å’Œåº“çš„é›†åˆï¼‰ä¸­ä½¿ç”¨ [vLLM](https://github.com/vllm-project/vllm/i)ï¼Œä»è€Œæ˜¾è‘—æé«˜ååé‡ï¼Œå¹¶å…è®¸æ‚¨åŒæ—¶å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒå’Œæ¨ç†ï¼åœ¨ä¸€å— A100 40GB æ˜¾å¡ä¸Šï¼Œä½¿ç”¨ Unsloth çš„ Llama 3.2 3B Instruct æ¨¡å‹çš„åŠ¨æ€ 4bit é‡åŒ–ï¼Œé¢„è®¡å¯ä»¥è¾¾åˆ°çº¦ **4000 ä¸ª Token/s** çš„å¤„ç†é€Ÿåº¦ã€‚åœ¨ 16GB Tesla T4ï¼ˆå…è´¹ Colab GPUï¼‰ä¸Šï¼Œæ‚¨å¯ä»¥è·å¾— 300 ä¸ª Token/s çš„å¤„ç†é€Ÿåº¦ã€‚

[vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs](https://github.com/vllm-project/vllm)

We also magically **removed double memory usage**when loading vLLM and Unsloth together, allowing for savings of 5GB or so for Llama 3.1 8B and 3GB for Llama 3.2 3B (thanks to inspiration from [Boris](https://github.com/borisshapa)). Unsloth could originally finetune Llama 3.3 70B Instruct in 1x 48GB GPU with Llama 3.3 70B weights taking 40GB of VRAM. If we do not remove double memory usage, then weâ€™ll need >= 80GB of VRAM when loading Unsloth and vLLM together. 

æˆ‘ä»¬è¿˜å·§å¦™åœ°**è§£å†³äº†åŒæ—¶åŠ è½½ vLLM å’Œ Unsloth æ—¶å†…å­˜å ç”¨ç¿»å€çš„é—®é¢˜**ã€‚è¿™ä¸€æ”¹è¿›ä¸º Llama 3.1 8B æ¨¡å‹èŠ‚çœäº†çº¦ 5GB æ˜¾å­˜ï¼Œä¸º Llama 3.2 3B æ¨¡å‹èŠ‚çœäº† 3GB æ˜¾å­˜ï¼ˆçµæ„Ÿæ¥è‡ª [Boris](https://github.com/borisshapa)ï¼‰ã€‚æœ€åˆï¼ŒUnsloth èƒ½å¤Ÿåœ¨å•å¼  48GB æ˜¾å­˜çš„ GPU ä¸Šå¾®è°ƒ Llama 3.3 70B Instruct æ¨¡å‹ï¼Œå…¶ä¸­ Llama 3.3 70B çš„æ¨¡å‹æƒé‡ä¼šå ç”¨ 40GB æ˜¾å­˜ï¼ˆVRAMï¼‰ã€‚å¦‚æœæ²¡æœ‰è§£å†³å†…å­˜å ç”¨ç¿»å€çš„é—®é¢˜ï¼ŒåŒæ—¶åŠ è½½ Unsloth å’Œ vLLM æ—¶ï¼Œåˆ™è‡³å°‘éœ€è¦ 80GB æ˜¾å­˜ï¼ˆVRAMï¼‰ã€‚

But with Unsloth, you can still finetune and get the benefits of fast inference in one package in under 48GB of VRAM! To use fast inference, first install vllm, and instantiate Unsloth with fast_inference: 

ä½†æ˜¯æœ‰äº† Unslothï¼Œä½ ä»ç„¶å¯ä»¥åœ¨ä¸€ä¸ªåŒ…ä¸­å¾®è°ƒå¹¶è·å¾—å¿«é€Ÿæ¨ç†çš„å¥½å¤„ï¼Œå¹¶ä¸”åªéœ€è¦ä½äº 48GB çš„æ˜¾å­˜ï¼ˆVRAM)ï¼è¦ä½¿ç”¨å¿«é€Ÿæ¨ç†ï¼Œé¦–å…ˆå®‰è£… vllmï¼Œå¹¶ä½¿ç”¨ fast_inference å®ä¾‹åŒ– Unslothï¼š

pip install unsloth vllm
from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained(
 model_name = "unsloth/Llama-3.2-3B-Instruct",
 fast_inference = True,
)
model.fast_generate(["Hello!"])`

#### vLLM Findings in Unsloth

Unsloth ä¸­ä½¿ç”¨ vLLM çš„å‘ç°

1 vLLM can now load Unsloth Dynamic 4-bit quants. Just like our 1.58bit Dynamic R1 GGUF, we showed that dynamically quantizing certain layers to 4-bit and some to 16-bit can dramatically improve accuracy whilst keeping the model small.

vLLM ç°åœ¨å¯ä»¥åŠ è½½ Unsloth Dynamic 4-bit quantsï¼ˆåŠ¨æ€ 4-bit é‡åŒ–ï¼‰ã€‚å°±åƒæˆ‘ä»¬çš„ 1.58bit Dynamic R1 GGUFï¼ˆè¶…é‡åŒ–æŠ€æœ¯ï¼‰ä¸€æ ·ï¼Œæˆ‘ä»¬å‘ç°å°†æŸäº›å±‚åŠ¨æ€é‡åŒ–åˆ° 4-bitï¼Œè€Œå°†å¦ä¸€äº›å±‚é‡åŒ–åˆ° 16-bitï¼Œå¯ä»¥æ˜¾è‘—æé«˜å‡†ç¡®æ€§ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹ä½“ç§¯è¾ƒå°ã€‚

2 We auto select multiple parameters to account for RAM, VRAM efficiency and maximum throughput (like # of chunked prefill tokens, # max sequences etc). We enable -O3 in vLLM by default and enable prefix caching. We found Flashinfer on old GPUs to be actually 10% slower. FP8 KV cache makes things 10% slower, but doubles throughput potential.

æˆ‘ä»¬è‡ªåŠ¨é€‰æ‹©å¤šä¸ªå‚æ•°æ¥å…¼é¡¾ RAMã€VRAM æ•ˆç‡å’Œæœ€å¤§ååé‡ï¼Œä¾‹å¦‚åˆ†å—é¢„å¡«å…… Tokenï¼ˆTokenï¼‰çš„æ•°é‡ã€æœ€å¤§åºåˆ—æ•°ç­‰ã€‚æˆ‘ä»¬é»˜è®¤åœ¨ vLLM ä¸­å¯ç”¨ -O3 ä¼˜åŒ–å¹¶å¯ç”¨å‰ç¼€ç¼“å­˜ã€‚æˆ‘ä»¬å‘ç°ï¼Œåœ¨æ—§ GPU ä¸Šä½¿ç”¨ Flashinfer å®é™…ä¸Šä¼šé™ä½ 10% çš„é€Ÿåº¦ã€‚FP8 KV ç¼“å­˜ä¼šä½¿é€Ÿåº¦é™ä½ 10%ï¼Œä½†èƒ½ä½¿ååé‡æ½œåŠ›ç¿»å€ã€‚

3 We allow LoRA loading in vLLM via parsing a state dict instead of loading from disk - this can make your GRPO training runs 1.5x faster. An active area of research is to somehow directly edit the LoRA adapters in vLLM (Iâ€™m not sure how to yet). This can boost speeds a lot, since weâ€™re doing unnecessary GPU data movement now.

æˆ‘ä»¬å…è®¸åœ¨ vLLM ä¸­é€šè¿‡è§£æ state dictï¼ˆæ¨¡å‹æƒé‡çŠ¶æ€å­—å…¸ï¼‰è€Œä¸æ˜¯ä»ç£ç›˜åŠ è½½æ¥åŠ è½½ LoRAï¼ˆLow-Rank Adaptationï¼‰â€”â€” è¿™å¯ä»¥ä½¿ä½ çš„ GRPOï¼ˆGroup Recursive Partitioning Optimizationï¼‰è®­ç»ƒè¿è¡Œé€Ÿåº¦æé«˜ 1.5 å€ã€‚ç›®å‰ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸæ˜¯å¦‚ä½•ç›´æ¥åœ¨ vLLM ä¸­ç¼–è¾‘ LoRA é€‚é…å™¨ï¼ˆLoRA adaptersï¼‰(æˆ‘è¿˜ä¸ç¡®å®šå…·ä½“æ–¹æ³•ï¼‰ã€‚å¦‚æœèƒ½å®ç°è¿™ä¸€ç‚¹ï¼Œå¯ä»¥å¤§å¤§æé«˜é€Ÿåº¦ï¼Œå› ä¸ºç°åœ¨æˆ‘ä»¬æ­£åœ¨è¿›è¡Œä¸å¿…è¦çš„ GPU æ•°æ®ç§»åŠ¨ï¼Œä»è€Œå¯¼è‡´æ•ˆç‡é™ä½ã€‚

4 vLLM will have random VRAM spikes weirdly, especially during batched generation. We added a batched generate function to reduce memory spikes.

vLLM å°¤å…¶æ˜¯åœ¨æ‰¹é‡ç”ŸæˆæœŸé—´ï¼Œä¼šå¥‡æ€ªåœ°å‡ºç°éšæœº VRAM å³°å€¼ã€‚æˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ªæ‰¹é‡ç”Ÿæˆå‡½æ•°ï¼ˆbatched generate functionï¼‰æ¥å‡å°‘å†…å­˜å³°å€¼ã€‚

### å‚è€ƒ

[X ä¸Šçš„ Jiayi Panï¼šâ€œQuick ablations on CountDown: Base model quality is key: We run Qwen-2.5-Base 0.5B, 1.5B, 3B to 7B. 0.5B guess a solution and stop. From 1.5B, the model start learning to search, to self-verify and to revise its solutions, enabling them to achieve much higher scores. https://t.co/BNhx9e4Sv4â€ / X](https://x.com/jiayi_pirate/status/1882839487417561307)

Jiayi Pan
@jiayi_pirate

1æœˆ25æ—¥

We reproduced DeepSeek R1-Zero in the CountDown game, and it just works 

Through RL, the 3B base LM develops self-verification and search abilities all on its own 

You can experience the Ahah moment yourself for < $30 

Code: http://github.com/Jiayi-Pan/TinyZero

Here's what we learned ğŸ§µ

Jiayi Pan
@jiayi_pirate

1æœˆ25æ—¥

The recipe: 

We follow DeepSeek R1-Zero alg -- Given a base LM, prompts and ground-truth reward, we run RL. 

We apply it to CountDown: a game where players combine numbers with basic arithmetic to reach a target number.

Jiayi Pan
@jiayi_pirate

1æœˆ25æ—¥

The results: It just works! 

Model start from dummy outputs but gradually develop tactics such as revision and search. 

In the following sample, the model propose a solution, self-verify, and iteratively revise it until it works. 

Full experiment log: https://wandb.ai/jiayipan/TinyZero

Jiayi Pan
@jiayi_pirate

Quick ablations on CountDown: 

Base model quality is key: 

We run Qwen-2.5-Base 0.5B, 1.5B, 3B to 7B. 0.5B guess a solution and stop. From 1.5B, the model start learning to search, to self-verify and to revise its solutions, enabling them to achieve much higher scores.

ä¸Šåˆ1:14 Â· 2025å¹´1æœˆ25æ—¥

Jiayi Pan
@jiayi_pirate

1æœˆ25æ—¥

Either base or instruct model works 

- Instruct model learns faster, but converges to about same performance as base 
- Instruct model's output are more structured and readable

So extra instruction tuning isn't necessary, which supports R1-Zero's design decision

Jiayi Pan
@jiayi_pirate

1æœˆ25æ—¥

The specific RL alg doesn't matter much

We tried PPO, GRPO and PRIME. Long cot all emerge and they seem all work well. We haven't got the time to tune the hyper-parameters, so don't want to make quantitative conclusions about which alg works better.
Jiayi Pan
@jiayi_pirate

1æœˆ25æ—¥

Model's reasoning behavior is very task dependent: 

- For countdown, the model learns to do search and self-verificatoin 
- For number multiplicatoin, the model instead learns to break down the problem using distirbution rule and solve it step by step.
Jiayi Pan
@jiayi_pirate

1æœˆ25æ—¥

Everything's open at http://github.com/Jiayi-Pan/TinyZero  

And it costs < $30 to train the model! We hope this project helps to demystify the emerging RL scaling research and make it more accessible!

æ¥è‡ª github.com
Jiayi Pan
@jiayi_pirate

1æœˆ25æ—¥
One caveat, of course, is that it's validated only in the Countdown task but not the general reasoning domain. We are now bounded by compute, and please reach out if you wanna help!

Jiayi Pan
@jiayi_pirate

1æœˆ25æ—¥

A wild ride with 

@JunjieZhang12
 
@xingyaow_
 
@lifan__yuan