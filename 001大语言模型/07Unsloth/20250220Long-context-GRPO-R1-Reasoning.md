## 20250220Long-context-GRPO-R1-Reasoning

[Long-context GRPO (R1 Reasoning)](https://unsloth.ai/blog/grpo)

Feb 20, 2025

By Daniel & Michael

You can now train your own reasoning model with just **5GB VRAM**for Qwen2.5 (1.5B) - down from 7GB in our previous GRPO release 2 weeks ago! 

å¥½æ¶ˆæ¯ï¼ç°åœ¨ï¼Œåªéœ€ **5GB æ˜¾å­˜**ï¼Œä½ å°±èƒ½ç”¨ Qwen2.5ï¼ˆ1.5Bï¼‰è®­ç»ƒè‡ªå·±çš„æ¨ç†æ¨¡å‹äº†ï¼ç›¸æ¯”äºä¸¤å‘¨å‰æˆ‘ä»¬å‘å¸ƒçš„ GRPO ç‰ˆæœ¬ï¼Œæ‰€éœ€æ˜¾å­˜ä» 7GB å¤§å¹…é™ä½ï¼

Currently, achieving longer context lengths is one of GRPO's biggest challenges. Our newly derived Unsloth Efficient GRPO algorithm enables **10x longer context**lengths while using **90% less VRAM**vs. all other GRPO LoRA/QLoRA implementations, even those utilizing Flash Attention 2 (FA2). 

ç›®å‰ï¼Œå¦‚ä½•å®ç°æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦æ˜¯ GRPO çš„ä¸€å¤§éš¾é¢˜ã€‚æˆ‘ä»¬å…¨æ–°æ¨å‡ºçš„ Unsloth Efficient GRPO ç®—æ³•ï¼Œèƒ½å°†ä¸Šä¸‹æ–‡é•¿åº¦æå‡ **10 å€**ï¼ŒåŒæ—¶ç›¸æ¯”å…¶ä»–æ‰€æœ‰ GRPO LoRA/QLoRA å®ç°ï¼Œ ç”šè‡³åŒ…æ‹¬é‚£äº›ä½¿ç”¨äº† Flash Attention 2ï¼ˆFA2ï¼‰çš„å®ç°ï¼Œæ˜¾å­˜å ç”¨è¿˜èƒ½é™ä½ **90%**ï¼

With a GRPO setup using TRL + FA2, Llama 3.1 (8B) trai ning at 20K context length demands 510.8GB of VRAM. However, Unslothâ€™s 90% VRAM reduction brings the requirement down to just 54.3GB in the same setup. 

åœ¨ä½¿ç”¨ TRL + FA2 çš„ GRPO è®¾ç½®ä¸‹ï¼Œè®­ç»ƒä¸Šä¸‹æ–‡é•¿åº¦ä¸º 20K çš„ Llama 3.1ï¼ˆ8Bï¼‰æ¨¡å‹éœ€è¦ 510.8GB çš„æ˜¾å­˜ï¼ˆVRAMï¼‰ã€‚ç„¶è€Œï¼ŒUnsloth å¸¦æ¥çš„ 90% æ˜¾å­˜å ç”¨å‡å°‘ï¼Œä½¿å¾—åœ¨ç›¸åŒè®¾ç½®ä¸‹ä»…éœ€ 54.3GB æ˜¾å­˜ã€‚

Try our free GRPO notebook with 10x longer context: [Llama 3.1 (8B) on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)
We'd highly recommend reading [our Guide](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl)for everything on GRPO + reward functions/verifiers. 
View our GRPO notebooks featuring other models like Phi-4 [here](https://docs.unsloth.ai/). 

ä½“éªŒæˆ‘ä»¬å…è´¹çš„ GRPO notebook å§ï¼å®ƒå…·æœ‰ 10 å€äºä»¥å¾€çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼š[Llama 3.1ï¼ˆ8Bï¼‰on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)
æˆ‘ä»¬å¼ºçƒˆæ¨èæ‚¨é˜…è¯» [æˆ‘ä»¬çš„æŒ‡å—](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl)ï¼Œå®ƒè¯¦ç»†ä»‹ç»äº† GRPO åŠå…¶å¥–åŠ±å‡½æ•° / éªŒè¯å™¨ã€‚
æƒ³äº†è§£æ›´å¤šï¼Ÿè¯·ç‚¹å‡» [è¿™é‡Œ](https://docs.unsloth.ai/)ï¼ŒæŸ¥çœ‹åŒ…å« Phi-4 ç­‰å…¶ä»–æ¨¡å‹çš„ GRPO notebooksã€‚

P.S. If you enjoyed our work, don't forget to â­Star us: [github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)

P.S. å¦‚æœæ‚¨å–œæ¬¢æˆ‘ä»¬çš„å·¥ä½œï¼Œè¯·ä¸è¦å¿˜è®° â­Star æˆ‘ä»¬ï¼š[github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)

### 90% less VRAM for long context 

å¤„ç†é•¿æ–‡æœ¬æ—¶ï¼Œæ˜¾å­˜å ç”¨å‡å°‘ 90%

When youâ€™re using Unsloth to do GRPO, we smartly reduce VRAM usage by over 90% when compared to standard implementations with Flash Attention 2 by using multiple tricks! On 20K context lengths for example with 8 generations per prompt, Unsloth uses only **54.3GB of VRAM for Llama 3.1 8B**, whilst standard implementations take **510.8GB (90% less for Unsloth)**. 

å½“æ‚¨ä½¿ç”¨ Unsloth è¿›è¡Œ GRPOï¼ˆGrouped Relative Position Outputï¼‰æ—¶ï¼Œä¸ä½¿ç”¨ Flash Attention 2 çš„æ ‡å‡†å®ç°ç›¸æ¯”ï¼Œæˆ‘ä»¬é€šè¿‡å¤šç§æŠ€å·§ï¼Œèƒ½å¤Ÿæ™ºèƒ½åœ°å°†æ˜¾å­˜ä½¿ç”¨é‡å‡å°‘ 90% ä»¥ä¸Šï¼ä¸¾ä¾‹æ¥è¯´ï¼Œå¯¹äºä¸Šä¸‹æ–‡é•¿åº¦ä¸º 20Kï¼Œæ¯ä¸ªæç¤ºç”Ÿæˆ 8 ä¸ªç»“æœçš„æƒ…å†µï¼ŒUnsloth ä»…éœ€ **54.3GB æ˜¾å­˜å³å¯è¿è¡Œ Llama 3.1 8B æ¨¡å‹**ï¼Œè€Œæ ‡å‡†å®ç°åˆ™éœ€è¦ **510.8GBï¼ˆUnsloth å‡å°‘ 90%)**ã€‚

1 Our new memory efficient linear algorithm for GRPO slashes memory usage by 8x or more. This shaves 68.5GB of memory, whilst being actually faster through the help of torch.compile for num_generations = 8 and 20K context length.

æˆ‘ä»¬ç”¨äº GRPOï¼ˆGrouped Relative Position Outputï¼‰çš„æ–°å‹å†…å­˜é«˜æ•ˆçº¿æ€§ç®—æ³•ï¼Œå¯ä»¥å°†å†…å­˜ä½¿ç”¨é‡å‡å°‘ 8 å€ç”šè‡³æ›´å¤šã€‚åœ¨ `num_generations = 8`ï¼ˆç”Ÿæˆæ•°é‡ä¸º 8ï¼‰å’Œ 20K ä¸Šä¸‹æ–‡é•¿åº¦çš„æƒ…å†µä¸‹ï¼Œè¿™èŠ‚çœäº† 68.5GB çš„å†…å­˜ï¼Œå¹¶ä¸”åœ¨ `torch.compile` çš„å¸®åŠ©ä¸‹ï¼Œé€Ÿåº¦å®é™…ä¸Šæ›´å¿«ã€‚

2 We leverage our smart Unsloth gradient checkpointing algorithm which we released a while ago. It smartly offloads intermediate activations to system RAM asynchronously whilst being only 1% slower. This shaves a whopping 372GB VRAM since we need num_generations = 8. We can reduce this memory usage even further through intermediate gradient accumulation.

æˆ‘ä»¬åˆ©ç”¨ä¹‹å‰å‘å¸ƒçš„ Unsloth é«˜æ•ˆæ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰ç®—æ³•ã€‚è¯¥ç®—æ³•èƒ½å¤Ÿå·§å¦™åœ°å°†ä¸­é—´æ¿€æ´»ï¼ˆintermediate activationsï¼‰å¼‚æ­¥å¸è½½åˆ°ç³»ç»Ÿå†…å­˜ä¸­ï¼Œé€Ÿåº¦ä»…é™ä½ 1%ã€‚ç”±äº `num_generations = 8`ï¼ˆç”Ÿæˆæ•°é‡ä¸º 8ï¼‰ï¼Œå› æ­¤èŠ‚çœäº†é«˜è¾¾ 372GB çš„æ˜¾å­˜ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸­é—´æ¢¯åº¦ç´¯ç§¯ï¼ˆintermediate gradient accumulationï¼‰è¿›ä¸€æ­¥å‡å°‘å†…å­˜ä½¿ç”¨ã€‚

3 Unsloth also uses the same GPU / CUDA memory space as the underlying inference engine (vLLM), unlike implementations in other packages. This shaves 16GB of VRAM.

ä¸å…¶ä»–è½¯ä»¶åŒ…ä¸­çš„å®ç°ä¸åŒï¼ŒUnsloth è¿˜ä¸åº•å±‚æ¨ç†å¼•æ“ï¼ˆvLLMï¼‰å…±äº« GPU / CUDA å†…å­˜ç©ºé—´ã€‚è¿™æ ·å¯ä»¥èŠ‚çœ 16GB çš„æ˜¾å­˜ã€‚

| Metric                               | ğŸ¦¥ Unsloth           | TRL + FA2            |
| :----------------------------------- | :------------------ | :------------------- |
| Training Memory Cost (GB)            | 42GB                | 414GB                |
| GRPO Memory Cost (GB)                | 9.8GB               | 78.3GB               |
| Inference Cost (GB)                | 0GB                 | 16GB                 |
| Inference KV Cache for 20K context (GB) | 2.5GB               | 2.5GB                |
| Total Memory Usage                     | 54.3GB (90% less) | 510.8GB              |

In typical standard GRPO implementations, you need to create 2 logits of size (8, 20K) to calculate the GRPO loss. This takes 2 * 2 bytes * 8 (num generations) * 20K (context length) * 128256 (vocabulary size) = 78.3GB in VRAM. 

åœ¨å…¸å‹çš„æ ‡å‡† GRPO å®ç°ä¸­ï¼Œéœ€è¦åˆ›å»º 2 ä¸ªå¤§å°ä¸ºï¼ˆ8ï¼Œ20Kï¼‰çš„ logitsï¼ˆlogitsï¼‰æ¥è®¡ç®— GRPO æŸå¤±ã€‚è¿™éœ€è¦ 2 * 2 å­—èŠ‚ï¼ˆbytesï¼‰* 8ï¼ˆç”Ÿæˆæ•°é‡ï¼‰* 20Kï¼ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼‰* 128256ï¼ˆè¯æ±‡è¡¨å¤§å°ï¼‰= 78.3GB çš„æ˜¾å­˜ï¼ˆVRAMï¼‰ã€‚

Unsloth shaves 8x memory usage for long context GRPO, so we need only an extra 9.8GB in extra VRAM for 20K context lengths! 

Unsloth å°†é•¿ä¸Šä¸‹æ–‡ GRPO çš„å†…å­˜å ç”¨é™ä½äº† 8 å€ï¼Œå› æ­¤å¯¹äº 20K çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæˆ‘ä»¬åªéœ€è¦é¢å¤– 9.8GB çš„æ˜¾å­˜ï¼

We also need to from the KV Cache in 16bit. Llama 3.1 8B has 32 layers, and both K and V are 1024 in size. So memory usage for 20K context length = 2 * 2 bytes * 32 layers * 20K context length * 1024 = 2.5GB per batch. We would set the batch size for vLLM to 8, but we shall leave it at 1 for our calculations to save VRAM. Otherwise you will need 20GB for the KV cache. 

æˆ‘ä»¬è¿˜éœ€è¦ä½¿ç”¨ 16bit çš„ KV ç¼“å­˜ï¼ˆKV Cacheï¼‰ã€‚Llama 3.1 8B å…·æœ‰ 32 å±‚ï¼Œå…¶ K å’Œ V çš„å¤§å°å‡ä¸º 1024ã€‚å› æ­¤ï¼Œå¯¹äº 20K ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œå†…å­˜ä½¿ç”¨é‡ = 2 * 2 å­—èŠ‚ / ä¸ª * 32 å±‚ * 20K ä¸Šä¸‹æ–‡é•¿åº¦ * 1024 = æ¯ä¸ªæ‰¹æ¬¡ 2.5GBã€‚è™½ç„¶æˆ‘ä»¬ä¼šå°† vLLM çš„æ‰¹å¤„ç†å¤§å°è®¾ç½®ä¸º 8ï¼Œä½†ä¸ºäº†èŠ‚çœæ˜¾å­˜ï¼Œè¿™é‡Œæˆ‘ä»¬å°†å…¶ä¿ç•™ä¸º 1 è¿›è¡Œè®¡ç®—ã€‚å¦åˆ™ï¼Œæ‚¨å°†éœ€è¦ 20GB ç”¨äº KV ç¼“å­˜ã€‚

### Unsloth Efficient GRPO algorithm 

Unsloth é«˜æ•ˆ GRPO ç®—æ³•

We got inspired from Horace [Heâ€™s linear cross entropy](https://gist.github.com/Chillee/22cd93e11b887db1f596ab754d60a899)implementation, and managed to make it work for GRPO! We actually found a few surprising points: 

Unsloth é«˜æ•ˆ GRPO ç®—æ³•çš„çµæ„Ÿæ¥è‡ª Horace çš„çº¿æ€§äº¤å‰ç†µï¼ˆlinear cross entropyï¼‰[å®ç°](https://gist.github.com/Chillee/22cd93e11b887db1f596ab754d60a899)ï¼Œå¹¶è®¾æ³•ä½¿å…¶é€‚ç”¨äº GRPOï¼æˆ‘ä»¬å®é™…ä¸Šå‘ç°äº†ä¸€äº›ä»¤äººæƒŠè®¶çš„ç‚¹ï¼š

1 The reference GRPO implementation uses the reverse KL divergence, not the forward KL divergence.

å‚è€ƒ GRPO å®ç°ä½¿ç”¨åå‘ KL æ•£åº¦ï¼ˆKL divergenceï¼‰ï¼Œè€Œä¸æ˜¯æ­£å‘ KL æ•£åº¦ã€‚

2 Naively implementing linear cross entropy on float16 mixed precision (and also float8) with automatic mixed precision scaling mechanisms will break if not handled properly.

å¦‚æœå¤„ç†ä¸å½“ï¼Œåœ¨å…·æœ‰è‡ªåŠ¨æ··åˆç²¾åº¦ç¼©æ”¾æœºåˆ¶ï¼ˆautomatic mixed precision scaling mechanismsï¼‰çš„ float16 æ··åˆç²¾åº¦ï¼ˆmixed precision)ï¼ˆä»¥åŠ float8ï¼‰ä¸Šç›´æ¥å®ç°çº¿æ€§äº¤å‰ç†µå°†ä¼šå¤±æ•ˆã€‚

3 We found other quirks in terms of the implementation of the GRPO loss - primarily in terms of the formulation of the reverse KL divergence.

æˆ‘ä»¬åœ¨ GRPO æŸå¤±çš„å®ç°æ–¹é¢å‘ç°äº†å…¶ä»–é—®é¢˜ï¼Œä¸»è¦æ˜¯åœ¨åå‘ KL æ•£åº¦çš„å…¬å¼æ–¹é¢ã€‚

### Maths of GRPO & Issues Found 

GRPO çš„æ•°å­¦åŸç†åŠç›¸å…³é—®é¢˜

GRPO was first introduced in [DeepSeekâ€™s Math paper](https://arxiv.org/abs/2402.03300)back in February 2024 to April 2024 DeepSeek then leveraged the GRPO algorithm in creating DeepSeek R1, as mentioned in their [paper](https://arxiv.org/abs/2501.12948). 

GRPO ç®—æ³•æœ€æ—©åœ¨ DeepSeek å‘è¡¨äº 2024 å¹´ 2 æœˆè‡³ 4 æœˆçš„ [æ•°å­¦è®ºæ–‡](https://arxiv.org/abs/2402.03300)ä¸­è¢«æå‡ºã€‚ä¹‹åï¼ŒDeepSeek åœ¨æ„å»º DeepSeek R1 æ—¶é‡‡ç”¨äº† GRPO ç®—æ³•ï¼Œç›¸å…³ç»†èŠ‚å¯ä»¥å‚è€ƒä»–ä»¬çš„ [è®ºæ–‡](https://arxiv.org/abs/2501.12948)ã€‚

[[2501.12948] DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)

We leverage Hugging Faceâ€™s TRL GRPO implementation [here](https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_trainer.py). We see that the TRL implementation performs: 

æˆ‘ä»¬ä½¿ç”¨äº† Hugging Face çš„ TRL åº“ä¸­ GRPO çš„å®ç°ï¼Œç›¸å…³ä»£ç å¯ä»¥åœ¨ [è¿™é‡Œ](https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_trainer.py)æ‰¾åˆ°ã€‚TRL çš„å®ç°è¿‡ç¨‹å¦‚ä¸‹ï¼š

L = \frac{1}{n} \sum \beta D_{KL} (q \parallel p) + A

where we utilize the **reverse KL divergence**(not the forward KL divergence). Beta is a scaling factor set to 0.04, and A is the advantages obtained after considering all reward functions.Q is the new trained model, and P is the original reference model. 

åœ¨è¿™ä¸ªå…¬å¼ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† **åå‘ KL æ•£åº¦**ï¼ˆReverse KL Divergenceï¼Œä¸æ˜¯å‰å‘ KL æ•£åº¦ï¼‰ã€‚å…¶ä¸­ï¼ŒBeta æ˜¯ä¸€ä¸ªç¼©æ”¾å› å­ï¼Œè®¾ç½®ä¸º 0.04ï¼›A ä»£è¡¨è€ƒè™‘æ‰€æœ‰å¥–åŠ±å‡½æ•°åè·å¾—çš„**ä¼˜åŠ¿ï¼ˆAdvantage)**ï¼Œå¯ä»¥ç†è§£ä¸ºæ¨¡å‹è¡¨ç°è¶…è¶ŠåŸºçº¿çš„ç¨‹åº¦ï¼›Q ä»£è¡¨æ–°è®­ç»ƒçš„æ¨¡å‹ï¼Œè€Œ P ä»£è¡¨åŸå§‹çš„å‚è€ƒæ¨¡å‹ã€‚

We then notice interestingly that the implementation calculates the reverse KL divergence as: 

ç„¶åæˆ‘ä»¬æœ‰è¶£åœ°æ³¨æ„åˆ°ï¼Œä»£ç å®ç°ä¸­åå‘ KL æ•£åº¦ï¼ˆKL divergenceï¼‰çš„è®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š

$$
\begin{aligned}
p &= \sigma(f(x)) \\
q &= \sigma(f'(x)) \\
D_{KL}(q \parallel p)_i &= \exp(\log(p) - \log(q)) - (\log(p) - \log(q)) - 1 \\
&= \exp \left( \log \left(\frac{p}{q}\right) \right) - \log \left(\frac{p}{q}\right) - 1 \\
&= \frac{p}{q} - \log \left( \frac{p}{q} \right) - 1
\end{aligned}
$$

But is this actually correct? We first try to derive it, and collect like terms: 

å…¶ä¸­ï¼Œ$D_{\text {KL}}\bigï¼ˆq \,\|\ï¼Œp \big)_i$ è¡¨ç¤ºç¬¬ $i$ ä¸ªå…ƒç´ çš„ KL æ•£åº¦ã€‚ä½†è¿™çœŸçš„æ˜¯æ­£ç¡®çš„å—ï¼Ÿæˆ‘ä»¬é¦–å…ˆå°è¯•æ¨å¯¼è¿™ä¸ªå…¬å¼ï¼Œå¹¶åˆå¹¶åŒç±»é¡¹ï¼š

$$
\begin{aligned}
D_{KL}(q \parallel p) &= \sum q \left[ \frac{p}{q} - \log\left( \frac{p}{q} \right) - 1 \right] \\
&= \sum q \frac{p}{q} - \sum q \log\left( \frac{p}{q} \right) - \sum q \\
&= \sum p - \sum q \log \left( \frac{p}{q} \right) - 1 \\
&= 1 - \sum q \log \left( \frac{p}{q} \right) - 1 \\
&= - \sum q \log \left( \frac{p}{q} \right) \\
D_{KL}(q \parallel p) &= \sum q \log \left( \frac{q}{p} \right)
\end{aligned}
$$

So what this means is that the implementation might be missing a multiplication by the Q (new distribution term)? 

ä¸Šé¢çš„æ¨å¯¼ä¸­ï¼Œåˆ©ç”¨äº†æ¦‚ç‡åˆ†å¸ƒçš„æ€§è´¨ï¼Œ$\sum q = 1$ ä»¥åŠ $\sum p = 1$ã€‚é‚£ä¹ˆï¼Œè¿™æ„å‘³ç€ä»£ç å®ç°å¯èƒ½ç¼ºå°‘ä¸ $Q$ï¼ˆæ–°åˆ†å¸ƒé¡¹ï¼Œå…·ä½“å«ä¹‰éœ€è¦æ ¹æ®ä¸Šä¸‹æ–‡ç¡®å®šï¼‰çš„ä¹˜æ³•å—ï¼Ÿ

But this seems to be correct as seen in the DeepSeek Math paper which first introduced GRPO on [page 14](https://arxiv.org/pdf/2402.03300). Likewise John [Schulman's blog](http://joschu.net/blog/kl-approx.html)also says that an unbiased estimator for the reverse KL term in fact does not need the extra Q term. We see in the blog that: 

ä½†è¿™ä¸ªå…¬å¼ä¼¼ä¹åˆæ˜¯æ­£ç¡®çš„ï¼Œæ­£å¦‚ DeepSeek Math è®ºæ–‡ä¸­æ‰€è¿°ï¼Œè¯¥è®ºæ–‡åœ¨ [ç¬¬ 14 é¡µ](https://arxiv.org/pdf/2402.03300)é¦–æ¬¡æå‡ºäº† GRPO ç®—æ³•ã€‚ç±»ä¼¼åœ°ï¼ŒJohn Schulman çš„åšå®¢(http://joschu.net/blog/kl-approx.html)ä¹ŸæŒ‡å‡ºï¼Œåå‘ KL æ•£åº¦çš„æ— åä¼°è®¡é‡å®é™…ä¸Šå¹¶ä¸éœ€è¦é¢å¤–çš„ Q é¡¹ã€‚æˆ‘ä»¬åœ¨è¯¥åšå®¢ä¸­çœ‹åˆ°ï¼š

$$
\begin{aligned}
r &= \frac{p(x)}{q(x)} \\
\text{KL}[q, p] &= (r - 1) - \log r \\
&= \frac{p}{q} - 1 - \log \frac{p}{q}
\end{aligned}
$$

We also found interestingly that: `torch.exp(q - q.detach()) * advantages.unsqueeze(1)`Is used, which should be evaluated to 1 right? 

æˆ‘ä»¬è¿˜å‘ç°æœ‰è¶£çš„æ˜¯ï¼Œä»£ç ä¸­ä½¿ç”¨äº† `torch.expï¼ˆq - q.detachï¼ˆ)ï¼‰* advantages.unsqueezeï¼ˆ1)`ï¼Œè¿™éƒ¨åˆ†ç†è®ºä¸Šåº”è¯¥ç­‰äº 1ï¼Œå¯¹å—ï¼Ÿ

We actually found this is necessary - it seems that the autograd engine might not be propagating gradients correctly. 

æˆ‘ä»¬å®é™…æµ‹è¯•åå‘ç°ï¼Œè¿™éƒ¨åˆ†ä»£ç æ˜¯å¿…è¦çš„ã€‚åŸå› æ˜¯ PyTorch çš„è‡ªåŠ¨æ±‚å¯¼å¼•æ“ï¼ˆautograd engineï¼‰å¯èƒ½æ²¡æœ‰æ­£ç¡®åœ°åå‘ä¼ æ’­æ¢¯åº¦ï¼ˆbackpropagation gradientsï¼‰ã€‚





So we perform 4 experiments: 
* Do normal GRPO via reference implementation (red line)
* Remove detach code (blue line)
* Full reverse KL with an extra term as discussed before (yellow line)
* Forward KL divergence instead (green line)

å› æ­¤ï¼Œæˆ‘ä»¬è¿›è¡Œäº† 4 ä¸ªå®éªŒï¼š
* é€šè¿‡å‚è€ƒå®ç°æ‰§è¡Œå¸¸è§„ GRPOï¼ˆGradient Ratio Policy Optimizationï¼Œæ¢¯åº¦æ¯”ä¾‹ç­–ç•¥ä¼˜åŒ–ï¼‰(çº¢çº¿)
* ç§»é™¤ç”¨äºåˆ‡æ–­æ¢¯åº¦åå‘ä¼ æ’­çš„ detach ä»£ç ï¼ˆè“çº¿)
* å®Œæ•´çš„åå‘ KL æ•£åº¦ï¼ˆKullback-Leibler divergenceï¼Œåº“å°”è´å…‹ - è±å¸ƒå‹’æ•£åº¦ï¼‰ï¼Œå¸¦æœ‰ä¸€ä¸ªé¢å¤–çš„é¡¹ï¼Œå¦‚ç¬¬ 3.2 èŠ‚è®¨è®ºçš„ï¼ˆé»„çº¿)
* ä½¿ç”¨å‰å‘ KL æ•£åº¦ï¼ˆKullback-Leibler divergenceï¼Œåº“å°”è´å…‹ - è±å¸ƒå‹’æ•£åº¦ï¼‰ä»£æ›¿ï¼ˆç»¿çº¿)

In general, removing detach definitely breaks all training, so we must leave it there - this will most likely need more investigation. It seems like all other implementations seem similar? We might need to run the model for longer to see different effects maybe. 

ä¸€èˆ¬æ¥è¯´ï¼Œç§»é™¤ detach æ“ä½œä¸€å®šä¼šå¯¼è‡´è®­ç»ƒå¤±è´¥ï¼Œæ‰€ä»¥å¿…é¡»ä¿ç•™å®ƒï¼Œä½†è¿™å¯èƒ½éœ€è¦æ›´æ·±å…¥çš„è°ƒæŸ¥ã€‚çœ‹èµ·æ¥å…¶ä»–çš„å®ç°æ–¹æ³•ä¹Ÿç±»ä¼¼ï¼Ÿ æˆ‘ä»¬å¯èƒ½éœ€è¦è¿è¡Œæ¨¡å‹æ›´é•¿æ—¶é—´ï¼Œæ‰èƒ½è§‚å¯Ÿåˆ°æ›´æ˜æ˜¾çš„æ•ˆæœã€‚

In all implementations, we also utilize the logsumexp trick: log â¡ Ïƒ â¡ ( x ) = log â¡ exp â¡ ( x ) âˆ‘ exp â¡ ( x ) = x âˆ’ log â¡ âˆ‘ exp â¡ ( x ) = x âˆ’ logsumexp â¡ ( x ) \begin{align}
\log\sigma(x) = \log{\frac{\exp(x)}{\sum{\exp(x)}}} &= x - \log\sum{\exp(x)} \\
&= x - \text{logsumexp}(x)
\end{align} ğŸ“ˆ Full Logging for GRPO We also provide full logging details for all reward functions now! Previously we only showed the total aggregated reward function itself. You also do not need to call functions to patch GRPO anymore! I.e. remove this at the top (we do it automatically): `from unsloth import PatchFastRL
PatchFastRL("GRPO", FastLanguageModel)`ğŸ–¥ï¸ vLLM inference options We also now allow you to use FP8 KV caches for vLLM, which allows for 2x less KV cache space usage on newer GPUs (RTX 3090, A100 and newer) `model, tokenizer = FastLanguageModel.from_pretrained(
 model_name = "meta-llama/meta-Llama-3.1-8B-Instruct",
 max_seq_length = max_seq_length,
 load_in_4bit = True, # False for LoRA 16bit
 fast_inference = True, # Enable vLLM fast inference
 max_lora_rank = lora_rank,
 gpu_memory_utilization = 0.6, # Reduce if out of memory
 float8_kv_cache = True, # Enable float8 KV cache
)`If you want to use min_p = 0.1, or other sampling params in vLLM, we also support passing anything in vLLMâ€™s SamplingParams arguments! `max_prompt_length = 256
from trl import GRPOConfig, GRPOTrainer
from unsloth import vLLMSamplingParams
vllm_sampling_params = vLLMSamplingParams(
 min_p = 0.1,
 seed = 3407,
 ...
)
training_args = GRPOConfig(
 ...
 vllm_sampling_params = vllm_sampling_params,
 temperature = 1.5,
)`âœ¨ Other Updates # ğŸ¦¥ Run Unsloth Dynamic 4-bit directly with vLLM
You can now run and do inference with our dynamic quants directly in vLLM. This was due to an [accepted PR](https://github.com/vllm-project/vllm/pull/12974)we did for the vLLM repo. Read how our dynamic quants greatly increase accuracy than standard 4-bit with examples and benchmarks [here](https://unsloth.ai/blog/dynamic-4bit). # ğŸš€ Run Perplexity's R1-1776
You also now download our [R1-1776 Dynamic GGUFs](https://huggingface.co/unsloth/r1-1776-GGUF)for Perplexity AIâ€™s new R1-1776 model which is a finetune of DeepSeek-R1 that removes all censorship whilst maintaining reasoning capabilities. Run them locally on your own device! # ğŸ± GitHub Universe Interview
In October during GitHub's 2024 Universe, we did a wonderful interview with Andrea and now the video is out! We talk about our backgrounds from Australia, how we built Unsloth, how amazing all of you are and more! [Watch on YouTube](https://www.youtube.com/watch?v=lyVxD0bJDOk)

åœ¨æ‰€æœ‰å®ç°ä¸­ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨äº† logsumexp æŠ€å·§ï¼š
\begin {align}
\log\sigmaï¼ˆxï¼‰= \log {\frac {\expï¼ˆx)}{\sum {\expï¼ˆx)}}} &= x - \log\sum {\expï¼ˆx)} \\
&= x - \text {logsumexp}(x)
\end {align}

ğŸ“ˆ GRPO å…¨é¢æ—¥å¿—è®°å½•ï¼šæˆ‘ä»¬ç°åœ¨ä¸ºæ‰€æœ‰å¥–åŠ±å‡½æ•°æä¾›æ›´è¯¦ç»†çš„æ—¥å¿—ä¿¡æ¯ã€‚ä¹‹å‰ï¼Œæˆ‘ä»¬åªå±•ç¤ºäº†æ€»ä½“çš„èšåˆå¥–åŠ±å‡½æ•°ã€‚ç°åœ¨ï¼Œä½ ä¸å†éœ€è¦æ‰‹åŠ¨è°ƒç”¨å‡½æ•°æ¥ä¿®è¡¥ GRPO äº†ï¼ç›¸å…³ä»£ç ï¼ˆä¾‹å¦‚ `from unsloth import PatchFastRL; PatchFastRLï¼ˆ"GRPOã€ï¼ŒFastLanguageModel)`ï¼‰å¯ä»¥ç§»é™¤ï¼Œæˆ‘ä»¬ä¼šè‡ªåŠ¨å¤„ç†ã€‚

ğŸ–¥ï¸ vLLM æ¨ç†ä¼˜åŒ–ï¼šæˆ‘ä»¬ç°åœ¨æ”¯æŒåœ¨ vLLM ä¸­ä½¿ç”¨ FP8 KV ç¼“å­˜ï¼ˆFP8 KV cacheï¼‰ï¼Œè¿™å¯ä»¥åœ¨è¾ƒæ–°çš„ GPUï¼ˆRTX 3090ã€A100 åŠæ›´æ–°å‹å·ï¼‰ä¸Šå‡å°‘ 2 å€çš„ KV ç¼“å­˜ç©ºé—´å ç”¨ã€‚ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š
```python
modelï¼Œtokenizer = FastLanguageModel.from_pretrainedï¼ˆ
 model_name =ã€Œmeta-llama/meta-Llama-3.1-8B-Instruct",
 max_seq_length = max_seq_length,
 load_in_4bit = Trueï¼Œ# False for LoRA 16bit
 fast_inference = Trueï¼Œ# Enable vLLM fast inference
 max_lora_rank = lora_rank,
 gpu_memory_utilization = 0.6ï¼Œ# Reduce if out of memory
 float8_kv_cache = Trueï¼Œ# Enable float8 KV cache
)
```å¦‚æœä½ æƒ³åœ¨ vLLM ä¸­ä½¿ç”¨`min_p = 0.1`æˆ–å…¶ä»–é‡‡æ ·å‚æ•°ï¼Œæˆ‘ä»¬ä¹Ÿæ”¯æŒç›´æ¥ä¼ é€’ vLLM çš„`SamplingParams`å‚æ•°æ¥å®ç°ï¼ç¤ºä¾‹ï¼š```python
max_prompt_length = 256
from trl import GRPOConfigï¼ŒGRPOTrainer
from unsloth import vLLMSamplingParams
vllm_sampling_params = vLLMSamplingParamsï¼ˆ
 min_p = 0.1,
 seed = 3407,
 ...
)
training_args = GRPOConfigï¼ˆ
 ...
 vllm_sampling_params = vllm_sampling_params,
 temperature = 1.5,
)
```

âœ¨ å…¶ä»–æ›´æ–°

*  ğŸ¦¥ ** ç›´æ¥é€šè¿‡ vLLM è¿è¡Œ Unsloth åŠ¨æ€ 4 ä½ï¼ˆUnsloth Dynamic 4-bit)**

  ç°åœ¨ï¼Œä½ å¯ä»¥ç›´æ¥åœ¨ vLLM ä¸­ä½¿ç”¨æˆ‘ä»¬çš„åŠ¨æ€é‡åŒ–æŠ€æœ¯è¿›è¡Œæ¨ç†ã€‚è¿™å¾—ç›Šäºæˆ‘ä»¬ä¸º vLLM ä»“åº“æäº¤çš„ [PR](https://github.com/vllm-project/vllm/pull/12974ï¼‰å¹¶å·²è¢«æ¥å—ã€‚æƒ³äº†è§£æˆ‘ä»¬çš„åŠ¨æ€é‡åŒ–æŠ€æœ¯å¦‚ä½•æ˜¾è‘—æé«˜ 4 ä½é‡åŒ–çš„ç²¾åº¦å—ï¼Ÿè¯·å‚è€ƒè¿™ç¯‡ [æ–‡ç« ](https://unsloth.ai/blog/dynamic-4bitï¼‰ï¼Œå†…å«ç¤ºä¾‹å’ŒåŸºå‡†æµ‹è¯•ã€‚

*  ğŸš€ ** è¿è¡Œ Perplexity AI çš„ R1-1776 æ¨¡å‹ **

  ç°åœ¨ï¼Œä½ å¯ä»¥ä¸‹è½½æˆ‘ä»¬æä¾›çš„ [R1-1776 Dynamic GGUFs](https://huggingface.co/unsloth/r1-1776-GGUFï¼‰ï¼Œç”¨äº Perplexity AI çš„æœ€æ–° R1-1776 æ¨¡å‹ã€‚è¯¥æ¨¡å‹åŸºäº DeepSeek-R1 è¿›è¡Œäº†å¾®è°ƒï¼Œç§»é™¤äº†æ‰€æœ‰å®¡æŸ¥æœºåˆ¶ï¼ŒåŒæ—¶ä¿ç•™äº†å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ã€‚å¿«åœ¨ä½ çš„è®¾å¤‡ä¸Šæœ¬åœ°è¿è¡Œä½“éªŒå§ï¼

*  ğŸ± **Unsloth å›¢é˜Ÿ GitHub Universe è®¿è°ˆ **

  åœ¨ 2024 å¹´ 10 æœˆä¸¾è¡Œçš„ GitHub Universe å¤§ä¼šä¸Šï¼Œæˆ‘ä»¬å’Œ Andrea è¿›è¡Œäº†ä¸€æ¬¡æ„‰å¿«çš„è®¿è°ˆï¼Œè§†é¢‘å·²ç»å‘å¸ƒï¼æˆ‘ä»¬èŠäº†èŠæ¥è‡ªæ¾³å¤§åˆ©äºšçš„èƒŒæ™¯ï¼ŒUnsloth çš„åˆ›å»ºå†ç¨‹ï¼Œä»¥åŠæˆ‘ä»¬å¯¹ç¤¾åŒºçš„æ„Ÿæ¿€ä¹‹æƒ…ï¼æ¬¢è¿ [è§‚çœ‹è§†é¢‘](https://www.youtube.com/watch?v=lyVxD0bJDOkï¼‰ã€‚
