## 20250210Unsloth-Re-introducing-Unsloth

[Re-introducing Unsloth](https://unsloth.ai/blog/reintroducing)

Feb 10, 2025

By Daniel & Michael

Today, weâ€™re the #1 trending repo on [GitHub](https://github.com/unslothai/unsloth)and it's all thanks to you! To celebrate, we want to take a moment to welcome anyone whether youâ€™ve been with us from the beginning or just discovered Unsloth! ğŸ’– 

ä»Šå¤©ï¼Œæˆ‘ä»¬è£ç™» [GitHub](https://github.com/unslothai/unsloth) çƒ­æ¦œç¬¬ä¸€åï¼Œè¿™éƒ½å½’åŠŸäºå¤§å®¶çš„æ”¯æŒï¼ä¸ºäº†åº†ç¥è¿™ä¸€æ—¶åˆ»ï¼Œæˆ‘ä»¬çƒ­çƒˆæ¬¢è¿æ¯ä¸€ä½æœ‹å‹ï¼Œæ— è®ºæ‚¨æ˜¯ä¸€ç›´ä»¥æ¥çš„å¿ å®ä¼™ä¼´ï¼Œè¿˜æ˜¯åˆšåˆšè®¤è¯† Unsloth çš„æ–°æœ‹å‹ï¼ 

But first, we wanted to thank each and everyone of you. Weâ€™re incredibly grateful for your support whether itâ€™s reading our blogs, engaging with us, or contributing to our repo or just using/sharing Unsloth. Your support truly means the world to us and we wouldn't be here today without you guys! 

ä½†é¦–å…ˆï¼Œæˆ‘ä»¬è¡·å¿ƒæ„Ÿè°¢æ¯ä¸€ä½æœ‹å‹ã€‚éå¸¸æ„Ÿè°¢å¤§å®¶ä¸€ç›´ä»¥æ¥çš„æ”¯æŒï¼Œæ— è®ºæ˜¯é˜…è¯»æˆ‘ä»¬çš„åšå®¢ã€ç§¯æäº’åŠ¨ã€ä¸ºæˆ‘ä»¬çš„ä»£ç ä»“åº“ï¼ˆrepoï¼‰åšå‡ºè´¡çŒ®ï¼Œè¿˜æ˜¯ä»…ä»…ä½¿ç”¨å’Œåˆ†äº« Unslothã€‚ä½ ä»¬çš„æ”¯æŒå¯¹æˆ‘ä»¬è‡³å…³é‡è¦ï¼Œæ²¡æœ‰å¤§å®¶çš„æ”¯æŒï¼Œå°±æ²¡æœ‰ Unsloth çš„ä»Šå¤©ï¼

Also, we're also blown away by how much you guys enjoyed our [1.58-bit Dynamic](https://unsloth.ai/blog/deepseekr1-dynamic)DeepSeek-R1 and our latest [Reasoning](https://unsloth.ai/blog/r1-reasoning)release. But this is just the beginning - weâ€™re not slowing down anytime soon! 

å¦å¤–ï¼Œå¤§å®¶å¯¹æˆ‘ä»¬çš„ [1.58-bit Dynamic](https://unsloth.ai/blog/deepseekr1-dynamic) DeepSeek-R1 ä»¥åŠæœ€æ–° [Reasoning](https://unsloth.ai/blog/r1-reasoning) ç‰ˆæœ¬çš„çƒ­çƒˆåå“ä¹Ÿè®©æˆ‘ä»¬å€æ„ŸæŒ¯å¥‹ã€‚ä½†è¿™ä»…ä»…æ˜¯ä¸€ä¸ªå¼€ç«¯ï¼Œæˆ‘ä»¬å‰è¿›çš„æ­¥ä¼ç»ä¸ä¼šåœæ­‡ï¼

[Run DeepSeek-R1 Dynamic 1.58-bit](https://unsloth.ai/blog/deepseekr1-dynamic)

[Train your own R1 reasoning model locally (GRPO)](https://unsloth.ai/blog/r1-reasoning)

Here's a little bit about our journey so far: 

ä»¥ä¸‹æ˜¯å…³äºæˆ‘ä»¬ç›®å‰ä¸ºæ­¢çš„æ—…ç¨‹çš„ä¸€ç‚¹ä»‹ç»ï¼š

### Unsloth Highlights 

Unsloth äº®ç‚¹

1 Dec 2023: We launched Unsloth, making training 2x faster and 50% more memory-efficient. We built a custom autograd engine, rewrote all kernels in OpenAI's Triton, including the RoPE embeddings with optimized forward & backward passes and had our own custom Fast Cross Entropy Loss kernel.

2023 å¹´ 12 æœˆï¼šæˆ‘ä»¬æ¨å‡ºäº† Unslothï¼Œä½¿è®­ç»ƒé€Ÿåº¦æé«˜ 2 å€ï¼Œå†…å­˜æ•ˆç‡æé«˜ 50%ã€‚æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªè‡ªå®šä¹‰çš„è‡ªåŠ¨å¾®åˆ†å¼•æ“ï¼ˆautograd engineï¼‰ï¼Œä½¿ç”¨ OpenAI çš„ Triton é‡å†™äº†æ‰€æœ‰å†…æ ¸ï¼ŒåŒ…æ‹¬å…·æœ‰ä¼˜åŒ–æ­£å‘å’Œåå‘ä¼ é€’çš„ RoPEï¼ˆRotary Position Embeddingï¼‰åµŒå…¥ï¼Œå¹¶æ‹¥æœ‰æˆ‘ä»¬è‡ªå·±çš„è‡ªå®šä¹‰å¿«é€Ÿäº¤å‰ç†µæŸå¤±ï¼ˆFast Cross Entropy Lossï¼‰å†…æ ¸ã€‚

[Unsloth update: Mistral support + more](https://unsloth.ai/blog/mistral-benchmark#Breakdown)

2 Jan 2024: We started uploading models to Hugging Face which has been downloaded by you amazing people millions of times! We also collabed with HF and are so thankful for their continued support. 

2024 å¹´ 1 æœˆï¼šæˆ‘ä»¬å¼€å§‹å°†æ¨¡å‹ä¸Šä¼ åˆ° Hugging Faceï¼ˆHFï¼‰ï¼Œè¿™äº›æ¨¡å‹å·²è¢«ä½ ä»¬ä¸‹è½½äº†æ•°ç™¾ä¸‡æ¬¡ï¼æˆ‘ä»¬è¿˜ä¸ HF åˆä½œï¼Œéå¸¸æ„Ÿè°¢ä»–ä»¬æŒç»­çš„æ”¯æŒã€‚

[unsloth (Unsloth AI)](https://huggingface.co/unsloth)

[Make LLM Fine-tuning 2x faster with Unsloth and ğŸ¤— TRL](https://huggingface.co/blog/unsloth-trl)

3 Mar 2024: We found & fixed 8 bugs in Google's incredible Gemma models. Thank you Andrej Karpathy for acknowledging our work and the Google team for their support!

2024 å¹´ 3 æœˆï¼šæˆ‘ä»¬å‘ç°äº†å¹¶ä¿®å¤äº† Google çš„ Gemma æ¨¡å‹ä¸­çš„ 8 ä¸ªé”™è¯¯ã€‚æ„Ÿè°¢ Andrej Karpathy å¯¹æˆ‘ä»¬å·¥ä½œçš„è®¤å¯ï¼Œä»¥åŠ Google å›¢é˜Ÿçš„æ”¯æŒï¼

[Unsloth Fixing Gemma bugs](https://unsloth.ai/blog/gemma-bugs)

[(1) X ä¸Šçš„ Andrej Karpathyï¼šâ€œBeautiful work / attention to detail trying to get Gemma to finetune correctly. There are so many foot guns here to be super careful with. All of these issues don't throw any errors, they silently make your network worse. A great example of what I wrote about in my "A Recipe forâ€ / X](https://x.com/karpathy/status/1765473722985771335)

4 Apr 2024: We introduced Unsloth Gradient Checkpointing which allowed for 4x longer context lengths and 1.7x larger batch sizes. The epic Llama 3 models were released and we enabled the 70B model fit on 48GB of VRAM.

2024 å¹´ 4 æœˆï¼šæˆ‘ä»¬æ¨å‡ºäº† Unsloth æ¢¯åº¦æ£€æŸ¥æŠ€æœ¯ï¼ˆUnsloth Gradient Checkpointingï¼‰ï¼Œå®ƒå…è®¸ 4 å€æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦å’Œ 1.7 å€æ›´å¤§çš„æ‰¹é‡å¤§å°ã€‚å²è¯—çº§çš„ Llama 3 æ¨¡å‹å‘å¸ƒï¼Œæˆ‘ä»¬å®ç°äº†åœ¨ 48GB æ˜¾å­˜ï¼ˆVRAMï¼‰ä¸Šå¾®è°ƒ 70B å‚æ•°çš„ Llama 3 æ¨¡å‹ã€‚

[Unsloth Gradient Checkpointing - 4x longer context windows](https://unsloth.ai/blog/long-context)

[Finetune Llama 3 with Unsloth](https://unsloth.ai/blog/llama3)

5 May 2024: We found & fixed many bugs in Llama 3 which greatly improved accuracy for training. The amazing Phi-3 model was released and we Llamafied it.

2024 å¹´ 5 æœˆï¼šæˆ‘ä»¬å‘ç°äº†å¹¶ä¿®å¤äº† Llama 3 ä¸­çš„è®¸å¤šé”™è¯¯ï¼Œè¿™å¤§å¤§æé«˜äº†è®­ç»ƒçš„å‡†ç¡®æ€§ã€‚ä»¤äººæƒŠå¹çš„ Phi-3 æ¨¡å‹å‘å¸ƒï¼Œæˆ‘ä»¬ä½¿å…¶å…¼å®¹ Llama çš„æ¶æ„ã€‚

[Finetune Phi-3 with Unsloth](https://unsloth.ai/blog/phi3)

6 Jul 2024: We fixed many bugs in Gemma 2 and introduced fine-tuning with CSV/Excel files and multiple columns.

2024 å¹´ 7 æœˆï¼šæˆ‘ä»¬ä¿®å¤äº† Gemma 2 ä¸­çš„è®¸å¤šé”™è¯¯ï¼Œå¹¶æ¨å‡ºäº†ä½¿ç”¨ CSV/Excel æ–‡ä»¶å’Œå¤šåˆ—è¿›è¡Œå¾®è°ƒçš„åŠŸèƒ½ã€‚

[Finetune Gemma 2 with Unsloth](https://unsloth.ai/blog/gemma2)

[Finetune Mistral NeMo with Unsloth](https://unsloth.ai/blog/mistral-nemo)

7 Oct 2024: We fixed a universal issue in Gradient Accumulation that negatively impacted everyone's training runs (not just for Unsloth but everyone who was training models).

2024 å¹´ 10 æœˆï¼šæˆ‘ä»¬ä¿®å¤äº†æ¢¯åº¦ç´¯ç§¯ï¼ˆGradient Accumulationï¼‰ä¸­çš„ä¸€ä¸ªæ™®éé—®é¢˜ï¼Œè¯¥é—®é¢˜å¯¹æ‰€æœ‰æ¨¡å‹çš„è®­ç»ƒéƒ½äº§ç”Ÿäº†è´Ÿé¢å½±å“ã€‚

[Bug Fixes in LLM Training - Gradient Accumulation](https://unsloth.ai/blog/gradient)

8 Nov 2024: 2x faster & 70% less VRAM Vision fine-tuning with Llama, Qwen, Pixtral and Llava support. We also found & fixed many Qwen 2.5 bugs.

2024 å¹´ 11 æœˆï¼šä½¿ç”¨ Llamaã€Qwenã€Pixtral å’Œ Llava æ”¯æŒï¼Œè§†è§‰å¾®è°ƒçš„é€Ÿåº¦æé«˜ 2 å€ï¼Œæ˜¾å­˜å ç”¨å‡å°‘ 70%ã€‚æˆ‘ä»¬è¿˜å‘ç°äº†å¹¶ä¿®å¤äº†è®¸å¤š Qwen 2.5 é”™è¯¯ã€‚

[Llama 3.2 Vision Fine-tuning with Unsloth](https://unsloth.ai/blog/vision)

[(1) X ä¸Šçš„ Daniel Hanï¼šâ€œBug fixes &amp; analysis for Qwen 2.5: 1. Pad\_token should NOT be &lt;|endoftext|&gt; Inf gens 2. Base &lt;|im\_start|&gt; &lt;|im\_end|&gt; are untrained 3. PCA on embeddings has a BPE hierarchy 4. YaRN 128K extended context from 32B 5. Fixed versions + 128K GGUFs: https://t.co/DY3NvI2F2o Details: https://t.co/B8ogqObC0gâ€ / X](https://x.com/danielhanchen/status/1856442699689414970)

9 Dec 2024: We introduced our Unsloth 4-bit Dynamic Quants which greatly increased accuracy compared to standard 4-bit by selectively avoiding quantizing certain layers. And we added support for 13x longer context lengths for Llama 3.3.

2024 å¹´ 12 æœˆï¼šæˆ‘ä»¬æ¨å‡ºäº† Unsloth 4 æ¯”ç‰¹åŠ¨æ€é‡åŒ–æŠ€æœ¯ï¼ˆUnsloth 4-bit Dynamic Quantsï¼‰ï¼Œé€šè¿‡é€‰æ‹©æ€§åœ°é¿å…é‡åŒ–æŸäº›å±‚ï¼Œä¸æ ‡å‡† 4 æ¯”ç‰¹é‡åŒ–ç›¸æ¯”ï¼Œå¤§å¤§æé«˜äº†å‡†ç¡®æ€§ã€‚å¹¶ä¸”æˆ‘ä»¬å¢åŠ äº†å¯¹ Llama 3.3 çš„ 13 å€æ›´é•¿ä¸Šä¸‹æ–‡é•¿åº¦çš„æ”¯æŒã€‚

[Unsloth - Dynamic 4-bit Quantization](https://unsloth.ai/blog/dynamic-4bit)

[Fine-tune Llama 3.3 with Unsloth](https://unsloth.ai/blog/llama3-3)

10 Jan 2025: We released our 1.58-bit Dynamic R1 GGUF allowing you guys to run R1 on your local device and found & fixed many bugs in Phi-4.

2025 å¹´ 1 æœˆï¼šæˆ‘ä»¬å‘å¸ƒäº†æˆ‘ä»¬çš„ 1.58-bit Dynamic R1 GGUFï¼Œå…è®¸ç”¨æˆ·åœ¨æœ¬åœ°è®¾å¤‡ä¸Šè¿è¡Œ R1ï¼Œå¹¶å‘ç°äº†å¹¶ä¿®å¤äº† Phi-4 ä¸­çš„è®¸å¤šé”™è¯¯ã€‚

11 Feb 2025: We introduced Reasoning in Unsloth and made GRPO work with QLoRA & LoRA, which previously did not work - allowing home users to train their own reasoning model like R1.

2025 å¹´ 2 æœˆï¼šæˆ‘ä»¬åœ¨ Unsloth ä¸­å¼•å…¥äº†æ¨ç†èƒ½åŠ›ï¼ˆReasoningï¼‰ï¼Œå¹¶ä½¿ GRPO ä¸ QLoRA å’Œ LoRA é…åˆä½¿ç”¨ï¼Œè¿™åœ¨ä»¥å‰æ˜¯è¡Œä¸é€šçš„ï¼Œä»è€Œå…è®¸ç”¨æˆ·è®­ç»ƒä»–ä»¬è‡ªå·±çš„æ¨ç†æ¨¡å‹ï¼Œä¾‹å¦‚ R1ã€‚

[Train your own R1 reasoning model locally (GRPO)](https://unsloth.ai/blog/r1-reasoning)

### What's Next? 

æœªæ¥å±•æœ›

We're always trying to innovate and won't stop just yet. This year, you can expect even more exciting new cool stuff that will make creating your own custom models much easier and more accessible. 

æˆ‘ä»¬å§‹ç»ˆè‡´åŠ›äºåˆ›æ–°ï¼Œæ¢ç´¢çš„è„šæ­¥æ°¸ä¸åœæ­‡ã€‚ä»Šå¹´ï¼Œæˆ‘ä»¬å°†æ¨å‡ºæ›´å¤šä»¤äººæœŸå¾…çš„å…¨æ–°åŠŸèƒ½ï¼Œè®©å®šåˆ¶æ‚¨ä¸“å±çš„æ¨¡å‹å˜å¾—å‰æ‰€æœªæœ‰çš„è½»æ¾ä¾¿æ·ã€‚

And of course - multiGPU & Unsloth Studio are still on the way so don't worry - but we've got to support all models first which is coming pretty soon. Our goal is first and foremost open-source and Unslothing things for you guys, and nothing will ever change that so we hope you follow along this year for even more cool stuff!

å½“ç„¶ï¼ŒmultiGPU å’Œ Unsloth Studio ä»åœ¨å¼€å‘ä¸­ï¼Œè¯·ä¸å¿…æ‹…å¿ƒã€‚æˆ‘ä»¬é¦–è¦çš„ä»»åŠ¡æ˜¯æ”¯æŒæ‰€æœ‰æ¨¡å‹ï¼Œè¿™é¡¹å·¥ä½œå¾ˆå¿«å°±ä¼šå®Œæˆã€‚æˆ‘ä»¬å§‹ç»ˆåšæŒå¼€æºï¼Œå¹¶è‡´åŠ›äºä¸ºå¤§å®¶ç®€åŒ– Unsloth çš„ç›¸å…³æµç¨‹ã€‚è¿™ä¸€ç‚¹æ°¸è¿œä¸ä¼šæ”¹å˜ï¼Œæ‰€ä»¥å¸Œæœ›å¤§å®¶ä»Šå¹´ç»§ç»­å…³æ³¨æˆ‘ä»¬ï¼ŒæœŸå¾…æ›´å¤šç²¾å½©çš„å†…å®¹ï¼

### å‚è€ƒ

Andrej Karpathy
@karpathy

Beautiful work / attention to detail trying to get Gemma to finetune correctly. There are so many foot guns here to be super careful with. All of these issues don't throw any errors, they silently make your network worse.

A great example of what I wrote about in my "A Recipe for Training Neural Networks":
"""The "fast and furious" approach to training neural networks does not work and only leads to suffering. Now, suffering is a perfectly natural part of getting a neural network to work well, but it can be mitigated by being thorough, defensive, paranoid, and obsessed with visualizations of basically every possible thing. The qualities that in my experience correlate most strongly to success in deep learning are patience and attention to detail."""

And why I so emphasize the need for understanding all the parts of the deep learning stack in great detail. I exist in a perpetually terrified state of the remaining 20 silent bugs that certainly remain in my code.

ä½ åœ¨åŠªåŠ›è®© Gemma æ­£ç¡®å¾®è°ƒæ—¶å±•ç°å‡ºçš„ç»†èŠ‚æŠŠæ§ä»¤äººèµå¹ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸­å­˜åœ¨è®¸å¤šçœ‹ä¼¼æ— å®³å´æå…·å¨èƒçš„éšæ‚£ï¼Œæœ€å±é™©çš„æ˜¯è¿™äº›é—®é¢˜ä¸ä¼šäº§ç”Ÿä»»ä½•é”™è¯¯æç¤ºï¼Œè€Œæ˜¯ä¼šæ‚„æ— å£°æ¯åœ°é™ä½æ¨¡å‹æ€§èƒ½ã€‚

è¿™æ­£å¥½å°è¯äº†æˆ‘åœ¨ã€Šè®­ç»ƒç¥ç»ç½‘ç»œçš„ç§˜è¯€ã€‹ä¸­æ‰€å†™çš„ï¼š

"""åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œé‡‡ç”¨ ' å¿«é€Ÿå†’è¿› ' çš„æ–¹æ³•æ³¨å®šå¤±è´¥ï¼Œåªä¼šå¾’å¢ç—›è‹¦ã€‚è¯šç„¶ï¼Œé­é‡æŒ«æŠ˜æ˜¯è®©ç¥ç»ç½‘ç»œè¾¾åˆ°ç†æƒ³æ•ˆæœè¿‡ç¨‹ä¸­ä¸å¯é¿å…çš„ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡å…¨é¢ä¸¥è°¨çš„æ€åº¦ã€é˜²å¾¡æ€§çš„ç¼–ç¨‹æ€ç»´ã€æè‡´çš„æ±‚è¯ç²¾ç¥ï¼Œä»¥åŠå¯¹æ‰€æœ‰å¯èƒ½æƒ…å†µçš„å¯è§†åŒ–åˆ†ææ¥å‡å°‘è¿™ç§ç—›è‹¦ã€‚åŸºäºæˆ‘çš„ç»éªŒï¼Œåœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸï¼Œæœ€èƒ½å¸¦æ¥æˆåŠŸçš„å“è´¨æ˜¯è€å¿ƒå’Œå¯¹ç»†èŠ‚çš„ä¸“æ³¨ã€‚"""

è¿™ä¹Ÿè§£é‡Šäº†ä¸ºä»€ä¹ˆæˆ‘ä¸€ç›´å¼ºè°ƒéœ€è¦æ·±å…¥ç†è§£æ·±åº¦å­¦ä¹ æŠ€æœ¯æ ˆï¼ˆdeep learning stackï¼‰çš„æ¯ä¸ªç»„æˆéƒ¨åˆ†ã€‚å³ä¾¿å¦‚æ­¤ï¼Œæˆ‘ä»ç„¶æ—¶åˆ»æ‹…å¿§ç€ä»£ç ä¸­é‚£äº›æ½œä¼ç€çš„ã€ä¸ä¼šæŠ¥é”™ä½†ä¼šé»˜é»˜å½±å“æ€§èƒ½çš„ 20 ä¸ªé—®é¢˜ã€‚

---

Daniel Han
@danielhanchen

Bug fixes & analysis for Qwen 2.5:

1. Pad_token should NOT be <|endoftext|> Inf gens
2. Base <|im_start|> <|im_end|> are untrained
3. PCA on embeddings has a BPE hierarchy 
4. YaRN 128K extended context from 32B
5. Fixed versions + 128K GGUFs: http://huggingface.co/unsloth

Details:
1. Pad token bug - for finetuning, never use pad_token = EOS - this will result in infinite generations since finetuning will ignore them. Base model also has a chat template - remove this. 
@UnslothAI
 versions fixed them

2. Untrained tokens issues. Do NOT use the Qwen 2.5 chat template for the base version - <|im_start|>, <|im_end|> are untrained since Norm(<im_x>, pad_token) is close to 0. Instruct version have them trained.

3. PCA on embeddings for Base and Instruct show a BPE hierarchy. Less frequent tokens are obvious since they're ordered by ID. PCA shows <|im_x|> moving away from being untrained. Same phenomenon for Llama & more models.

4.  Uploaded native 128K extended YaRN GGUFs for Coder 0.5B all the way until 32B to https://huggingface.co/collections/unsloth/qwen-25-coder-all-versions-6732bc833ed65dd1964994d4. Use the 128K version for long contexts. Use the 32B native version for general chats.

Also, Unsloth can finetune 14B in a free Colab! Conversational style finetuning: https://colab.research.google.com/drive/18sN803sU23XuJV9Q8On2xgqHSer6-UZF?usp=sharing
Kaggle 14B notebook: https://kaggle.com/code/danielhanchen/kaggle-qwen-2-5-coder-14b-conversational

Unsloth can also finetune the 72B variants in a 48GB card!

Qwen 2.5 çš„é—®é¢˜ä¿®å¤å’Œåˆ†æï¼š

1. å¡«å……è¯ç¬¦ (pad_token) ä¸åº”è®¾ä¸ºæ–‡æœ¬ç»“æŸç¬¦ (<|endoftext|>)ï¼Œå¦åˆ™ä¼šå¯¼è‡´æ¨¡å‹æ— é™è¾“å‡º
2. åŸºç¡€ç‰ˆæœ¬ä¸­çš„å¯¹è¯æ ‡è®° <|im_start|> å’Œ <|im_end|> æœªç»è¿‡è®­ç»ƒ
3. å¯¹è¯åµŒå…¥è¿›è¡Œä¸»æˆåˆ†åˆ†æ (PCA) å‘ç°äº†å­—èŠ‚å¯¹ç¼–ç  (BPE) çš„å±‚çº§ç‰¹å¾
4. ä½¿ç”¨ YaRN æŠ€æœ¯å°†ä¸Šä¸‹æ–‡é•¿åº¦ä» 32B æ‰©å±•åˆ° 128K
5. ä¿®å¤ç‰ˆæœ¬å’Œ 128K GGUF æ ¼å¼æ–‡ä»¶å·²ä¸Šä¼ è‡³ï¼šhttp://huggingface.co/unsloth

è¯¦ç»†è¯´æ˜ï¼š

1. å¡«å……è¯ç¬¦é—®é¢˜ - åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œç»ä¸è¦å°†å¡«å……è¯ç¬¦è®¾ç½®ä¸ºç»“æŸç¬¦ (EOS)ï¼Œè¿™ä¼šå¯¼è‡´æ¨¡å‹æŒç»­ä¸æ–­åœ°ç”Ÿæˆå†…å®¹ï¼Œå› ä¸ºå¾®è°ƒè¿‡ç¨‹ä¼šå¿½ç•¥è¿™äº›æ ‡è®°ã€‚åŸºç¡€æ¨¡å‹ä¸­çš„å¯¹è¯æ¨¡æ¿ä¹Ÿéœ€è¦ç§»é™¤ã€‚@UnslothAI çš„ç‰ˆæœ¬å·²ä¿®å¤è¿™äº›é—®é¢˜ã€‚

2. æœªè®­ç»ƒè¯ç¬¦é—®é¢˜ã€‚åŸºç¡€ç‰ˆæœ¬ä¸è¦ä½¿ç”¨ Qwen 2.5 çš„å¯¹è¯æ¨¡æ¿ - å› ä¸º <|im_start|> å’Œ <|im_end|> è¿™äº›æ ‡è®°æœªç»è®­ç»ƒï¼ˆå…¶ä¸å¡«å……è¯ç¬¦çš„èŒƒæ•°æ¥è¿‘äº 0ï¼‰ã€‚è¿™äº›æ ‡è®°åªåœ¨æŒ‡ä»¤ç‰ˆæœ¬ä¸­ç»è¿‡äº†è®­ç»ƒã€‚

3. å¯¹åŸºç¡€ç‰ˆå’ŒæŒ‡ä»¤ç‰ˆçš„è¯åµŒå…¥è¿›è¡Œä¸»æˆåˆ†åˆ†æï¼Œæ­ç¤ºäº†å­—èŠ‚å¯¹ç¼–ç çš„å±‚çº§ç»“æ„ã€‚ç”±äºä¸å¸¸ç”¨çš„è¯ç¬¦æŒ‰ ID æ’åºï¼Œå› æ­¤å¾ˆå®¹æ˜“è¯†åˆ«ã€‚åˆ†ææ˜¾ç¤ºå¯¹è¯æ ‡è®° <|im_x|> æ­£é€æ¸è·å¾—æœ‰æ„ä¹‰çš„è¡¨ç¤ºã€‚è¿™ç§ç°è±¡åœ¨ Llama ç­‰å…¶ä»–æ¨¡å‹ä¸­ä¹ŸåŒæ ·å­˜åœ¨ã€‚

4. æˆ‘ä»¬å·²å°†æ”¯æŒ 128K é•¿åº¦çš„ YaRN ç‰ˆæœ¬ GGUF æ–‡ä»¶ï¼ˆåŒ…æ‹¬ä» 0.5B åˆ° 32B çš„ Coder ç‰ˆæœ¬ï¼‰ä¸Šä¼ è‡³ Hugging Faceã€‚å»ºè®®åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶ä½¿ç”¨ 128K ç‰ˆæœ¬ï¼Œæ—¥å¸¸å¯¹è¯åˆ™ä½¿ç”¨ 32B åŸç”Ÿç‰ˆæœ¬ã€‚

ç‰¹åˆ«æé†’ï¼šUnsloth ç°åœ¨æ”¯æŒåœ¨å…è´¹ç‰ˆ Colab ä¸Šå¾®è°ƒ 140 äº¿å‚æ•°çš„æ¨¡å‹ï¼å¯¹è¯å¼å¾®è°ƒæ•™ç¨‹å·²ä¸Šçº¿ã€‚åŒæ—¶åœ¨ Kaggle ä¸Šä¹Ÿæä¾›äº† 140 äº¿å‚æ•°æ¨¡å‹çš„è®­ç»ƒç¤ºä¾‹ã€‚

æ›´ä»¤äººæƒŠå–œçš„æ˜¯ï¼ŒUnsloth ç”šè‡³èƒ½å¤Ÿåœ¨é…å¤‡ 48GB æ˜¾å­˜çš„æ˜¾å¡ä¸Šå¾®è°ƒ 720 äº¿å‚æ•°çš„æ¨¡å‹å˜ä½“ï¼