## 20250211Langchain-Upgraded-DeepSeek-R1-RAG-Just-Revolutionized-AI-Forever

[Langchain (Upgraded) + DeepSeek-R1 + RAG Just Revolutionized AI Forever | by Gao Dalie (é«˜é”çƒˆ) | Jan, 2025 | Towards AI](https://pub.towardsai.net/langchain-upgraded-deepseek-r1-rag-just-revolutionized-ai-forever-27dcbb0e3493)

Gao Dalie (é«˜é”çƒˆ)

Towards AI

Jan 29, 2025

Last week, I made a video about DeepSeek-V3, and it caused a huge stir in the global AI community.

ä¸Šå‘¨ï¼Œæˆ‘åˆ¶ä½œäº†ä¸€ä¸ªå…³äº DeepSeek-V3 çš„è§†é¢‘ï¼Œåœ¨å…¨çƒ AI ç¤¾åŒºå¼•èµ·äº†å¾ˆå¤§çš„åå“ã€‚

Two Days ago, a Chinese DeepSeek released the inference-based large-scale language model â€œDeepSeek-R1â€ as open source!

ä¸¤å¤©å‰ï¼Œä¸­å›½çš„ DeepSeek å…¬å¸å¼€æºäº†åŸºäºæ¨ç†çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼ŒLLM)ã€ŒDeepSeek-R1ã€ï¼

Itâ€™s said to perform just as well as OpenAIâ€™s most accurate inference model, â€˜o1.â€™ Whatâ€™s even more impressive is that It is an incredibly price-breaking model with an API price of less than 1/25 of OpenAI o1. On top of that, itâ€™s been open-sourced under the highly flexible MIT license, so anyone can download and use the model

æ®ç§°ï¼Œå®ƒçš„æ€§èƒ½ä¸ OpenAI æœ€ç²¾ç¡®çš„æ¨ç†æ¨¡å‹ 'o1' ä¸ç›¸ä¸Šä¸‹ã€‚æ›´ä»¤äººå°è±¡æ·±åˆ»çš„æ˜¯ï¼Œå®ƒæ˜¯ä¸€æ¬¾æå…·æ€§ä»·æ¯”çš„æ¨¡å‹ï¼ŒAPI ä»·æ ¼ä»…ä¸º OpenAI o1 çš„ 1/25ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå®ƒå·²é‡‡ç”¨æå…¶çµæ´»çš„ MIT è®¸å¯è¯å¼€æºï¼Œä»»ä½•äººéƒ½å¯ä»¥ä¸‹è½½å¹¶ä½¿ç”¨è¯¥æ¨¡å‹ã€‚

As soon as the R1 model came out this time, it not only refuted the previous statement of distilling OpenAI o1, but the official also directly said: â€œWe can tie with the open source version of o1.â€

è¿™æ¬¡ R1 æ¨¡å‹ä¸€ç»å‘å¸ƒï¼Œä¸ä»…é©³æ–¥äº†ä¹‹å‰å…³äºä» OpenAI çš„ o1 æ¨¡å‹ä¸­è¿›è¡ŒçŸ¥è¯†æç‚¼çš„è¯´æ³•ï¼Œè€Œä¸”å®˜æ–¹è¿˜ç›´æ¥å®£ç§°ï¼šã€Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ä¸ o1 æ¨¡å‹çš„å¼€æºç‰ˆæœ¬æ€§èƒ½ç›¸å½“ã€‚ã€

It is worth mentioning that R1 breaks through the previous model training methods and does not use any SFT data at all. It only trains the model through pure RL. This shows that R1 has learned to think about problems by itself â€” which is actually more in line with human thinking. rule.

å€¼å¾—ä¸€æçš„æ˜¯ï¼ŒR1 çªç ´äº†ä»¥å¾€çš„æ¨¡å‹è®­ç»ƒæ–¹æ³•ï¼Œå®Œå…¨æ²¡æœ‰ä½¿ç”¨ä»»ä½•æœ‰ç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼ŒSFTï¼‰æ•°æ®ï¼Œè€Œä»…ä»…é€šè¿‡çº¯å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼ŒRLï¼‰æ¥è®­ç»ƒæ¨¡å‹ã€‚è¿™è¡¨æ˜ R1 å·²ç»å­¦ä¼šç‹¬ç«‹æ€è€ƒï¼Œè€Œè¿™ç§æ–¹å¼å®é™…ä¸Šæ›´åŠ è´´è¿‘äººç±»çš„æ€ç»´æ¨¡å¼ã€‚

While I wanted to develop the RAG chatbot, I found that LangChain made huge updates before I developed the RAG chatbot with it. Unfortunately, it does not remember the conversation content like ChatGPT, and it also cannot input new data or be trained to answer specific questions like a customer service chatbot

è™½ç„¶æˆ‘è®¡åˆ’å¼€å‘ RAG èŠå¤©æœºå™¨äººï¼Œä½†å‘ç° LangChain åœ¨æˆ‘å¼€å§‹ä¹‹å‰å°±ç»å†äº†é‡å¤§æ›´æ–°ã€‚é—æ†¾çš„æ˜¯ï¼Œå®ƒä¸å…·å¤‡åƒ ChatGPT é‚£æ ·çš„å¯¹è¯è®°å¿†åŠŸèƒ½ï¼Œä¹Ÿæ— æ³•åƒå®¢æœèŠå¤©æœºå™¨äººé‚£æ ·å¯¼å…¥æ–°æ•°æ®æˆ–é€šè¿‡è®­ç»ƒæ¥å›ç­”ç‰¹å®šé—®é¢˜ã€‚

So, Let me give you a quick demo of a live chatbot to show you what I mean.

æ¥ä¸‹æ¥ï¼Œæˆ‘å°†å¿«é€Ÿæ¼”ç¤ºä¸€ä¸ªå®æ—¶èŠå¤©æœºå™¨äººï¼Œä»¥ä¾¿æ›´ç›´è§‚åœ°è¯´æ˜æˆ‘çš„æƒ³æ³•ã€‚

I will upload a PDF that includes images and then ask the chatbot a question: â€˜Summarize this PDF.â€™ Feel free to ask any questions you want. If you look at how the chatbot generates the output, you will see that the PDF file stores its content in a temporary file, processes it with PDFPlumberLoader to extract complex data like tables, and cleans up the temporary file.

æˆ‘ä¼šä¸Šä¼ ä¸€ä¸ªåŒ…å«å›¾åƒçš„ PDF æ–‡æ¡£ï¼Œå¹¶å‘èŠå¤©æœºå™¨äººæå‡ºä¸€ä¸ªé—®é¢˜ï¼šã€Œæ€»ç»“è¿™ä»½ PDF æ–‡æ¡£ã€ã€‚æ‚¨å¯ä»¥éšæ—¶æå‡ºä»»ä½•é—®é¢˜ã€‚è§‚å¯ŸèŠå¤©æœºå™¨äººç”Ÿæˆç»“æœçš„è¿‡ç¨‹ï¼Œæ‚¨ä¼šå‘ç°ï¼ŒPDF æ–‡ä»¶é¦–å…ˆå°†è‡ªèº«å†…å®¹å­˜å‚¨åœ¨ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ä¸­ï¼Œç„¶ååˆ©ç”¨ PDFPlumberLoader å¯¹å…¶è¿›è¡Œå¤„ç†ï¼Œæå–å…¶ä¸­çš„è¡¨æ ¼ç­‰å¤æ‚ç»“æ„åŒ–æ•°æ®ï¼Œæœ€åæ¸…ç†è¯¥ä¸´æ—¶æ–‡ä»¶ã€‚

It uses SemanticChunker to split documents into semantic chunks and FAISS for efficient similarity-based search. The retriever finds the top 3 most similar chunks for a query and uses history_aware_retriever to enhance the agentâ€™s ability to incorporate the entire conversation history into the retrieval process.

å®ƒä½¿ç”¨ SemanticChunker å°†æ–‡æ¡£åˆ†å‰²æˆè¯­ä¹‰åˆ†å—ï¼Œå¹¶ä½¿ç”¨ FAISS è¿›è¡Œé«˜æ•ˆçš„åŸºäºç›¸ä¼¼æ€§çš„æœç´¢ã€‚æ£€ç´¢å™¨æ‰¾åˆ°ä¸æŸ¥è¯¢æœ€ç›¸ä¼¼çš„å‰ 3 ä¸ªåˆ†å—ï¼Œå¹¶ä½¿ç”¨å†å²æ„ŸçŸ¥æ£€ç´¢å™¨ï¼ˆhistory_aware_retrieverï¼‰æ¥å¢å¼º AI æ™ºèƒ½ä½“å°†å®Œæ•´çš„å¯¹è¯å†å²æ•´åˆåˆ°æ£€ç´¢è¿‡ç¨‹ä¸­çš„èƒ½åŠ›ã€‚

It also integrates context_chain for external input data and docs_chain for conversation records, using create_retrieval_chain to form a complete retrieval process. It handles context-aware responses by verifying if documents are uploaded, retrieving context, and generating responses with an RAG chain

å®ƒè¿˜æ•´åˆäº† `context_chain` ç”¨äºå¤„ç†å¤–éƒ¨è¾“å…¥æ•°æ®ï¼Œä»¥åŠ `docs_chain` ç”¨äºè®°å½•å¯¹è¯å†å²ã€‚é€šè¿‡ `create_retrieval_chain` å‡½æ•°ï¼Œå°†ä¸¤è€…ç»“åˆï¼Œæ„å»ºå®Œæ•´çš„æ£€ç´¢æµç¨‹ã€‚è¯¥æµç¨‹é€šè¿‡ä»¥ä¸‹æ­¥éª¤å¤„ç†å…·å¤‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å›å¤ï¼šé¦–å…ˆéªŒè¯æ˜¯å¦å·²ä¸Šä¼ ç›¸å…³æ–‡æ¡£ï¼Œç„¶åæ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæœ€åä½¿ç”¨ RAGï¼ˆRetrieval-Augmented Generationï¼‰é“¾ç”Ÿæˆæœ€ç»ˆå›å¤ã€‚

By the end of this video, you will understand how Deepseek R1 is trained and how to use Deepseek-R1 with RAG.

åœ¨æœ¬è§†é¢‘ç»“æŸæ—¶ï¼Œä½ å°†ä¼šæ˜ç™½ Deepseek R1 æ˜¯å¦‚ä½•è®­ç»ƒçš„ï¼Œä»¥åŠå¦‚ä½•å°† Deepseek-R1 åº”ç”¨äº RAGã€‚

### Overview of DeepSeek R1 learning method

DeepSeek R1 is characterized by its post-training using reinforcement learning (RL). In general, large-scale language models are developed through the following steps:

1 Pre-training: A large corpus creates a model that â€œpredicts the next word.â€

é¢„è®­ç»ƒï¼šåˆ©ç”¨å¤§å‹è¯­æ–™åº“è®­ç»ƒå‡ºä¸€ä¸ªèƒ½å¤Ÿã€Œé¢„æµ‹ä¸‹ä¸€ä¸ª Tokenã€çš„æ¨¡å‹ã€‚

2 Supervised fine-tuning (SFT): High-quality, human-created instruction-response pairs are used to fine-tune the model for a specific task.

ç›‘ç£å¼å¾®è°ƒï¼ˆSFT)ï¼šä½¿ç”¨é«˜è´¨é‡çš„äººå·¥æ ‡æ³¨çš„æŒ‡ä»¤ - å“åº”æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚

3 RLHF (Reinforcement Learning with Human Feedback): Humans evaluate the modelâ€™s output and use the score as a reward to update the model.

RLHFï¼ˆReinforcement Learning with Human Feedbackï¼ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ )ï¼šé€šè¿‡äººç±»è¯„ä¼°æ¨¡å‹çš„è¾“å‡ºï¼Œå¹¶å°†è¯„ä¼°åˆ†æ•°ä½œä¸ºå¥–åŠ±ä¿¡å·æ¥æ›´æ–°æ¨¡å‹ã€‚

DeepSeek R1 reportedly performed reinforcement learning, especially step 3, on a large scale. Additionally, they explored an approach by applying RL directly, without the conventional SFT (a version called DeepSeek-R1-Zero), and then added SFT to complete DeepSeek R1. This method is said to have promoted the modelâ€™s internal thought process (chain-of-thought) and self-verification.

æ®æŠ¥é“ï¼ŒDeepSeek R1 åœ¨å¤§è§„æ¨¡ä¸Šä½¿ç”¨äº†å¼ºåŒ–å­¦ä¹ ï¼Œç‰¹åˆ«æ˜¯ç¬¬ä¸‰æ­¥ã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜æ¢ç´¢äº†ä¸€ç§ç›´æ¥åº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰çš„æ–¹æ³•ï¼Œå³ä¸ä½¿ç”¨ä¼ ç»Ÿçš„ SFTï¼ˆä»–ä»¬ç§°ä¹‹ä¸º DeepSeek-R1-Zero ç‰ˆæœ¬ï¼‰ï¼Œè€Œæ˜¯ç›´æ¥åº”ç”¨å¼ºåŒ–å­¦ä¹ ï¼Œç„¶åå†åŠ å…¥ SFT æ¥å®Œæˆ DeepSeek R1 çš„è®­ç»ƒã€‚æ®è¯´ï¼Œè¿™ç§æ–¹æ³•èƒ½å¤Ÿä¿ƒè¿›æ¨¡å‹å†…éƒ¨çš„æ€è€ƒè¿‡ç¨‹ï¼ˆchain-of-thoughtï¼Œæ€ç»´é“¾ï¼‰å’Œè‡ªæˆ‘éªŒè¯èƒ½åŠ›ã€‚

Normally, handling the chain of thought effectively requires some SFT data and human labels. However, DeepSeek-R1-Zero claims to have achieved inference power through RL alone, without going through SFT. This approach offers the following advantages:

1 Reducing the cost of large-scale data collection

é™ä½å¤§è§„æ¨¡æ•°æ®é‡‡é›†çš„æˆæœ¬ã€‚

2 Enabling self-correction for unknown tasks and complex situations

é’ˆå¯¹æœªçŸ¥çš„ä»»åŠ¡å’Œå¤æ‚æƒ…å¢ƒï¼Œå¯ç”¨è‡ªæˆ‘ä¿®æ­£åŠŸèƒ½ã€‚

However, models that have not gone through SFT completely still have issues such as text being somewhat difficult to read and unintended multilingual content.

ç„¶è€Œï¼Œé‚£äº›æ²¡æœ‰ç»è¿‡å®Œæ•´çš„ SFTï¼ˆç›‘ç£å¼å¾®è°ƒï¼‰çš„æ¨¡å‹ï¼Œä»ç„¶å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œæ¯”å¦‚æ–‡æœ¬å¯è¯»æ€§è¾ƒå·®ï¼Œä»¥åŠå‡ºç°æ„æ–™ä¹‹å¤–çš„å¤šè¯­è¨€å†…å®¹ã€‚

### Letâ€™s Build The APP

Let us now explore step by step and unravel the answer to how to create RAG APP. we will install the libraries that support the model. For this, we will do a pip install requirements

è®©æˆ‘ä»¬æ„å»ºåº”ç”¨ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ¢ç´¢ï¼Œæ­å¼€å¦‚ä½•åˆ›å»º RAG åº”ç”¨çš„å¥¥ç§˜ã€‚æˆ‘ä»¬å°†å®‰è£…æ”¯æŒæ¨¡å‹çš„å¿…è¦åº“ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ pip å®‰è£… requirements æ–‡ä»¶ä¸­çš„ä¾èµ–é¡¹ã€‚

pip install -r requirements.txt

The next step is the usual one where we will import the relevant libraries, the significance of which will become evident as we proceed.

æ¥ä¸‹æ¥æ˜¯å¸¸è§„æ“ä½œï¼Œæˆ‘ä»¬å°†å¯¼å…¥å¿…è¦çš„ç¨‹åºåº“ï¼Œéšç€åç»­æ­¥éª¤çš„å±•å¼€ï¼Œæ‚¨ä¼šé€æ¸ä½“ä¼šåˆ°å®ƒä»¬çš„é‡è¦æ€§ã€‚

PDFPlumberLoader is a very powerful tool for processing complex structured data such as tables from PDF files. It can extract text, images, tables, fields, etc

PDFPlumberLoader æ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„å·¥å…·ï¼Œå¯ä»¥å¤„ç† PDF æ–‡ä»¶ä¸­å¤æ‚çš„ç»“æ„åŒ–æ•°æ®ï¼Œä¾‹å¦‚è¡¨æ ¼ã€‚å®ƒèƒ½å¤Ÿæå–æ–‡æœ¬ã€å›¾åƒã€è¡¨æ ¼å’Œå­—æ®µç­‰ä¿¡æ¯ã€‚

SemanticChunker splits text into chunks based on semantic similarity, ensuring that related content stays together in the same chunk.

SemanticChunker åŸºäºè¯­ä¹‰ç›¸ä¼¼æ€§å°†æ–‡æœ¬åˆ†å‰²æˆå—ï¼ˆchunksï¼‰ï¼Œç¡®ä¿è¯­ä¹‰ç›¸å…³çš„å†…å®¹è¢«ä¿å­˜åœ¨åŒä¸€ä¸ªå—ä¸­ã€‚

and so on please, if you want to know more about the latest langchain upgrade where I will explain it in detail

æ¥ä¸‹æ¥ï¼Œæˆ‘å°†è¯¦ç»†ä»‹ç»æœ€æ–°çš„ Langchain å‡çº§ï¼Œå¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šä¿¡æ¯ã€‚

```python
import os
import streamlit as st
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_huggingface import HuggingFaceEmbeddings
import os
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplateï¼ŒMessagesPlaceholder
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from langchain_core.messages import AIMessageï¼ŒHumanMessage
import tempfile
from langchain_openai.chat_models.base import BaseChatOpenAI
```

I create a function to handle an uploaded file using a temporary file to store its content. I then process the PDF with PDFPlumberLoader to process complex structured data such as tables from PDF files, ensuring I clean up the temporary file afterwards to keep the system tidy. Finally, I return the extracted data for further use.

æˆ‘åˆ›å»ºäº†ä¸€ä¸ªå‡½æ•°æ¥å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼Œè¯¥å‡½æ•°ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶æ¥å­˜å‚¨æ–‡ä»¶å†…å®¹ã€‚ç„¶åï¼Œæˆ‘ä½¿ç”¨`PDFPlumberLoader` æ¥åŠ è½½ PDF æ–‡ä»¶ï¼Œå®ƒå¯ä»¥å¤„ç†å¤æ‚ç»“æ„çš„ PDF æ•°æ®ï¼Œä¾‹å¦‚è¡¨æ ¼ã€‚ä¸ºäº†ä¿æŒç³»ç»Ÿçš„æ•´æ´ï¼Œæˆ‘ä¼šç¡®ä¿åœ¨å¤„ç†å®Œæ¯•åæ¸…ç†è¿™äº›ä¸´æ—¶æ–‡ä»¶ã€‚æœ€åï¼Œå‡½æ•°ä¼šè¿”å›æå–çš„æ•°æ®ï¼Œä»¥ä¾›åç»­æµç¨‹ä½¿ç”¨ã€‚

```python
def process_uploaded_file(uploaded_file):

    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        loader = PDFPlumberLoader(tmp_file.name)
        documents = loader.load()
    os.unlink(tmp_file.name)
    return documents
```

I create a function to process a list of documents and build a retriever for similarity-based searches. First, I use the SemanticChunker with OpenAIEmbeddings to split the documents into smaller chunks while preserving semantic relationships, ensuring related content stays grouped together.

æˆ‘åˆ›å»ºäº†ä¸€ä¸ªå‡½æ•°æ¥å¤„ç†æ–‡æ¡£åˆ—è¡¨ï¼Œå¹¶æ„å»ºä¸€ä¸ªåŸºäºç›¸ä¼¼æ€§æœç´¢çš„æ£€ç´¢å™¨ã€‚é¦–å…ˆï¼Œæˆ‘ä½¿ç”¨å¸¦æœ‰ OpenAI åµŒå…¥ï¼ˆOpenAIEmbeddingsï¼‰çš„ SemanticChunkerï¼ˆSemanticChunkerï¼‰å°†æ–‡æ¡£åˆ†å‰²æˆæ›´å°çš„å—ï¼ŒåŒæ—¶ä¿ç•™è¯­ä¹‰å…³ç³»ï¼Œç¡®ä¿ç›¸å…³å†…å®¹ä¿æŒåœ¨ä¸€èµ·ã€‚

After confirming the splitting process, I developed a vector store using FAISS, which organizes the chunks for efficient similarity search.

ç¡®è®¤åˆ†å‰²æµç¨‹åï¼Œæˆ‘ä½¿ç”¨ FAISS æ„å»ºäº†ä¸€ä¸ªå‘é‡æ•°æ®åº“ï¼Œå®ƒèƒ½æ•´ç†åˆ†å—åçš„æ•°æ®ï¼Œä»è€Œå®ç°é«˜æ•ˆçš„ç›¸ä¼¼åº¦æœç´¢ã€‚

Finally, I return a retriever that can find the top 3 most similar chunks for a given query, making the document retrieval process both accurate and efficient.

æœ€ç»ˆï¼Œæˆ‘è¿”å›ä¸€ä¸ªæ£€ç´¢å™¨ï¼Œå®ƒå¯ä»¥æ ¹æ®ç»™å®šçš„æŸ¥è¯¢ï¼Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„ 3 ä¸ªæ•°æ®å—ï¼Œä»è€Œä¿è¯æ–‡æ¡£æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚

```python
def get_vs_retriever_from_docs(doc_list):

    text_splitter = SemanticChunker(HuggingFaceEmbeddings())
    documents = text_splitter.split_documents(doc_list)
    st.write('Document splitting done')
    
    
    embedder = HuggingFaceEmbeddings()
    vector = FAISS.from_documents(documents, embedder)

    return vector.as_retriever(search_type="similarity", search_kwargs={"k": 3})
```

I create the init_ui() function to set up a user-friendly interface for uploading and analyzing PDF documents using Streamlit. First, I configure the page with a title and descriptive header.

æˆ‘åˆ›å»ºäº† `init_uiï¼ˆ)` å‡½æ•°ï¼Œç”¨äºè®¾ç½®ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„ç•Œé¢ï¼Œä»¥ä¾¿é€šè¿‡ Streamlit ä¸Šä¼ å’Œåˆ†æ PDF æ–‡æ¡£ã€‚é¦–å…ˆï¼Œæˆ‘ä½¿ç”¨æ ‡é¢˜å’Œæè¿°æ€§æ ‡é¢˜æ¥é…ç½®é¡µé¢ã€‚

I then initialize session state variables to manage the retriever, chat history, and upload status. Next, I provide a file uploader widget for users to upload PDF files.

æˆ‘éšååˆå§‹åŒ–ä¼šè¯çŠ¶æ€å˜é‡ï¼Œç”¨äºç®¡ç†æ£€ç´¢å™¨ï¼ˆretrieverï¼‰ã€èŠå¤©å†å²å’Œä¸Šä¼ çŠ¶æ€ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘æä¾›äº†ä¸€ä¸ªæ–‡ä»¶ä¸Šä¼ åŠŸèƒ½ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡å®ƒä¸Šä¼  PDF æ–‡ä»¶ã€‚

Once a file is uploaded, I process it to extract its content, create a retriever using get_vs_retriever_from_docs, and store it in the session state for further use. Finally, I notify the user of the successful document processing with a success message.

ä¸€æ—¦æ–‡ä»¶è¢«ä¸Šä¼ ï¼Œæˆ‘ä¼šå¯¹å…¶è¿›è¡Œå¤„ç†ä»¥æå–å†…å®¹ï¼Œä½¿ç”¨ `get_vs_retriever_from_docs` åˆ›å»ºä¸€ä¸ªæ£€ç´¢å™¨ï¼ˆretrieverï¼‰ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨ä¼šè¯çŠ¶æ€ä¸­ï¼Œä»¥ä¾¿åç»­ä½¿ç”¨ã€‚æœ€åï¼Œæˆ‘ä¼šå‘é€ä¸€æ¡æˆåŠŸæ¶ˆæ¯ï¼Œé€šçŸ¥ç”¨æˆ·æ–‡æ¡£å¤„ç†å®Œæ¯•ã€‚

```python
def init_ui():
    st.set_page_config(page_title='Document uploader')
    st.markdown('#### :books:ğŸ§™Gao Dalie (é«˜é”çƒˆ): Your Document Summarizer')
    st.markdown("<h8 style='text-align: right; color: green;'>*Share the pdf of the book you want to read*</h8>", unsafe_allow_html=True)
    
    if "vector_store" not in st.session_state:
        st.session_state.vector_store = None
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []  
    if "doc_upload" not in st.session_state:
        st.session_state.doc_upload = False

    uploaded_file = st.file_uploader("Upload PDF", type=["pdf"])
    if uploaded_file:
        docs = process_uploaded_file(uploaded_file)
        if docs:
            retriever = get_vs_retriever_from_docs(docs)
            st.session_state.vector_store = retriever
            st.session_state.doc_upload = True
            st.success("Document processed successfully")
```

I create the get_related_context() function to improve the retrieval of relevant information in a conversational system. First, I initialize the Ollama model (deepseek-r1) to process user inputs.

æˆ‘åˆ›å»ºäº† `get_related_contextï¼ˆ)` å‡½æ•°ï¼Œæ—¨åœ¨æå‡å¯¹è¯ç³»ç»Ÿä¸­ç›¸å…³ä¿¡æ¯çš„æ£€ç´¢æ•ˆæœã€‚é¦–å…ˆï¼Œæˆ‘åˆå§‹åŒ– Ollama æ¨¡å‹ï¼ˆdeepseek-r1ï¼‰ï¼Œç”¨äºå¤„ç†ç”¨æˆ·è¾“å…¥ã€‚

Then, I define a prompt template that combines the chat history and user input, instructing the model to generate a search query based on the ongoing conversation.

ç„¶åï¼Œæˆ‘è®¾è®¡äº†ä¸€ä¸ªæç¤ºæ¨¡æ¿ï¼Œå®ƒå°†èŠå¤©è®°å½•å’Œç”¨æˆ·è¾“å…¥æ•´åˆåœ¨ä¸€èµ·ï¼Œå¼•å¯¼æ¨¡å‹æ ¹æ®å½“å‰çš„å¯¹è¯å†…å®¹ç”Ÿæˆæœç´¢æŸ¥è¯¢ã€‚

Finally, I use create_history_aware_retriever to enhance the agent's ability to incorporate the entire conversation history into the retrieval process.

```python
def get_related_context(vector_store):

    # llm = Ollama(model="deepseek-r1")
  
    
    llm = BaseChatOpenAI(
        model='deepseek-reasoner', 
        openai_api_key='sk-68f459660c7b4d179e074cbedce962c0', 
        openai_api_base='https://api.deepseek.com',
        max_tokens=1024
    )
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Generate a search query based on the conversation."),
        ("user", "{input}")
    ])
    return create_history_aware_retriever(llm, vector_store, prompt)
```

In this function, I designed a function that integrates two core chains:

åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œæˆ‘è®¾è®¡å¹¶æ•´åˆäº†ä¸¤ä¸ªæ ¸å¿ƒé“¾ï¼š

1 context_chain Responsible for finding external input data

2 Responsible for finding conversation records

context_chain è´Ÿè´£å¯»æ‰¾å¤–éƒ¨è¾“å…¥æ•°æ®ï¼Œå¹¶è´Ÿè´£å¯»æ‰¾å¯¹è¯è®°å½•ã€‚

Finally, use it retrieval_chain = create_retrieval_chain(context_chain, docs_chain) to create a chain that can retrieve conversations and external input data, and complete a chain that can allow natural conversations.

æœ€ç»ˆï¼Œåˆ©ç”¨ context_chain å’Œ docs_chain åˆ›å»ºæ£€ç´¢é“¾ï¼ˆretrieval_chainï¼‰ï¼Œå³ `retrieval_chain = create_retrieval_chainï¼ˆcontext_chainï¼Œdocs_chain)`ï¼Œè¯¥æ£€ç´¢é“¾å¯ä»¥æ£€ç´¢å¯¹è¯å’Œå¤–éƒ¨è¾“å…¥æ•°æ®ï¼Œä»è€Œæ„å»ºä¸€ä¸ªèƒ½å¤Ÿè¿›è¡Œè‡ªç„¶å¯¹è¯çš„é“¾ã€‚

The conversation record needs to create a List storage by itself, which is chat_history the part.

éœ€è¦åˆ›å»ºä¸€ä¸ªåˆ—è¡¨æ¥å­˜å‚¨å¯¹è¯è®°å½•ï¼Œè¿™éƒ¨åˆ†å†…å®¹å¯¹åº”äº `chat_history`ã€‚

```python
def get_context_aware_prompt(context_chain):
    # llm_! = Ollama(model="deepseek-r1")
    
    llm = BaseChatOpenAI(
        model='deepseek-reasoner', 
        openai_api_key='sk-68f459660c7b4d179e074cbedce962c0', 
        openai_api_base='https://api.deepseek.com',
        max_tokens=1024
    )
    prompt = ChatPromptTemplate.from_messages([
       ("system", "Answer questions using the provided context:\n\n{context}"),
       ("user", "{input}")
   ])
    # st.write(docs_chain)
    return create_retrieval_chain(context_chain, docs_chain)
```

In this function, I handle the process of generating a context-aware response for a given query. I start checking if the necessary documents are uploaded.

åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œæˆ‘å¤„ç†ä¸ºç»™å®šçš„æŸ¥è¯¢ç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å“åº”çš„è¿‡ç¨‹ã€‚å‡½æ•°å¼€å§‹æ—¶ï¼Œä¼šæ£€æŸ¥æ˜¯å¦å·²ä¸Šä¼ å¿…è¦çš„æ–‡ä»¶ã€‚

If no documents are uploaded, I return an error. Once the documents are verified, I retrieve a context chain to access relevant information from the uploaded documents. If retrieving the context fails, I return an error.

å¦‚æœæ²¡æœ‰ä¸Šä¼ ä»»ä½•æ–‡æ¡£ï¼Œç³»ç»Ÿä¼šæŠ¥é”™ã€‚æ–‡æ¡£éªŒè¯é€šè¿‡åï¼Œç³»ç»Ÿä¼šæå–ä¸€ä¸ªä¸Šä¸‹æ–‡é“¾ï¼ˆcontext chainï¼‰ï¼Œç”¨äºè®¿é—®å·²ä¸Šä¼ æ–‡æ¡£ä¸­çš„ç›¸å…³ä¿¡æ¯ã€‚å¦‚æœæå–ä¸Šä¸‹æ–‡ä¿¡æ¯å¤±è´¥ï¼Œç³»ç»Ÿä¹Ÿä¼šæŠ¥é”™ã€‚

I then create an RAG chain that combines the context with the conversation history and the userâ€™s query to generate a response. Finally, I invoke the RAG chain, process the response, and return the generated answer.

ç„¶åï¼Œæˆ‘åˆ›å»ºä¸€ä¸ª RAG é“¾ï¼ˆRAG chainï¼‰ï¼Œè¯¥ RAG é“¾ä¼šå°†ä¸Šä¸‹æ–‡ã€å¯¹è¯å†å²ä»¥åŠç”¨æˆ·çš„æŸ¥è¯¢æ•´åˆèµ·æ¥ï¼Œä»¥ç”Ÿæˆå“åº”ã€‚æœ€åï¼Œæˆ‘æ‰§è¡Œ RAG é“¾ï¼ˆRAG chainï¼‰ï¼Œåˆ†æå¤„ç†å“åº”ï¼Œå¹¶è¿”å›ç”Ÿæˆçš„ç­”æ¡ˆã€‚

```python
def get_response(query: str) -> str:
    if not st.session_state.vector_store:
        return "Error: Please upload documents first"
    try:
        context_chain = get_related_context(st.session_state.vector_store)
        # st.write(context_chain)
        if not context_chain:
            return "Error: Failed to process context"

        rag_chain = get_context_aware_prompt(context_chain)
        # st.write(rag_chain)
        current_history = st.session_state.chat_history[-2:] if len(st.session_state.chat_history) > 1 else []
        return rag_chain.invoke({
            "chat_history": current_history,
            "input": query
        })["answer"].do
    except Exception as e:
        return f"Error: {str(e)}"
```

In this function, I set up and display a chat interface for user interaction. First, I loop through the stored chat history and display both user and AI messages in the chat.

åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œæˆ‘æ­å»ºå¹¶å‘ˆç°äº†ä¸€ä¸ªç”¨æˆ·äº¤äº’çš„èŠå¤©ç•Œé¢ã€‚é¦–å…ˆï¼Œç¨‹åºä¼šéå†å·²å­˜å‚¨çš„èŠå¤©è®°å½•ï¼Œå¹¶åœ¨ç•Œé¢ä¸Šå±•ç¤ºç”¨æˆ·å’Œ AI çš„å¯¹è¯å†…å®¹ã€‚

Then, I provide an input where the user can type a new question. If the user submits a query, I call the get_response function to generate an answer, and then I update the chat history with the new message and response.

æ¥ä¸‹æ¥ï¼Œæˆ‘ä¼šæä¾›ä¸€ä¸ªè¾“å…¥æ¡†ï¼Œç”¨æˆ·å¯ä»¥åœ¨è¿™é‡Œè¾“å…¥æ–°çš„é—®é¢˜ã€‚å¦‚æœç”¨æˆ·æäº¤äº†ä¸€ä¸ªæŸ¥è¯¢ï¼Œæˆ‘ä¼šè°ƒç”¨ `get_response` å‡½æ•°æ¥ç”Ÿæˆç­”æ¡ˆï¼Œç„¶åå°†æ–°çš„æ¶ˆæ¯å’Œå›å¤æ·»åŠ åˆ°èŠå¤©è®°å½•ä¸­ï¼Œå®Œæˆæ›´æ–°ã€‚

```python
def init_chat_interface():
    for message in st.session_state.chat_history:
        with st.chat_message("user" if isinstance(message, HumanMessage) else "assistant"):
            st.write(message.content)

    if prompt := st.chat_input("Ask a question...", disabled=not st.session_state.doc_upload):
        st.session_state.chat_history.append(HumanMessage(content=prompt))
        response = get_response(prompt)
        st.session_state.chat_history.append(AIMessage(content=response))

if __name__ == "__main__":
    init_ui()
    if st.session_state.vector_store:
        init_chat_interface()
```

### Conclusion

The launch of DeepSeek R1 is a technological breakthrough and a key milestone in AIâ€™s democratization. Its open approach is reshaping our view of AI, allowing more people to contribute to its development.

DeepSeek R1 çš„å‘å¸ƒæ˜¯ä¸€é¡¹æŠ€æœ¯çªç ´ï¼Œä¹Ÿæ˜¯äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ™®åŠçš„ä¸€ä¸ªå…³é”®é‡Œç¨‹ç¢‘ã€‚å…¶å¼€æ”¾çš„æ–¹æ³•æ­£åœ¨é‡å¡‘æˆ‘ä»¬å¯¹äººå·¥æ™ºèƒ½çš„è®¤çŸ¥ï¼Œè®©æ›´å¤šäººèƒ½å¤Ÿå‚ä¸åˆ°äººå·¥æ™ºèƒ½çš„å¼€å‘ä¸­æ¥ã€‚

Understanding concepts like the Retrieval Chain and the Conversation Retrieval Chain is crucial for using LangChain.

ç†è§£æ£€ç´¢é“¾ï¼ˆRetrieval Chainï¼‰å’Œå¯¹è¯æ£€ç´¢é“¾ï¼ˆConversation Retrieval Chainï¼‰ç­‰æ¦‚å¿µï¼Œå¯¹äºä½¿ç”¨ LangChain è‡³å…³é‡è¦ã€‚

As long as you understand the principles of these two Chains, you can not only know the operating principles behind customer service chatbots made with language models but also enable us to do similar applications.

åªè¦ç†è§£è¿™ä¸¤ç§é“¾çš„åŸç†ï¼Œä¸ä»…èƒ½äº†è§£åŸºäºè¯­è¨€æ¨¡å‹æ„å»ºçš„å®¢æœèŠå¤©æœºå™¨äººçš„è¿ä½œæ–¹å¼ï¼Œè¿˜èƒ½å¼€å‘ç±»ä¼¼çš„åº”ç”¨ã€‚

However, LangChain is not just that. Many functions and tips need to be learned to create language model-related applications.

ç„¶è€Œï¼ŒLangChain ä¸ä»…äºæ­¤ã€‚è¦åˆ›å»ºåŸºäºè¯­è¨€æ¨¡å‹çš„åº”ç”¨ï¼Œè¿˜éœ€è¦å­¦ä¹ è®¸å¤šå‡½æ•°å’ŒæŠ€å·§ã€‚

### Reference

https://github.com/deepseek-ai/DeepSeek-R1