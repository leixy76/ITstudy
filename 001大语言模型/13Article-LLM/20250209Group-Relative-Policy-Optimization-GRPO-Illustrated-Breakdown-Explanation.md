## 20250209Group-Relative-Policy-Optimization-GRPO-Illustrated-Breakdown-Explanation

[Group Relative Policy Optimization (GRPO) Illustrated Breakdown & Explanation | by Ebrahim Pichka | Jan, 2025 | Towards AI](https://pub.towardsai.net/group-relative-policy-optimization-grpo-illustrated-breakdown-explanation-684e71b8a3f2)

A simplified intro to GRPO, an efficient policy optimization method used for LLM reasoning training

Ebrahim Pichka

Towards AI

Ebrahim Pichka

Jan 31, 2025

### 01. Introduction

ä»‹ç»

Reinforcement Learning (RL) has emerged as a powerful tool for enhancing Large Language Models (LLMs) after their initial training, particularly in reasoning-intensive tasks. DeepSeekâ€™s recent breakthroughs with DeepSeek-Math [2] and DeepSeek-R1 [3] models have demonstrated the remarkable potential of RL in improving mathematical reasoning and problem-solving abilities of LLMs.

å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²ç»æˆä¸ºå¢å¼ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æœ‰åŠ›å·¥å…·ï¼Œå°¤å…¶æ˜¯åœ¨åˆå§‹è®­ç»ƒä¹‹åï¼Œå¯¹äºæ¨ç†å¯†é›†å‹ä»»åŠ¡çš„æå‡æ•ˆæœæ˜¾è‘—ã€‚DeepSeek è¿‘æœŸå‘å¸ƒçš„ DeepSeek-Math [2] å’Œ DeepSeek-R1 [3] æ¨¡å‹ï¼Œå……åˆ†å±•ç¤ºäº† RL åœ¨æå‡ LLM çš„æ•°å­¦æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚

These achievements were made possible through an innovative RL approach called Group Relative Policy Optimization (GRPO), which addresses the unique challenges of applying RL to language models. In this post, weâ€™ll dive deep into how GRPO works and why it represents a significant advancement in LLM training.

è¿™äº›æˆå°±å¾—ç›Šäºä¸€ç§åˆ›æ–°çš„å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼ŒRLï¼‰æ–¹æ³• â€”â€” ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGroup Relative Policy Optimizationï¼ŒGRPOï¼‰ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåº”å¯¹äº†å°†å¼ºåŒ–å­¦ä¹ åº”ç”¨äºè¯­è¨€æ¨¡å‹æ—¶é‡åˆ°çš„ç‰¹æ®ŠæŒ‘æˆ˜ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨ GRPO çš„å·¥ä½œåŸç†ï¼Œä»¥åŠå®ƒå¦‚ä½•æˆä¸ºå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒé¢†åŸŸçš„é‡è¦è¿›å±•ã€‚

### 02. PPO vs GRPO

Proximal Policy Optimization (PPO) [1] has been the go-to algorithm for RL fine-tuning of language models. At its core, PPO is a policy gradient method that uses clipping to limit policy updates (gradients), preventing destructive large policy changes. The objective function for PPO can be written as:

è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼ˆProximal Policy Optimizationï¼ŒPPOï¼‰[1] æ˜¯å¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒæ—¶å¸¸ç”¨çš„ç®—æ³•ã€‚ä»æœ¬è´¨ä¸Šè®²ï¼ŒPPO æ˜¯ä¸€ç§ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œå®ƒé€šè¿‡è£å‰ªæ¥é™åˆ¶ç­–ç•¥çš„æ›´æ–°å¹…åº¦ï¼ˆæ¢¯åº¦ï¼‰ï¼Œä»¥æ­¤é¿å…å¯èƒ½é€ æˆç ´åæ€§å½±å“çš„ç­–ç•¥çªå˜ã€‚PPO çš„ç›®æ ‡å‡½æ•°å¯ä»¥è¡¨ç¤ºä¸ºï¼š

$$
J_{PPO}(\Theta)= E[s \sim P(S), a \sim \pi_{\theta_{old}}(A \mid s)]
\min \left[ \frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{old}}(a \mid s)} A(s, a), \operatorname{clip}\left(\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{old}}(a \mid s)}, 1-\epsilon, 1+\epsilon\right) A(s, a) \right]
$$

GRPO â€” first introduced in [2] â€” builds upon PPOâ€™s foundation but introduces several key innovations that make it more efficient and better suited for language models:

GRPOï¼Œé¦–æ¬¡åœ¨ [2] ä¸­æå‡ºï¼Œå®ƒä»¥ PPO ä¸ºåŸºç¡€ï¼Œä½†å¼•å…¥äº†å‡ é¡¹å…³é”®åˆ›æ–°ï¼Œä½¿å…¶æ•ˆç‡æ›´é«˜ï¼Œæ›´é€‚åˆç”¨äºè¯­è¨€æ¨¡å‹ï¼š

1 Eliminates the need for a value network, hence less memory/compute usage

æ— éœ€ä»·å€¼ç½‘ç»œï¼Œä»è€Œå‡å°‘å†…å­˜ / è®¡ç®—æ¶ˆè€—

2 Uses group sampling for more efficient stable advantage estimation

ä½¿ç”¨ç»„é‡‡æ ·ï¼Œæ¥å®ç°æ›´é«˜æ•ˆã€æ›´ç¨³å®šçš„ä¼˜åŠ¿å‡½æ•°

3 Uses a more conservative update mechanism through further penalizing both the objective and the rewards

ä¼°è®¡é‡‡ç”¨æ›´ä¿å®ˆçš„æ›´æ–°æœºåˆ¶ï¼Œé€šè¿‡è¿›ä¸€æ­¥æƒ©ç½šç›®æ ‡å‡½æ•°å’Œå¥–åŠ±æ¥å®ç°

### 03. GRPO: A Closer Look

GRPOï¼šæ·±å…¥è§£æ

LLM as a Policy

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºç­–ç•¥

In GRPO, the language model serves as the policy network (actor), taking a question q as input observation s and producing a sequence of tokens as actions. The policy distribution factors across tokens:

åœ¨ GRPO ä¸­ï¼Œå¤§è¯­è¨€æ¨¡å‹å……å½“ç­–ç•¥ç½‘ç»œï¼ˆè¡ŒåŠ¨è€… actorï¼‰ï¼Œå®ƒæ¥æ”¶é—®é¢˜ q ä½œä¸ºè¾“å…¥è§‚å¯Ÿ sï¼Œå¹¶ç”Ÿæˆä¸€ç³»åˆ— token ä½œä¸ºåŠ¨ä½œã€‚ç­–ç•¥çš„åˆ†å¸ƒå¯ä»¥åˆ†è§£ä¸ºæ¯ä¸ª token çš„æ¡ä»¶æ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ¨¡å‹ä¼šä¾æ¬¡é¢„æµ‹æ¯ä¸ª token ï¼Œä»è€Œæ„æˆå®Œæ•´çš„åŠ¨ä½œåºåˆ—ã€‚

$\pi_\theta(a \mid q) = \prod_{t=1}^{N} \pi_\theta(a_t \mid q, a_{< t})$

Note: In the original paper [2], they use o_t to denote the output token at timestep t. Whereas we use a_t instead to conform with standard RL notation of action.

æ³¨æ„ï¼šåœ¨åŸå§‹è®ºæ–‡ [2] ä¸­ï¼Œä½œè€…ä½¿ç”¨ o_t æ¥è¡¨ç¤º *t* æ—¶åˆ»è¾“å‡ºçš„ Tokenã€‚ä¸ºäº†ä¸å¼ºåŒ–å­¦ä¹ ä¸­åŠ¨ä½œï¼ˆactionï¼‰çš„æ ‡å‡†è¡¨ç¤ºæ³•ä¿æŒä¸€è‡´ï¼Œæˆ‘ä»¬ä½¿ç”¨ a_t æ¥è¡¨ç¤ºã€‚

### 04. Sequential Token Generation

åºåˆ—åŒ– Token ç”Ÿæˆ

The generation process is inherently sequential because of auto-regressive nature of transformers/LLMs:

ç”±äº Transformer / å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è‡ªå›å½’ç‰¹æ€§ï¼Œç”Ÿæˆè¿‡ç¨‹æœ¬è´¨ä¸Šæ˜¯åºåˆ—åŒ–çš„ï¼š

1 Each token is generated conditionally on previous tokens

æ¯ä¸ª Token çš„ç”Ÿæˆéƒ½ä¾èµ–äºä¹‹å‰çš„ Token

2 The policy network (LLM) maintains a running context

ç­–ç•¥ç½‘ç»œï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰ç»´æŠ¤ç€ä¸€ä¸ªåŠ¨æ€çš„ä¸Šä¸‹æ–‡

3 Each token generation step can be viewed as an action a_t in the RL framework

åœ¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¡†æ¶ä¸­ï¼Œæ¯ä¸ª Token çš„ç”Ÿæˆæ­¥éª¤éƒ½å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªåŠ¨ä½œ a\_t

### 05. Reward and Advantage Calculation

å¥–åŠ±å’Œä¼˜åŠ¿è®¡ç®—

For each generated sequence, GRPO computes per-token rewards as follows:

å¯¹äºæ¯ä¸ªç”Ÿæˆçš„åºåˆ—ï¼ŒGRPO æŒ‰ç…§ä»¥ä¸‹æ–¹å¼è®¡ç®—æ¯ä¸ª Token çš„å¥–åŠ±ï¼š

$r_{+} = r_{\phi}(s, a_{\le t}) - \beta \log \frac{\pi_{\theta}(a_t \mid s, a_{< t})}{\pi_{ref}(a_t \mid s, a_{< t})}$

Instead of using a value network, GRPO estimates baseline advantages A by normalizing a group (batch) of rewards obtained from sampling multiple different outputs from the reference policy produced in response to the same question as input:

GRPO å¹¶æ²¡æœ‰ä½¿ç”¨ä»·å€¼ç½‘ç»œï¼Œè€Œæ˜¯é‡‡ç”¨äº†ä¸€ç§ä¸åŒçš„æ–¹æ³•æ¥ä¼°è®¡åŸºçº¿ä¼˜åŠ¿ Aï¼ˆAdvantageï¼‰ã€‚å®ƒé€šè¿‡å¯¹ä¸€æ‰¹å¥–åŠ±è¿›è¡Œå½’ä¸€åŒ–æ¥å®ç°ï¼Œè¿™äº›å¥–åŠ±æ˜¯é€šè¿‡å‚è€ƒç­–ç•¥å¯¹åŒä¸€è¾“å…¥é—®é¢˜è¿›è¡Œå¤šæ¬¡é‡‡æ ·ï¼Œä»è€Œè·å¾—å¤šä¸ªä¸åŒçš„è¾“å‡ºã€‚

$\hat{A}_{i,t} = \tilde{r}_i = \frac{r_i - mean(\mathbf{r})}{std(\mathbf{r})}$

The GRPO Objective

GRPO ç›®æ ‡

for each question ğ‘, GRPO samples a group of outputs {ğ‘œ1, ğ‘œ2, Â· Â· Â· , ğ‘œğº} from the old policy ğœ‹ğœƒğ‘œğ‘™ğ‘‘ and then optimizes the policy model by maximizing the GRPO objective. The complete GRPO objective brings everything together:

å¯¹äºæ¯ä¸€ä¸ªé—®é¢˜ ğ‘ï¼ŒGRPO ä¼šä»æ—§çš„ç­–ç•¥ ğœ‹ğœƒğ‘œğ‘™ğ‘‘ ä¸­é‡‡æ ·ä¸€ç»„è¾“å‡º {ğ‘œ1ï¼Œğ‘œ2,Â·Â·Â·ï¼Œğ‘œğº}ï¼Œç„¶åé€šè¿‡æœ€å¤§åŒ– GRPO ç›®æ ‡å‡½æ•°æ¥ä¼˜åŒ–ç­–ç•¥æ¨¡å‹ã€‚å®Œæ•´çš„ GRPO ç›®æ ‡å‡½æ•°å°†æ‰€æœ‰è¦ç´ æ•´åˆå¦‚ä¸‹ï¼š

$$
J_{GRPO}(\Theta) = E[s \sim P(S), a \sim \pi_{\theta_{old}}(A \mid s)] \\
\frac{1}{G} \sum_{i=1}^{G} \left\{ \min \left[ \frac{\pi_{\theta}(a_i \mid s)}{\pi_{\theta_{old}}(a_i \mid s)} \hat{A}_i, \operatorname{clip}\left(\frac{\pi_{\theta}(a_i \mid s)}{\pi_{\theta_{old}}(a_i \mid s)}, 1-\epsilon, 1+\epsilon\right) \hat{A}_i \right] - \beta D_{KL}[\pi_\theta || \pi_{ref}] \right\}
$$

This objective:

è¿™ä¸ªç›®æ ‡ï¼š

1 Averages over both groups and sequence lengths

å¯¹ä¸¤ç»„æ•°æ®å’Œåºåˆ—é•¿åº¦è¿›è¡Œå¹³å‡ï¼›

2 Uses clipping for conservative updates

é‡‡ç”¨è£å‰ªï¼ˆclippingï¼‰æ–¹æ³•è¿›è¡Œä¿å®ˆæ›´æ–°ï¼›

3 Includes an estimate of the KL divergence as a penalty to prevent large deviations from the reference model

å¼•å…¥ KL æ•£åº¦ï¼ˆKL divergenceï¼‰çš„ä¼°è®¡ï¼Œä½œä¸ºä¸€ç§æƒ©ç½šé¡¹ï¼Œä»¥é¿å…ä¸å‚è€ƒæ¨¡å‹äº§ç”Ÿè¿‡å¤§çš„åå·®ã€‚

$$
J_{GRPO}(\Theta) = E[s \sim P(S), \{\mathbf{a}_i\}_{i=1}^G \sim \pi_{\theta_{old}}(A \mid s)] \\
\frac{1}{G} \sum_{i=1}^{G} \frac{1}{|\mathbf{a}_i|} \sum_{t=1}^{|\mathbf{a}_i|} \left\{ \min \left[ \frac{\pi_{\theta}(a_{i,t} \mid s, \mathbf{a}_{i,<t})}{\pi_{\theta_{old}}(a_{i,t} \mid s, \mathbf{a}_{i,<t})} \hat{A}_{i,t}, \operatorname{clip}\left(\frac{\pi_{\theta}(a_{i,t} \mid s, \mathbf{a}_{i,<t})}{\pi_{\theta_{old}}(a_{i,t} \mid s, \mathbf{a}_{i,<t})}, 1-\epsilon, 1+\epsilon\right) \hat{A}_{i,t} \right] - \beta D_{KL}[\pi_\theta || \pi_{ref}] \right\}
$$

=>

$$
D_{KL}[\pi_\theta || \pi_{ref}] = \frac{\pi_{ref}(a_{i,t} | q, a_{i,< t})}{\pi_\theta(a_{i,t} | q, a_{i,<t})} - \log \frac{\pi_{ref}(a_{i,t} | q, a_{i,< t})}{\pi_\theta(a_{i,t} | q, a_{i,< t})} - 1
$$

### 06. Conclusion

ç»“è®º

GRPO represents a significant advancement in applying RL to language models. By eliminating the need for a value network and introducing group-relative advantage estimation, it provides a more efficient and stable training process. The success of DeepSeek-Math and DeepSeek-R1 demonstrates the practical benefits of this approach.

GRPO ä»£è¡¨äº†åœ¨å°†å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼ŒRLï¼‰åº”ç”¨äºè¯­è¨€æ¨¡å‹æ–¹é¢çš„ä¸€é¡¹é‡å¤§è¿›æ­¥ã€‚å®ƒæ— éœ€ä»·å€¼ç½‘ç»œï¼Œå¹¶å¼•å…¥äº†ç»„ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡ï¼Œä»è€Œæä¾›äº†ä¸€ä¸ªæ›´é«˜æ•ˆã€æ›´ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹ã€‚DeepSeek-Math å’Œ DeepSeek-R1 çš„æˆåŠŸï¼Œå……åˆ†å±•ç¤ºäº†è¿™ç§æ–¹æ³•åœ¨å®è·µä¸­çš„ä¼˜åŠ¿ã€‚

The key innovations of GRPO â€” group sampling, relative advantage estimation, and the elimination of the value network â€” provide a blueprint for future developments in LLM training. As we continue to push the boundaries of what language models can achieve, techniques like GRPO will be crucial in unlocking their full potential.

GRPO çš„å‡ é¡¹å…³é”®åˆ›æ–° â€”â€” ç»„é‡‡æ ·ï¼ˆgroup samplingï¼‰ã€ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡ï¼ˆrelative advantage estimationï¼‰ä»¥åŠå»é™¤ä»·å€¼ç½‘ç»œï¼ˆelimination of the value network)â€”â€” ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM/Large Language Modelï¼‰è®­ç»ƒçš„æœªæ¥å‘å±•æ–¹å‘å¥ å®šäº†åŸºç¡€ã€‚éšç€æˆ‘ä»¬ä¸æ–­æ¢ç´¢è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›è¾¹ç•Œï¼ŒGRPO è¿™ç±»æŠ€æœ¯å°†åœ¨å……åˆ†æŒ–æ˜å…¶æ½œåŠ›æ–¹é¢å‘æŒ¥å…³é”®ä½œç”¨ã€‚

### References

[1] Schulman, John, et al. Proximal Policy Optimization Algorithms. arXiv:1707.06347, arXiv, 28 Aug. 2017. arXiv.org, https://doi.org/10.48550/arXiv.1707.06347.

[2] Shao, Zhihong, et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300, arXiv, 27 Apr. 2024. arXiv.org, https://doi.org/10.48550/arXiv.2402.03300.

[3] DeepSeek-AI, et al. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948, arXiv, 22 Jan. 2025. arXiv.org, https://doi.org/10.48550/arXiv.2501.12948.