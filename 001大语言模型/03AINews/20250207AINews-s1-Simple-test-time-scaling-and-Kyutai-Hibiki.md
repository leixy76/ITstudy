## 20250207AINews-s1-Simple-test-time-scaling-and-Kyutai-Hibiki

[[AINews] s1: Simple test-time scaling (and Kyutai Hibiki) â€¢ Buttondown](https://buttondown.com/ainews/archive/ainews-s1-simple-test-time-scaling-and-kyutai/)

February 7, 2025

[AINews] s1ï¼šç®€å•çš„æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆand Kyutai Hibiki)

This is AI News! an MVP of a service that goes thru all AI discords/Twitters/reddits and summarizes what people are talking about, so that you can keep up without the fatigue. Signing up here opts you in to the real thing when we launch it

è¿™é‡Œæ˜¯ AI æ–°é—»ï¼è¿™æ˜¯ä¸€ä¸ªæœ€å°å¯è¡Œäº§å“ï¼ˆMVPï¼ŒMinimum Viable Productï¼‰æœåŠ¡ï¼Œå®ƒä¼šæµè§ˆæ‰€æœ‰ AI ç›¸å…³çš„ Discord ç¾¤ç»„ã€Twitter å’Œ Reddit ç¤¾åŒºï¼Œå¹¶æ€»ç»“äººä»¬æ­£åœ¨è®¨è®ºçš„çƒ­ç‚¹ï¼Œè®©æ‚¨è½»æ¾æŒæ¡ AI é¢†åŸŸçš„æœ€æ–°åŠ¨æ€ã€‚åœ¨æ­¤å¤„æ³¨å†Œï¼Œå³å¯åœ¨æˆ‘ä»¬çš„æ­£å¼äº§å“å‘å¸ƒæ—¶ç¬¬ä¸€æ—¶é—´ä½“éªŒã€‚

"Wait" is all you need.

AI News for 2/5/2025-2/6/2025. We checked 7 subreddits, 433 Twitters and 29 Discords (210 channels, and 4396 messages) for you. Estimated reading time saved (at 200wpm): 490 minutes. You can now tag @smol_ai for AINews discussions!

2025 å¹´ 2 æœˆ 5 æ—¥ - 2025 å¹´ 2 æœˆ 6 æ—¥çš„äººå·¥æ™ºèƒ½æ–°é—»é€Ÿé€’ã€‚æˆ‘ä»¬ä¸ºä½ æ•´ç†äº†æ¥è‡ª 7 ä¸ª Reddit å­ç‰ˆå—ã€433 ä¸ª Twitter è´¦å·å’Œ 29 ä¸ª Discord ç¤¾ç¾¤ï¼ˆåŒ…å« 210 ä¸ªé¢‘é“ï¼Œå…±è®¡ 4396 æ¡æ¶ˆæ¯ï¼‰çš„æœ€æ–°åŠ¨æ€ã€‚é¢„è®¡ä¸ºä½ èŠ‚çœçš„é˜…è¯»æ—¶é—´ï¼ˆæŒ‰æ¯åˆ†é’Ÿé˜…è¯» 200 å­—è®¡ç®—)ï¼š490 åˆ†é’Ÿã€‚æ¬¢è¿ä½¿ç”¨ @smol_ai æ ‡ç­¾å‚ä¸ AINewsï¼ˆAI Newsï¼‰ç›¸å…³è®¨è®ºï¼

We're regrettably late to covering this paper, but late is better than never. s1: Simple test-time scaling documents a new reasoning model with 2 novel contributions:

æˆ‘ä»¬è™½ç„¶æ™šäº›æ‰å…³æ³¨åˆ°è¿™ç¯‡è®ºæ–‡ï¼Œä½†è¿Ÿåˆ°æ€»æ¯”é”™è¿‡å¥½ã€‚s1: Simple test-time scaling æå‡ºäº†ä¸€ç§æ–°çš„æ¨ç†æ¨¡å‹ï¼Œå…·æœ‰ä¸¤é¡¹åˆ›æ–°æ€§è´¡çŒ®ï¼š

1 finetuned from Qwen 2.5 32B on just 1000 questions paired with reasoning traces distilled from Gemini 2.0 Flash Thinking, filtered for difficulty, diversity, and quality (26 mins of training on 16 H100s)

åŸºäº Qwen 2.5 32B å¾®è°ƒï¼Œä»…ä½¿ç”¨ 1000 ä¸ªé™„å¸¦æ¨ç†è½¨è¿¹çš„é—®é¢˜è¿›è¡Œè®­ç»ƒï¼Œè¿™äº›æ¨ç†è½¨è¿¹æºè‡ª Gemini 2.0 Flash Thinkingï¼Œç»è¿‡ä¸¥æ ¼ç­›é€‰ï¼Œä»¥ç¡®ä¿åœ¨éš¾åº¦ã€å¤šæ ·æ€§å’Œè´¨é‡æ–¹é¢çš„é«˜æ ‡å‡†ï¼ˆåœ¨ 16 å— H100 ä¸Šä»…éœ€ 26 åˆ†é’Ÿè®­ç»ƒï¼‰ã€‚

2 controllable test-time compute by either forcefully terminating the model's thinking process or lengthening it by appending "Wait" multiple times to the model's generation when it tries to end.

æµ‹è¯•æ—¶è®¡ç®—é‡å¯æ§ï¼šæ—¢å¯ä»¥é€šè¿‡å¼ºåˆ¶ç»ˆæ­¢æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹æ¥ç¼©çŸ­è®¡ç®—æ—¶é—´ï¼Œä¹Ÿå¯ä»¥åœ¨æ¨¡å‹å°è¯•ç»“æŸç”Ÿæˆæ—¶å¤šæ¬¡æ·»åŠ ã€ŒWaitã€ï¼Œä»¥å»¶é•¿å…¶æ¨ç†è¿‡ç¨‹ã€‚

Lead author Niklas Muennighoff, who notably worked on Bloom, StarCoder, MTEB, and contributed to BIG-bench, notes that this second trick reproduces the famous o1 scaling chart:

è®ºæ–‡çš„ç¬¬ä¸€ä½œè€… Niklas Muennighoff æ›¾å‚ä¸ Bloomã€StarCoder å’Œ MTEB çš„å¼€å‘ï¼Œå¹¶ä¸º BIG-bench åšå‡ºé‡è¦è´¡çŒ®ã€‚ä»–æŒ‡å‡ºï¼Œç¬¬äºŒä¸ªæŠ€å·§èƒ½å¤ŸæˆåŠŸå¤ç°è‘—åçš„ o1 scaling æ›²çº¿ã€‚

Compared to Bespoke-Stratos (our coverage here), the filtering is also remarkably sample efficient.

ä¸ Bespoke-Stratosï¼ˆæœ¬æ–‡å°†å¯¹å…¶è¿›è¡Œä»‹ç»ï¼‰ç›¸æ¯”ï¼Œè¿™ç§è¿‡æ»¤æ–¹æ³•åœ¨æ ·æœ¬æ•ˆç‡æ–¹é¢ä¹Ÿè¡¨ç°å¾—éå¸¸ä¼˜ç§€ã€‚

We would also recommend Simonw and Tim Kellogg's explainers.

æˆ‘ä»¬åŒæ ·ä¼šæ¨è Simonw å’Œ Tim Kellogg çš„ç›¸å…³æ–‡ç« ã€‚

Honorable mention today:

ä»Šå¤©ç‰¹åˆ«å€¼å¾—å…³æ³¨ï¼š

Kyutai Moshi made a splash last year (our coverage here) for its realtime voice with inner monologue, and now Hibiki shows very impressive French-English live translation offline on an iPhone. Not bad for an intern project.

Kyutai Moshi å»å¹´å› å…¶å…·æœ‰å†…å¿ƒç‹¬ç™½çš„å®æ—¶è¯­éŸ³åŠŸèƒ½è€Œå¤‡å—ç©ç›®ï¼ˆç›¸å…³æŠ¥é“è¯·è§æ­¤å¤„ï¼‰ã€‚ç°åœ¨ï¼ŒHibiki åœ¨ iPhone ä¸Šå±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ³•è¯­ - è‹±è¯­å®æ—¶ç¦»çº¿ç¿»è¯‘ï¼Œè¿™å¯¹äºä¸€ä¸ªå®ä¹ é¡¹ç›®è€Œè¨€ï¼Œå®å±ä¸æ˜“ã€‚

Table of Contents

AI Twitter Recap
AI Reddit Recap
/r/LocalLlama Recap
Other AI Subreddit Recap
AI Discord Recap
PART 1: High level Discord summaries
Unsloth AI (Daniel Han) Discord
Stability.ai (Stable Diffusion) Discord
Codeium (Windsurf) Discord
aider (Paul Gauthier) Discord
OpenAI Discord
Cursor IDE Discord
Perplexity AI Discord
OpenRouter (Alex Atallah) Discord
LM Studio Discord
MCP (Glama) Discord
Yannick Kilcher Discord
Eleuther Discord
Nous Research AI Discord
Interconnects (Nathan Lambert) Discord
Notebook LM Discord
LLM Agents (Berkeley MOOC) Discord
GPU MODE Discord
Nomic.ai (GPT4All) Discord
Torchtune Discord
Modular (Mojo ğŸ”¥) Discord
Latent Space Discord
LlamaIndex Discord
MLOps @Chipro Discord
Cohere Discord
Gorilla LLM (Berkeley Function Calling) Discord
DSPy Discord
PART 2: Detailed by-Channel summaries and links
Unsloth AI (Daniel Han) â–· #general (516 messagesğŸ”¥ğŸ”¥ğŸ”¥):
Unsloth AI (Daniel Han) â–· #announcements (1 messages):
Unsloth AI (Daniel Han) â–· #off-topic (38 messagesğŸ”¥):
Unsloth AI (Daniel Han) â–· #help (188 messagesğŸ”¥ğŸ”¥):
Unsloth AI (Daniel Han) â–· #showcase (1 messages):
Unsloth AI (Daniel Han) â–· #research (6 messages):
Stability.ai (Stable Diffusion) â–· #announcements (1 messages):
Stability.ai (Stable Diffusion) â–· #general-chat (459 messagesğŸ”¥ğŸ”¥ğŸ”¥):
Codeium (Windsurf) â–· #announcements (3 messages):
Codeium (Windsurf) â–· #discussion (31 messagesğŸ”¥):
Codeium (Windsurf) â–· #windsurf (345 messagesğŸ”¥ğŸ”¥):
aider (Paul Gauthier) â–· #general (337 messagesğŸ”¥ğŸ”¥):
aider (Paul Gauthier) â–· #questions-and-tips (23 messagesğŸ”¥):
aider (Paul Gauthier) â–· #links (2 messages):
OpenAI â–· #ai-discussions (276 messagesğŸ”¥ğŸ”¥):
OpenAI â–· #gpt-4-discussions (5 messages):
OpenAI â–· #prompt-engineering (6 messages):
OpenAI â–· #api-discussions (6 messages):
Cursor IDE â–· #general (282 messagesğŸ”¥ğŸ”¥):
Perplexity AI â–· #general (247 messagesğŸ”¥ğŸ”¥):
Perplexity AI â–· #sharing (22 messagesğŸ”¥):
Perplexity AI â–· #pplx-api (7 messages):
OpenRouter (Alex Atallah) â–· #announcements (14 messagesğŸ”¥):
OpenRouter (Alex Atallah) â–· #app-showcase (1 messages):
OpenRouter (Alex Atallah) â–· #general (242 messagesğŸ”¥ğŸ”¥):
LM Studio â–· #general (215 messagesğŸ”¥ğŸ”¥):
LM Studio â–· #hardware-discussion (23 messagesğŸ”¥):
MCP (Glama) â–· #general (97 messagesğŸ”¥ğŸ”¥):
MCP (Glama) â–· #showcase (54 messagesğŸ”¥):
Yannick Kilcher â–· #general (112 messagesğŸ”¥ğŸ”¥):
Yannick Kilcher â–· #paper-discussion (10 messagesğŸ”¥):
Yannick Kilcher â–· #ml-news (10 messagesğŸ”¥):
Eleuther â–· #general (22 messagesğŸ”¥):
Eleuther â–· #research (92 messagesğŸ”¥ğŸ”¥):
Eleuther â–· #interpretability-general (1 messages):
Eleuther â–· #lm-thunderdome (7 messages):
Eleuther â–· #gpt-neox-dev (1 messages):
Nous Research AI â–· #general (100 messagesğŸ”¥ğŸ”¥):
Nous Research AI â–· #ask-about-llms (1 messages):
Nous Research AI â–· #research-papers (1 messages):
Nous Research AI â–· #interesting-links (3 messages):
Nous Research AI â–· #research-papers (1 messages):
Interconnects (Nathan Lambert) â–· #news (39 messagesğŸ”¥):
Interconnects (Nathan Lambert) â–· #ml-questions (3 messages):
Interconnects (Nathan Lambert) â–· #ml-drama (9 messagesğŸ”¥):
Interconnects (Nathan Lambert) â–· #random (23 messagesğŸ”¥):
Interconnects (Nathan Lambert) â–· #memes (2 messages):
Interconnects (Nathan Lambert) â–· #rl (11 messagesğŸ”¥):
Interconnects (Nathan Lambert) â–· #reads (8 messagesğŸ”¥):
Interconnects (Nathan Lambert) â–· #policy (1 messages):
Notebook LM â–· #use-cases (17 messagesğŸ”¥):
Notebook LM â–· #general (78 messagesğŸ”¥ğŸ”¥):
LLM Agents (Berkeley MOOC) â–· #mooc-announcements (1 messages):
LLM Agents (Berkeley MOOC) â–· #mooc-questions (57 messagesğŸ”¥ğŸ”¥):
GPU MODE â–· #general (13 messagesğŸ”¥):
GPU MODE â–· #triton (4 messages):
GPU MODE â–· #cuda (3 messages):
GPU MODE â–· #algorithms (8 messagesğŸ”¥):
GPU MODE â–· #cool-links (1 messages):
GPU MODE â–· #torchao (2 messages):
GPU MODE â–· #off-topic (2 messages):
GPU MODE â–· #thunderkittens (1 messages):
GPU MODE â–· #reasoning-gym (18 messagesğŸ”¥):
Nomic.ai (GPT4All) â–· #general (48 messagesğŸ”¥):
Torchtune â–· #general (30 messagesğŸ”¥):
Torchtune â–· #dev (16 messagesğŸ”¥):
Modular (Mojo ğŸ”¥) â–· #general (2 messages):
Modular (Mojo ğŸ”¥) â–· #mojo (24 messagesğŸ”¥):
Modular (Mojo ğŸ”¥) â–· #max (16 messagesğŸ”¥):
Latent Space â–· #ai-general-chat (36 messagesğŸ”¥):
LlamaIndex â–· #blog (2 messages):
LlamaIndex â–· #general (4 messages):
MLOps @Chipro â–· #general-ml (6 messages):
Cohere â–· #api-discussions (6 messages):
Gorilla LLM (Berkeley Function Calling) â–· #discussion (4 messages):
DSPy â–· #papers (1 messages):
DSPy â–· #examples (2 messages):
AI Twitter Recap
AI Models and Releases


### AI Twitter Recap

AI Twitter åŠ¨æ€å›é¡¾

AI æ¨¡å‹ä¸ç‰ˆæœ¬å‘å¸ƒ

DeepSeek R1 and R3 Open Source Release: @teortaxesTex announced that R1-low-mid-high models are coming soon, potentially marking the first real Open Source moment in LLMs comparable to nginx, Blender, or even Linux. This release could flatten the market owned by a cartel of incumbents with proprietary tech.
Hugging Face Releases SmolLM2: @_akhaliq shared that Hugging Face announced SmolLM2, detailed in the paper "When Smol Goes Big -- Data-Centric Training of a Small Language Model". @LoubnaBenAllal1 provided a breakdown of the SmolLM2 paper, emphasizing that data is the secret sauce behind strong performance in small LMs.
IBM's Granite-Vision-3.1-2B Model: @mervenoyann discussed the release of Granite-Vision-3.1-2B, a small vision language model with impressive performance on various tasks. A notebook is available to test the model.
AI Research Papers and Findings

DeepSeek R1 å’Œ R3 å¼€æºå‘å¸ƒï¼š@teortaxesTex å®£å¸ƒ DeepSeek R1 ç³»åˆ—çš„ä¸åŒè§„æ¨¡æ¨¡å‹ï¼ˆlow-mid-highï¼‰å³å°†æ¨å‡ºï¼Œè¿™å¯èƒ½æ ‡å¿—ç€å¤§è¯­è¨€æ¨¡å‹é¢†åŸŸè¿æ¥äº†é¦–ä¸ªçœŸæ­£æ„ä¹‰ä¸Šçš„å¼€æºæ—¶åˆ»ï¼Œå…¶å½±å“å ªæ¯” nginxã€Blender ä¹ƒè‡³ Linuxã€‚æ­¤ä¸¾å¯èƒ½ä¼šæ‰“ç ´å½“å‰ç”±å°‘æ•°æ‹¥æœ‰ä¸“æœ‰æŠ€æœ¯çš„ä¼ä¸šå·¨å¤´æ‰€å„æ–­çš„å¸‚åœºæ ¼å±€ã€‚
Hugging Face å‘å¸ƒ SmolLM2ï¼š@_akhaliq åˆ†äº«è¯´ Hugging Face å‘å¸ƒäº† SmolLM2ï¼Œç›¸å…³ç»†èŠ‚åœ¨è®ºæ–‡ã€ŠWhen Smol Goes Big -- Data-Centric Training of a Small Language Modelã€‹ ï¼ˆå½“ã€Œå°ã€å˜ã€Œå¤§ã€â€”â€” ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„è®­ç»ƒå°å‹è¯­è¨€æ¨¡å‹ï¼‰ä¸­æœ‰æ‰€é˜è¿°ã€‚@LoubnaBenAllal1 å¯¹ SmolLM2 è®ºæ–‡è¿›è¡Œäº†æ·±å…¥è§£è¯»ï¼Œå¼ºè°ƒæ•°æ®æ˜¯å°å‹è¯­è¨€æ¨¡å‹å®ç°å“è¶Šæ€§èƒ½çš„å…³é”®ã€‚
IBM çš„ Granite-Vision-3.1-2B æ¨¡å‹ï¼š@mervenoyann è®¨è®ºäº† IBM å‘å¸ƒ Granite-Vision-3.1-2Bï¼Œè¿™æ˜¯ä¸€ä¸ªå°å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œåœ¨å„é¡¹ä»»åŠ¡ä¸­å‡å±•ç°å‡ºä»¤äººç©ç›®çš„æ€§èƒ½ã€‚ç°å·²æä¾› notebook ä»¥ä¾›æµ‹è¯•è¯¥æ¨¡å‹ã€‚

AI ç ”ç©¶è®ºæ–‡å’Œå‘ç°

LIMO: Less is More for Reasoning: @_akhaliq highlighted LIMO, showing that complex reasoning capabilities can emerge through minimal but precisely crafted demonstrations. @arankomatsuzaki noted that LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH with only 817 training samples, significantly outperforming previous approaches.
Token-Assisted Reasoning: @_akhaliq shared insights from the paper "Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning", discussing how combining latent and text tokens can enhance reasoning in language models.
Advancements in Long Chains of Thought: @gneubig presented research providing insights on short vs. long chains of thought, the role of supervised fine-tuning vs. reinforcement learning, and methods to control reasoning length in language models.
AI Tools and Platforms

LIMOï¼šæ¨ç†ï¼Œå°‘å³æ˜¯å¤šï¼š@_akhaliq é‡ç‚¹ä»‹ç»äº† LIMOï¼Œå®ƒè¡¨æ˜ï¼Œå³ä½¿æ˜¯æœ€å°‘ä½†ç»è¿‡ç²¾å¿ƒè®¾è®¡çš„æ¼”ç¤ºï¼Œä¹Ÿèƒ½æ¶Œç°å‡ºå¤æ‚çš„æ¨ç†èƒ½åŠ›ã€‚@arankomatsuzaki æŒ‡å‡ºï¼ŒLIMO ä»…ä½¿ç”¨ 817 ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œå°±åœ¨ AIME ä¸Šå®ç°äº† 57.1% çš„å‡†ç¡®ç‡ï¼Œåœ¨ MATH ä¸Šå®ç°äº† 94.8% çš„å‡†ç¡®ç‡ï¼Œæ˜æ˜¾ä¼˜äºä»¥å¾€çš„æ–¹æ³•ã€‚

Token è¾…åŠ©çš„æ¨ç†ï¼š@_akhaliq åˆ†äº«äº†è®ºæ–‡ã€ŠToken Assortedï¼šMixing Latent and Text Tokens for Improved Language Model Reasoningã€‹ä¸­çš„è§è§£ï¼Œè®¨è®ºäº†å¦‚ä½•å°†æ½œåœ¨çš„ï¼ˆlatentï¼‰å’Œæ–‡æœ¬ Token ç»“åˆèµ·æ¥ï¼Œä»è€Œå¢å¼ºè¯­è¨€æ¨¡å‹ä¸­çš„æ¨ç†èƒ½åŠ›ã€‚

é•¿é“¾æ€è€ƒçš„è¿›å±•ï¼š@gneubig ä»‹ç»äº†ä¸€é¡¹ç ”ç©¶ï¼Œè¯¥ç ”ç©¶æ·±å…¥æ¢è®¨äº†çŸ­é“¾æ€è€ƒä¸é•¿é“¾æ€è€ƒä¹‹é—´çš„åŒºåˆ«ï¼Œç›‘ç£å¼å¾®è°ƒä¸å¼ºåŒ–å­¦ä¹ æ‰€æ‰®æ¼”çš„è§’è‰²ï¼Œä»¥åŠåœ¨è¯­è¨€æ¨¡å‹ä¸­æ§åˆ¶æ¨ç†é•¿åº¦çš„æ–¹æ³•ã€‚

AI å·¥å…·å’Œå¹³å°

Gradio DualVision App: @_akhaliq introduced DualVision, a Gradio template app for image processing featuring multi-modal predictions, GPU support, and an examples gallery for enhanced user experience.
Le Chat by Mistral AI Now on Mobile: @sophiamyang announced the release of Le Chat, an AI assistant by Mistral AI, now available on mobile platforms with features like code interpreter and blazing-fast responses powered by the Mistral models.
Canvas Sharing in ChatGPT: @OpenAIDevs announced that canvas sharing is now live in ChatGPT, allowing users to share, interact with, or edit canvases, enhancing collaborative capabilities.
AI Industry News and Events

Gradio DualVision åº”ç”¨ï¼š@_akhaliq ä»‹ç»äº† DualVisionï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäº Gradio çš„å›¾åƒå¤„ç†åº”ç”¨æ¨¡æ¿ï¼Œå®ƒæ”¯æŒå¤šæ¨¡æ€é¢„æµ‹ï¼Œå¹¶æä¾› GPU åŠ é€Ÿä»¥åŠç¤ºä¾‹å›¾åº“ï¼Œä»è€Œ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ ç”¨æˆ·ä½“éªŒã€‚
Mistral AI çš„ Le Chat ç§»åŠ¨ç‰ˆå‘å¸ƒï¼š@sophiamyang å®£å¸ƒ Mistral AI çš„ AI åŠ©æ‰‹ Le Chat å‘å¸ƒäº†ç§»åŠ¨ç‰ˆæœ¬ã€‚è¯¥ç‰ˆæœ¬å…·æœ‰ä»£ç è§£é‡Šå™¨ç­‰åŠŸèƒ½ï¼Œå¹¶ç”± Mistral æ¨¡å‹é©±åŠ¨ï¼Œå“åº”é€Ÿåº¦æå¿«ã€‚
ChatGPT æ¨å‡ºç”»å¸ƒå…±äº«åŠŸèƒ½ï¼š@OpenAIDevs å®£å¸ƒ ChatGPT ç°å·²æ”¯æŒç”»å¸ƒå…±äº«ï¼Œç”¨æˆ·å¯ä»¥å…±äº«ã€äº’åŠ¨æˆ–ç¼–è¾‘ç”»å¸ƒï¼Œä»è€Œå¢å¼ºåä½œèƒ½åŠ›ã€‚
AI è¡Œä¸šæ–°é—»å’Œäº‹ä»¶

Applied ML Days Workshops with Google DeepMind: @GoogleDeepMind invited participants to two workshops at Applied ML Days focused on Building LLM Applications using Google Gemini and Natural Interactions with Foundational Models.
Cerebras Powers Leading AI Lab: @draecomino shared that Cerebras is now powering a leading AI lab in production, showcasing advancements in AI infrastructure and computing capabilities.
Keras Community Meeting: @fchollet announced a public Keras team community meeting, offering updates on what's new in Keras and an opportunity for developers to ask questions.
Personal Achievements and Updates

Applied ML Days ç ”è®¨ä¼šä¸ Google DeepMindï¼š@GoogleDeepMind é‚€è¯·å‚ä¸è€…å‚åŠ åœ¨ Applied ML Days ä¸¾åŠçš„ä¸¤åœºç ”è®¨ä¼šï¼Œé‡ç‚¹æ˜¯ä½¿ç”¨ Google Gemini æ„å»ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM/Large Language Modelï¼‰åº”ç”¨ç¨‹åºï¼Œä»¥åŠä¸åŸºåº§æ¨¡å‹ï¼ˆFoundational Modelsï¼‰çš„è‡ªç„¶äº¤äº’ã€‚
Cerebras ä¸ºé¢†å…ˆçš„ AI å®éªŒå®¤æä¾›æ”¯æŒï¼š@draecomino åˆ†äº«è¯´ï¼ŒCerebras ç›®å‰ä¸ºä¸€å®¶é¢†å…ˆçš„ AI å®éªŒå®¤æä¾›ç”Ÿäº§ç®—åŠ›æ”¯æŒï¼Œå±•ç¤ºäº†åœ¨ AI åŸºç¡€è®¾æ–½å’Œè®¡ç®—èƒ½åŠ›æ–¹é¢çš„è¿›æ­¥ã€‚
Keras ç¤¾åŒºä¼šè®®ï¼š@fchollet å®£å¸ƒå°†ä¸¾è¡Œ Keras å›¢é˜Ÿçš„å…¬å¼€ç¤¾åŒºä¼šè®®ï¼Œä¼šä¸Šå°†ä»‹ç» Keras çš„æœ€æ–°è¿›å±•ï¼Œå¹¶ä¸ºå¼€å‘è€…æä¾›æé—®çš„æœºä¼šã€‚
ä¸ªäººæˆå°±å’Œæ›´æ–°

Google Developers India Recognition: @RisingSayak expressed gratitude for being nominated and thanked @GoogleDevsIN for the recognition, highlighting a sense of fulfillment in the community.
Philipp Schmid Joins Google DeepMind: @osanseviero welcomed Philipp Schmid to Google DeepMind, expressing excitement to work with a dream team including @DynamicWebPaige, @film_girl, and others.
Memes/Humor

Google å°åº¦å¼€å‘è€…ç¤¾åŒºçš„è®¤å¯ï¼š@RisingSayak å¯¹è·å¾—æåè¡¨ç¤ºæ„Ÿè°¢ï¼Œå¹¶æ„Ÿè°¢ @GoogleDevsIN çš„è®¤å¯ï¼Œä»–å¼ºè°ƒè¿™è®©ä»–æ„Ÿåˆ°åœ¨ç¤¾åŒºä¸­å®ç°äº†ä»·å€¼ã€‚
Philipp Schmid åŠ å…¥ Google DeepMindï¼š@osanseviero æ¬¢è¿ Philipp Schmid åŠ å…¥ Google DeepMindï¼Œå¹¶è¡¨ç¤ºéå¸¸æœŸå¾…ä¸åŒ…æ‹¬ @DynamicWebPaigeã€@film_girl åœ¨å†…çš„ä¼˜ç§€å›¢é˜Ÿæˆå‘˜åˆä½œã€‚
æ¨¡å›  / å¹½é»˜

Types of Programmers: @hyhieu226 humorously categorized programmers into two types: those who write verbose type declarations and those who use 'auto' for simplicity.
Overconfidence Warning: @qtnx_ shared a personal reflection reminding that overconfidence can lead to loss, advising to stay humble and work diligently.
AI Lab Grifters: @scaling01 called out YouTube grifters in the AI community, highlighting a shift from dismissing AI advancements to monetizing them, implying a focus on profit over technology.
AI Reddit Recap
/r/LocalLlama Recap
Theme 1. Hibiki Speech-to-Speech Translation - FR to EN Capability

ç¨‹åºå‘˜çš„ç±»å‹ï¼š@hyhieu226 å¹½é»˜åœ°å°†ç¨‹åºå‘˜åˆ†ä¸ºä¸¤ç§ç±»å‹ï¼šä¸€ç§æ˜¯ç¼–å†™å¤§é‡ç±»å‹å£°æ˜çš„ç¨‹åºå‘˜ï¼Œå¦ä¸€ç§æ˜¯ä½¿ç”¨ 'auto' æ¥ç®€åŒ–ä»£ç çš„ç¨‹åºå‘˜ã€‚
è¿‡åº¦è‡ªä¿¡è­¦å‘Šï¼š@qtnx_ åˆ†äº«äº†ä¸€ä¸ªä¸ªäººåæ€ï¼Œæé†’è¯´è¿‡åº¦è‡ªä¿¡ä¼šå¯¼è‡´å¤±è´¥ï¼Œå»ºè®®ä¿æŒè°¦é€Šå’ŒåŠªåŠ›å·¥ä½œã€‚
AI å®éªŒå®¤éª—å­ï¼š@scaling01 æŒ‡å‡ºäº† AI ç¤¾åŒºä¸­çš„ YouTube éª—å­ï¼Œå¼ºè°ƒäº†ä¸€ç§ä»å¿½ç•¥ AI è¿›å±•åˆ°åˆ©ç”¨ AI å˜ç°çš„è½¬å˜ï¼Œæš—ç¤ºäº†å¯¹åˆ©æ¶¦çš„å…³æ³¨è¶…è¿‡äº†å¯¹æŠ€æœ¯çš„å…³æ³¨ã€‚
AI Reddit è®ºå›æ‘˜è¦
/r/LocalLlama è®ºå›æ‘˜è¦ä¸»é¢˜ 1. Hibiki è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘ - FR åˆ° EN çš„èƒ½åŠ›

Hibiki by kyutai, a simultaneous speech-to-speech translation model, currently supporting FR to EN (Score: 448, Comments: 40): Hibiki, developed by Kyutai, is a simultaneous speech-to-speech translation model that currently supports translation from French (FR) to English (EN).
Hibiki's Capabilities: Hibiki is praised for its real-time translation quality, naturalness, and speaker similarity, with resources available on GitHub and Hugging Face. The model's ability to preserve the speaker's voice while adapting its pace to the semantic content is highlighted, and it is noted for outperforming previous systems.
Community Feedback and Requests: Users express admiration for the model's performance, with some desiring additional language support, particularly Spanish and Chinese. There is a desire for an on-device version for convenience and travel purposes, especially for non-English speaking regions.
Cultural and Developmental Observations: There are humorous remarks about the French's proficiency in English and the Japanese-inspired names of the French-developed model. The open-source nature of the project, similar to Mistral, is noted, with expectations for future advancements in on-device translation capabilities.
Theme 2. Challenges with Gemini 2.0 Pro Experimental Model

Hibikiï¼Œkyutai å‡ºå“çš„å®æ—¶è¯­éŸ³ç¿»è¯‘æ¨¡å‹ï¼Œç›®å‰æ”¯æŒæ³•è¯­åˆ°è‹±è¯­ï¼ˆFR to ENï¼‰(åˆ†æ•°ï¼š448ï¼Œè¯„è®ºï¼š40)ï¼šHibiki ç”± Kyutai å¼€å‘ï¼Œæ˜¯ä¸€æ¬¾å®æ—¶è¯­éŸ³ç¿»è¯‘æ¨¡å‹ï¼Œç›®å‰æ”¯æŒæ³•è¯­ï¼ˆFRï¼‰åˆ°è‹±è¯­ï¼ˆENï¼‰çš„ç¿»è¯‘ã€‚
Hibiki çš„èƒ½åŠ›ï¼šHibiki å› å…¶å‡ºè‰²çš„å®æ—¶ç¿»è¯‘è´¨é‡ã€è‡ªç„¶åº¦å’Œè¯´è¯äººç›¸ä¼¼åº¦è€Œå¤‡å—èµèª‰ï¼ŒGitHub å’Œ Hugging Face ä¸Šæä¾›äº†ç›¸å…³èµ„æºã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿä¿ç•™è¯´è¯äººçš„å£°éŸ³ï¼Œå¹¶æ ¹æ®è¯­ä¹‰å†…å®¹è°ƒæ•´è¯­é€Ÿï¼Œè¿™ä¸€ç‚¹å¤‡å—å…³æ³¨ï¼Œä¸”å…¶æ€§èƒ½ä¼˜äºä»¥å¾€çš„ç³»ç»Ÿã€‚
ç¤¾åŒºåé¦ˆå’Œè¯·æ±‚ï¼šç”¨æˆ·å¯¹è¯¥æ¨¡å‹çš„æ€§èƒ½è¡¨ç¤ºèµèµï¼Œå¹¶å¸Œæœ›å¢åŠ å¯¹å…¶ä»–è¯­è¨€çš„æ”¯æŒï¼Œå°¤å…¶æ˜¯è¥¿ç­ç‰™è¯­å’Œä¸­æ–‡ã€‚ç”¨æˆ·å¸Œæœ›æ¨å‡ºæ–¹ä¾¿æ—…è¡Œçš„ä¾¿æºç‰ˆæœ¬ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹éè‹±è¯­åœ°åŒºã€‚
æ–‡åŒ–å’Œå‘å±•è§‚å¯Ÿï¼šå‡ºç°äº†ä¸€äº›æœ‰è¶£çš„è¯„è®ºï¼Œå†…å®¹æ¶‰åŠæ³•å›½äººçš„è‹±è¯­æ°´å¹³ï¼Œä»¥åŠè¿™æ¬¾ç”±æ³•å›½å¼€å‘çš„æ¨¡å‹é‡‡ç”¨äº†å…·æœ‰æ—¥æœ¬é£æ ¼çš„åå­—ã€‚è¯¥é¡¹ç›®ä¸ Mistral ç±»ä¼¼ï¼Œéƒ½é‡‡ç”¨äº†å¼€æºæ¨¡å¼ï¼Œäººä»¬æœŸæœ›æœªæ¥èƒ½åœ¨æœ¬åœ°ç¿»è¯‘èƒ½åŠ›æ–¹é¢å–å¾—æ›´å¤šè¿›å±•ã€‚
ä¸»é¢˜ 2. Gemini 2.0 Pro å®éªŒæ¨¡å‹é¢ä¸´çš„æŒ‘æˆ˜

The New Gemini Pro 2.0 Experimental sucks Donkey Balls. (Score: 205, Comments: 83): The author criticizes the Gemini 2.0 Pro Experimental model for its poor performance compared to the previous 1206 model, highlighting issues like frequent mistakes and unwanted code refactoring. They express frustration with Google's pattern of releasing models that regress in quality, contrasting it with the impressive speed and efficiency of the Flesh light 2.0 for OCR tasks.
Many users express dissatisfaction with Gemini 2.0 Pro Experimental, noting issues like decreased intelligence and increased speed at the cost of quality, with some preferring the older 1206 model or other models like Flash 2.0 for better performance in specific tasks like coding and creative writing.
Flash 2.0 and o1 models are praised for their effectiveness, especially in handling complex queries and maintaining context over longer tasks, while newer models like o3-mini are criticized for requiring more verbose input to understand user intent, leading to inefficiencies.
The discussion highlights a broader trend where AI models are becoming faster and more efficient but at the expense of depth and consistency, with some users pointing out the limitations of current evaluation metrics and the challenges of balancing speed with quality in real-world applications.
Theme 3. Open WebUI Releases Code Interpreter and Exa Search Features

New Gemini Pro 2.0 Experimental è¡¨ç°ä»¤äººå¤±æœ›ï¼ˆå¾—åˆ†ï¼š205ï¼Œè¯„è®ºï¼š83)ï¼šä½œè€…æ‰¹è¯„ Gemini 2.0 Pro Experimental æ¨¡å‹ï¼Œè®¤ä¸ºå…¶æ€§èƒ½ä¸å¦‚ä¹‹å‰çš„ 1206 æ¨¡å‹ï¼Œå¹¶ç€é‡æŒ‡å‡ºäº†é¢‘ç¹å‡ºé”™å’Œä¸å¿…è¦çš„ä»£ç é‡æ„ç­‰é—®é¢˜ã€‚ä»–ä»¬å¯¹ Google é¢‘ç¹å‘å¸ƒè´¨é‡å€’é€€çš„æ¨¡å‹æ„Ÿåˆ° frustatedï¼Œå¹¶å°†å…¶ä¸ Flesh light 2.0 åœ¨ OCR ä»»åŠ¡ä¸­ä»¤äººå°è±¡æ·±åˆ»çš„é€Ÿåº¦å’Œæ•ˆç‡è¿›è¡Œäº†å¯¹æ¯”ã€‚
è®¸å¤šç”¨æˆ·å¯¹ Gemini 2.0 Pro Experimental è¡¨ç¤ºä¸æ»¡ï¼Œä»–ä»¬å‘ç°è¯¥æ¨¡å‹çš„æ™ºèƒ½æœ‰æ‰€ä¸‹é™ï¼Œå¹¶ä¸”ä¸ºäº†æé«˜é€Ÿåº¦è€Œç‰ºç‰²äº†è´¨é‡ã€‚ä¸€äº›ç”¨æˆ·æ›´å–œæ¬¢ä¹‹å‰çš„ 1206 æ¨¡å‹ï¼Œæˆ–è€… Flash 2.0 ç­‰å…¶ä»–æ¨¡å‹ï¼Œä»¥ä¾¿åœ¨ä»£ç ç¼–å†™å’Œåˆ›æ„å†™ä½œç­‰ç‰¹å®šä»»åŠ¡ä¸­è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚
Flash 2.0 å’Œ o1 æ¨¡å‹å› å…¶æœ‰æ•ˆæ€§è€Œå¤‡å—èµèª‰ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚æŸ¥è¯¢å’Œåœ¨é•¿æ—¶é—´ä»»åŠ¡ä¸­ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§æ–¹é¢ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œè¾ƒæ–°çš„ o3-mini æ¨¡å‹åˆ™å› éœ€è¦æ›´è¯¦ç»†çš„è¾“å…¥æ‰èƒ½ç†è§£ç”¨æˆ·æ„å›¾è€Œå—åˆ°æ‰¹è¯„ï¼Œè¿™å¯¼è‡´äº†æ•ˆç‡é™ä½ã€‚
è®¨è®ºçªå‡ºäº†ä¸€ç§æ™®éè¶‹åŠ¿ï¼šAI æ¨¡å‹åœ¨è¿½æ±‚é€Ÿåº¦å’Œæ•ˆç‡çš„åŒæ—¶ï¼Œç‰ºç‰²äº†æ·±åº¦å’Œä¸€è‡´æ€§ã€‚ä¸€äº›ç”¨æˆ·æŒ‡å‡ºï¼Œå½“å‰çš„è¯„ä¼°æŒ‡æ ‡å­˜åœ¨å±€é™æ€§ï¼Œå¹¶ä¸”åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¹³è¡¡é€Ÿåº¦ä¸è´¨é‡ä»ç„¶æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚
ä¸»é¢˜ 3. Open WebUI å‘å¸ƒä»£ç è§£é‡Šå™¨ï¼ˆCode Interpreterï¼‰å’Œ Exa æœç´¢åŠŸèƒ½ï¼ˆExa Search Features)

Open WebUI drops 3 new releases today. Code Interpreter, Native Tool Calling, Exa Search added (Score: 185, Comments: 61): Open WebUI introduced significant updates in version 0.5.8, including a Code Interpreter that executes code in real-time using Pyodide, a redesigned chat input UI, and Exa Search Engine Integration for retrieving information within the chat. Additionally, Native Tool Calling Support is now available experimentally, promising reduced query latency and improved contextual responses. Release details are available online.
Code Interpreter and Pyodide: Users appreciate the addition of the code interpreter using Pyodide, noting its limitations but recognizing its utility for common use cases. There's a call for improvements such as integrating Gradio and enabling result downloads, like plots or processed data.
Community Contributions: Despite many contributors, tjbck is acknowledged as the primary consistent contributor to Open WebUI, with suggestions to support them through GitHub sponsorship. The project is praised for its rapid feature updates and its competitive edge over proprietary UIs.
Document Handling and RAG: There are criticisms regarding document handling, particularly the use of simple vector DB RAG for single document references, which often fails on simple queries. Suggestions include moving document, RAG, and search functionalities to separate pipelines to keep up with fast-moving advancements, and disabling RAG by default for better user control.
Theme 4. Over-Tokenized Transformer Enhances LLM Performance

Open WebUI ä»Šå¤©å‘å¸ƒäº†ä¸‰ä¸ªæ–°ç‰ˆæœ¬ã€‚ä»£ç è§£é‡Šå™¨ã€åŸç”Ÿå·¥å…·è°ƒç”¨ã€Exa æœç´¢åŠŸèƒ½å·²åŠ å…¥ï¼ˆå¾—åˆ†ï¼š185ï¼Œè¯„è®ºï¼š61ï¼‰ã€‚Open WebUI åœ¨ 0.5.8 ç‰ˆæœ¬ä¸­è¿æ¥é‡è¦æ›´æ–°ï¼ŒåŒ…æ‹¬ï¼šä¸€ä¸ªä½¿ç”¨ Pyodide å®æ—¶æ‰§è¡Œä»£ç çš„ä»£ç è§£é‡Šå™¨ï¼Œä¸€ä¸ªé‡æ–°è®¾è®¡çš„èŠå¤©è¾“å…¥ç•Œé¢ï¼Œä»¥åŠé›†æˆäº† Exa æœç´¢å¼•æ“ï¼Œæ–¹ä¾¿åœ¨èŠå¤©ä¸­æ£€ç´¢ä¿¡æ¯ã€‚æ­¤å¤–ï¼Œç°åœ¨æä¾›äº†å®éªŒæ€§çš„åŸç”Ÿå·¥å…·è°ƒç”¨æ”¯æŒï¼Œæœ‰æœ›å‡å°‘æŸ¥è¯¢å»¶è¿Ÿï¼Œå¹¶æ”¹è¿›ä¸Šä¸‹æ–‡å“åº”ã€‚æ›´å¤šå‘å¸ƒè¯¦æƒ…è¯·è®¿é—®å®˜æ–¹ç½‘ç«™ã€‚

ä»£ç è§£é‡Šå™¨å’Œ Pyodideï¼šç”¨æˆ·å¯¹ä½¿ç”¨ Pyodide å®ç°çš„ä»£ç è§£é‡Šå™¨è¡¨ç¤ºèµèµï¼Œä»–ä»¬æ³¨æ„åˆ°äº†å®ƒçš„å±€é™æ€§ï¼ŒåŒæ—¶ä¹Ÿè®¤å¯äº†å®ƒåœ¨å¸¸è§åœºæ™¯ä¸­çš„å®ç”¨æ€§ã€‚å¤§å®¶å‘¼åå¯¹å…¶è¿›è¡Œæ”¹è¿›ï¼Œä¾‹å¦‚é›†æˆ Gradioï¼Œå¹¶æ”¯æŒä¸‹è½½ç»“æœï¼Œæ¯”å¦‚å›¾è¡¨æˆ–è€…ç»è¿‡å¤„ç†çš„æ•°æ®ã€‚

ç¤¾åŒºè´¡çŒ®ï¼šå°½ç®¡è´¡çŒ®è€…ä¼—å¤šï¼Œä½† tjbck è¢«å…¬è®¤ä¸ºæ˜¯ Open WebUI æœ€ä¸»è¦çš„é•¿æœŸè´¡çŒ®è€…ï¼Œå»ºè®®å¯ä»¥é€šè¿‡ GitHub èµåŠ©çš„æ–¹å¼æ”¯æŒä»–ã€‚è¯¥é¡¹ç›®å› å…¶å¿«é€Ÿçš„åŠŸèƒ½æ›´æ–°ä»¥åŠç›¸å¯¹äºå…¶ä»–ä¸“æœ‰ç•Œé¢çš„ç«äº‰ä¼˜åŠ¿è€Œå¤‡å—èµèª‰ã€‚

æ–‡æ¡£å¤„ç†å’Œ RAGï¼šæ–‡æ¡£å¤„ç†æ–¹é¢å—åˆ°äº†ä¸€äº›æ‰¹è¯„ï¼Œç‰¹åˆ«æ˜¯å¯¹äºç®€å•çš„å‘é‡æ•°æ®åº“ RAGï¼ˆRetrieval Augmented Generationï¼‰åœ¨å¤„ç†å•ä¸ªæ–‡æ¡£å¼•ç”¨æ—¶çš„è¡¨ç°ï¼Œåœ¨ç®€å•çš„æŸ¥è¯¢ä¸­ç»å¸¸å¤±æ•ˆã€‚å»ºè®®å°†æ–‡æ¡£å¤„ç†ã€RAG å’Œæœç´¢åŠŸèƒ½è½¬ç§»åˆ°ç‹¬ç«‹çš„æµç¨‹ä¸­ï¼Œä»¥ä¾¿è·Ÿä¸Šå¿«é€Ÿå‘å±•çš„æ­¥ä¼ï¼Œå¹¶ä¸”é»˜è®¤ç¦ç”¨ RAGï¼Œä»è€Œä¸ºç”¨æˆ·æä¾›æ›´å¥½çš„æ§åˆ¶ã€‚

ä¸»é¢˜ 4ï¼šè¿‡åº¦ Token åŒ–çš„ Transformer æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM/Large Language Modelï¼‰çš„æ€§èƒ½

Over-Tokenized Transformer - New paper shows massively increasing the input vocabulary (100x larger or more) of a dense LLM significantly enhances model performance for the same training cost (Score: 324, Comments: 37): A new paper demonstrates that massively increasing the input vocabulary of a dense Large Language Model (LLM) by 100 times or more significantly boosts model performance without increasing training costs. This finding suggests a potential strategy for improving transformer efficiency by expanding the vocabulary size.
Tokenization and Vocabulary Size: Increasing the vocabulary size to millions, as opposed to the typical 32k to 128k, can enhance model performance by using more meaningful, hierarchical tokens. This approach achieves faster convergence by combining multiple tokens into new ones, though it primarily improves training performance rather than final performance in direct proportion.
Potential Challenges and Considerations: Concerns arise about undertrained tokens due to greedy tokenizers, which might lead to performance issues with misspellings and tasks sensitive to single character mutations, such as arithmetic or algebraic reasoning. There are also questions regarding the impact on memory usage, inference speed, and effective context size when using smaller tokens.
Research and Comparisons: A similar study from three months ago suggested that models like Llama 2 70B should use at least 216k tokens for optimal compute use, and even larger token counts could be beneficial. The paper's findings are particularly interesting for dense models, but they did not show the same improvement for Mixture of Experts (MoE) models, highlighting a potential area for further exploration.
Other AI Subreddit Recap
/r/Singularity, /r/Oobabooga, /r/MachineLearning, /r/OpenAI, /r/ClaudeAI, /r/StableDiffusion, /r/ChatGPT

[Transformer çš„è¿‡åº¦ Token åŒ– - æœ€æ–°è®ºæ–‡æ˜¾ç¤ºï¼Œå¯¹äºå¯†é›†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œå¤§å¹…å¢åŠ è¾“å…¥è¯æ±‡é‡ï¼ˆ100 å€æˆ–æ›´å¤šï¼‰å¯ä»¥åœ¨ç›¸åŒçš„è®­ç»ƒæˆæœ¬ä¸‹æ˜¾è‘—æé«˜æ¨¡å‹æ€§èƒ½ï¼ˆè¯„åˆ†ï¼š324ï¼Œè¯„è®ºï¼š37)ï¼š ä¸€ç¯‡æ–°è®ºæ–‡è¡¨æ˜ï¼Œå°†å¯†é›†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å…¥è¯æ±‡é‡å¢åŠ  100 å€ç”šè‡³æ›´å¤šï¼Œèƒ½å¤Ÿåœ¨ä¸å¢åŠ è®­ç»ƒæˆæœ¬çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡æ¨¡å‹æ€§èƒ½ã€‚è¿™ä¸€å‘ç°æ­ç¤ºäº†ä¸€ç§é€šè¿‡æ‰©å±•è¯æ±‡é‡æ¥æå‡ Transformer æ•ˆç‡çš„æ½œåœ¨ç­–ç•¥ã€‚
Token åŒ–ä¸è¯æ±‡é‡ï¼š å°†è¯æ±‡é‡æ‰©å±•åˆ°æ•°ç™¾ä¸‡çº§åˆ«ï¼Œè€Œä¸æ˜¯é€šå¸¸çš„ 32000 åˆ° 128000ï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨æ›´æœ‰æ„ä¹‰å’Œå±‚çº§çš„ Token æ¥æå‡æ¨¡å‹æ€§èƒ½ã€‚è¿™ç§æ–¹æ³•é€šè¿‡å°†å¤šä¸ª Token ç»„åˆæˆæ–°çš„ Tokenï¼Œä»è€ŒåŠ å¿«æ”¶æ•›é€Ÿåº¦ã€‚å°½ç®¡è¿™ç§æ–¹æ³•ä¸»è¦æå‡çš„æ˜¯è®­ç»ƒæ€§èƒ½ï¼Œè€Œéæœ€ç»ˆæ€§èƒ½çš„åŒæ¯”ä¾‹æå‡ã€‚
æ½œåœ¨çš„æŒ‘æˆ˜ä¸è€ƒé‡ï¼š è€ƒè™‘åˆ°è´ªå©ª Tokenizerï¼Œäººä»¬æ‹…å¿ƒ Token è®­ç»ƒä¸è¶³çš„é—®é¢˜ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´æ‹¼å†™é”™è¯¯ä»¥åŠå¯¹å•å­—ç¬¦çªå˜ï¼ˆä¾‹å¦‚å°†ã€Œcatã€å˜ä¸ºã€Œbatã€è¿™ç§å•ä¸ªå­—ç¬¦çš„å˜åŒ–ï¼‰æ•æ„Ÿçš„ä»»åŠ¡ï¼ˆå¦‚ç®—æœ¯æˆ–ä»£æ•°æ¨ç†ï¼‰å‡ºç°æ€§èƒ½é—®é¢˜ã€‚æ­¤å¤–ï¼Œä½¿ç”¨è¾ƒå° Token è¿˜ä¼šå¼•å‘å¯¹å†…å­˜å ç”¨ã€æ¨ç†é€Ÿåº¦å’Œæœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦çš„å½±å“çš„æ‹…å¿§ã€‚
ç ”ç©¶ä¸å¯¹æ¯”ï¼š ä¸‰ä¸ªæœˆå‰çš„ä¸€é¡¹ç±»ä¼¼ç ”ç©¶è¡¨æ˜ï¼Œåƒ Llama 2 70B è¿™æ ·çš„æ¨¡å‹åº”è¯¥ä½¿ç”¨è‡³å°‘ 216k ä¸ª Tokenï¼Œæ‰èƒ½å®ç°æœ€ä½³çš„è®¡ç®—æ•ˆç‡ï¼›ç”šè‡³æ›´å¤§çš„ Token æ•°é‡å¯èƒ½ä¹Ÿä¼šå¸¦æ¥ç›Šå¤„ã€‚è¿™ç¯‡è®ºæ–‡çš„å‘ç°å¯¹äºå¯†é›†æ¨¡å‹æ¥è¯´å°¤å…¶æœ‰è¶£ï¼Œä½†å¯¹äºæ··åˆä¸“å®¶ï¼ˆMixture of Expertsï¼ŒMoEï¼‰æ¨¡å‹ï¼Œæ”¹è¿›æ•ˆæœå¹¶ä¸æ˜æ˜¾ï¼Œè¿™è¡¨æ˜è¯¥é¢†åŸŸæœ‰å¾…è¿›ä¸€æ­¥æ¢ç´¢ã€‚
å…¶ä»– AI Subreddit ç²¾é€‰
/r/Singularityï¼Œ/r/Oobaboogaï¼Œ/r/MachineLearningï¼Œ/r/OpenAIï¼Œ/r/ClaudeAIï¼Œ/r/StableDiffusionï¼Œ/r/ChatGPT]

Theme 1. Altman admits reduced competitive edge for OpenAI

Altman admits OpenAl will no longer be able to maintain big leads in AI (Score: 259, Comments: 69): Sam Altman acknowledged that OpenAI will face increased competition and will not maintain its previous lead in AI development. He noted that while OpenAI will produce better models, the competitive gap will be smaller, as reported in a Fortune.com interview. Source.
OpenAI's Competitive Strategy: Several commenters discussed the idea that OpenAI attempted to maintain a monopoly by controlling the release of their discoveries, which gave them an advantage of about 3-4 months before competitors could replicate their work. This strategy was seen as a temporary measure to stay ahead in the competitive landscape.
Technology Plateau and Model Training: There is a perception that AI technology may be plateauing, with users noting that OpenAI admitted to facing inevitable competition. Commenters highlighted the challenge of preventing others from using larger models' outputs to train their own, indicating that OpenAI will have to continue innovating alongside other companies.
Media and Public Interaction: A commenter's question ended up in a Fortune article, leading to discussions about media ethics and the value of such publications. There was also appreciation for Sam Altman's openness during an AMA, despite the limitations on what he could disclose.
Theme 2. Deep Reconstruction using AI tools for complex analysis

ä¸»é¢˜ 1. å¥¥ç‰¹æ›¼æ‰¿è®¤ OpenAI çš„ç«äº‰ä¼˜åŠ¿å‡å¼±å¥¥ç‰¹æ›¼æ‰¿è®¤ OpenAI å°†æ— æ³•ç»§ç»­ä¿æŒåœ¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰é¢†åŸŸçš„ç»å¯¹é¢†å…ˆåœ°ä½ï¼ˆå¾—åˆ†ï¼š259ï¼Œè¯„è®ºï¼š69)ï¼šSam Altman æ‰¿è®¤ï¼ŒOpenAI å°†é¢ä¸´æ—¥è¶‹æ¿€çƒˆçš„ç«äº‰ï¼Œæ— æ³•åƒè¿‡å»é‚£æ ·åœ¨ AI å¼€å‘é¢†åŸŸé¥é¥é¢†å…ˆã€‚æ­£å¦‚ Fortune.com çš„ä¸€ç¯‡é‡‡è®¿æŠ¥é“ä¸­æ‰€è¿°ï¼Œä»–æŒ‡å‡ºï¼Œè™½ç„¶ OpenAI ä¾ç„¶ä¼šæ¨å‡ºæ›´ä¼˜ç§€çš„æ¨¡å‹ï¼Œä½†é¢†å…ˆä¼˜åŠ¿å°†ä¼šç¼©å°ã€‚æ¥æºã€‚
OpenAI çš„ç«äº‰ç­–ç•¥ï¼šæœ‰è¯„è®ºæŒ‡å‡ºï¼ŒOpenAI æ›¾è¯•å›¾é€šè¿‡æ§åˆ¶å…¶ç ”ç©¶æˆæœçš„å‘å¸ƒæ¥ç»´æŒå„æ–­åœ°ä½ï¼Œä»è€Œåœ¨ç«äº‰å¯¹æ‰‹èƒ½å¤Ÿå¤ç°ç›¸å…³æŠ€æœ¯ä¹‹å‰ï¼Œèµ¢å¾—å¤§çº¦ 3-4 ä¸ªæœˆçš„é¢†å…ˆæ—¶é—´ã€‚è¿™ç§ç­–ç•¥è¢«è§†ä¸ºåœ¨æ¿€çƒˆç«äº‰ä¸­ä¿æŒä¼˜åŠ¿çš„ä¸´æ—¶æ€§æ‰‹æ®µã€‚
æŠ€æœ¯å¹³å°æœŸï¼ˆPlateauï¼‰å’Œæ¨¡å‹è®­ç»ƒï¼šæœ‰è§‚ç‚¹è®¤ä¸ºï¼ŒAI æŠ€æœ¯å¯èƒ½æ­£åœ¨è¿›å…¥å¹³å°æœŸã€‚ç”¨æˆ·æ³¨æ„åˆ°ï¼ŒOpenAI å·²ç»æ‰¿è®¤é¢ä¸´ç€ä¸å¯é¿å…çš„ç«äº‰ã€‚è¯„è®ºè€…ä»¬å¼ºè°ƒï¼Œå¦‚ä½•é˜»æ­¢ä»–äººåˆ©ç”¨æ›´å¤§è§„æ¨¡çš„æ¨¡å‹è¾“å‡ºç»“æœæ¥è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ï¼Œå°†æ˜¯ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼Œè¿™æ„å‘³ç€ OpenAI å¿…é¡»ä¸å…¶ä»–å…¬å¸å¹¶è‚©åˆ›æ–°ã€‚
åª’ä½“å’Œå…¬ä¼—äº’åŠ¨ï¼šä¸€ä½è¯„è®ºè€…æå‡ºçš„é—®é¢˜æœ€ç»ˆè¢« Fortune æ‚å¿—å¼•ç”¨ï¼Œå¼•å‘äº†å…³äºåª’ä½“ä¼¦ç†å’Œæ­¤ç±»å‡ºç‰ˆç‰©ä»·å€¼çš„è®¨è®ºã€‚æ­¤å¤–ï¼ŒSam Altman åœ¨ AMA é—®ç­”ç¯èŠ‚ä¸­å±•ç°å‡ºçš„å¼€æ”¾æ€åº¦ä¹Ÿå—åˆ°äº†èµèµï¼Œå°½ç®¡ä»–åœ¨ä¿¡æ¯æŠ«éœ²æ–¹é¢å—åˆ°ä¸€å®šçš„é™åˆ¶ã€‚
ä¸»é¢˜ 2. åˆ©ç”¨ AI å·¥å…·è¿›è¡Œæ·±åº¦é‡å»ºä»¥è¿›è¡Œå¤æ‚åˆ†æ

Give me a prompt for Deep Research and I'll run it for you! (Score: 246, Comments: 111): The user has paid $200 for access to Deep Research and is offering to run prompts for the community to evaluate its capabilities. They compare it to o3-mini-high, noting that Deep Research supports attachments but doesn't seem significantly better. They invite the community to submit serious prompts and vote on them to prioritize which ones to execute.
Complex Prompt Challenges: Users are submitting complex, multidisciplinary prompts, such as those involving particle physics, ontological spaces, and depression subtypes. These often require clarification from the AI to proceed with research or analysis, highlighting the need for precise inputs to optimize AI responses.
Investment and Economic Predictions: There is significant interest in using AI for stock market predictions and economic analysis in a post-ASI world. Users are curious about the impact of ASI on stock valuations, GDP growth, and bond markets, emphasizing the speculative nature of these inquiries and the need for AI to consider multiple scenarios and variables.
Agricultural and Environmental Systems: The discussion includes innovative agricultural methods like the 3 sisters method and its potential expansion using AI to optimize plant cooperation systems for different climates and soil types. This reflects a broader interest in applying AI to enhance sustainable agricultural practices.
Dear OpenAI, if I'm paying $200 per month for Deep Research, the ability to save to PDF/Markdown would be nice! (Score: 229, Comments: 40): The author expresses disappointment that OpenAI's Deep Research, despite its cost of $200 per month, lacks a feature to save reports directly to PDF or Markdown. They suggest a workaround by using the 'copy' button to obtain raw Markdown, which can then be pasted into Notion.
Many users express frustration over OpenAI's Deep Research lacking a straightforward PDF or Markdown export feature, emphasizing that the AI should reduce busy work and facilitate easier integration with other applications like Pages and Word. The absence of these features is seen as a significant oversight given the tool's high cost of $200 per month.
Suggestions for workarounds include using the 'copy' button for Markdown, then pasting into a Markdown Editor or using print > save as PDF. However, users find these manual processes counterintuitive to the AI's purpose of saving time and simplifying tasks.
There is a humorous discussion around the naming conventions of AI tools, with comparisons to Gemini Deep Research and anticipation for future tools like 'Microsoft Co-pilot - In to Deep' edition. The conversation highlights a broader dissatisfaction with current AI capabilities and the expectation for more seamless functionalities in premium tiers.
Theme 3. Open Source AI for Trackable Health Diagnostics

å…³äºæ·±åº¦ç ”ç©¶ï¼ˆDeep Researchï¼‰çš„ä¸€ä¸ªæµ‹è¯•ï¼Œæˆ‘å°†äº²è‡ªè¿è¡Œå®ƒï¼(å¾—åˆ†ï¼š246ï¼Œè¯„è®ºï¼š111)ï¼šä¸€ä½ç”¨æˆ·ä¸ºäº†ä½“éªŒæ·±åº¦ç ”ç©¶ï¼Œæ¯æœˆæ”¯ä»˜äº† 200 ç¾å…ƒï¼Œç°åœ¨ä»–ä¸»åŠ¨æå‡ºä¸ºç¤¾åŒºè¿è¡Œå„ç§æç¤ºï¼Œä»¥ä¾¿å¤§å®¶è¯„ä¼°å®ƒçš„èƒ½åŠ›ã€‚ä»–å°†æ·±åº¦ç ”ç©¶å’Œ o3-mini-high è¿›è¡Œäº†å¯¹æ¯”ï¼Œå‘ç°è™½ç„¶æ·±åº¦ç ”ç©¶æ”¯æŒä¸Šä¼ é™„ä»¶ï¼Œä½†æ•´ä½“è¡¨ç°å¹¶æ²¡æœ‰æ˜¾è‘—æå‡ã€‚å› æ­¤ï¼Œä»–é‚€è¯·å¤§å®¶è¸Šè·ƒæäº¤é«˜è´¨é‡çš„æç¤ºï¼Œå¹¶é€šè¿‡æŠ•ç¥¨æ¥å†³å®šå“ªäº›æç¤ºä¼˜å…ˆæ‰§è¡Œã€‚
å¤æ‚æç¤ºçš„æŒ‘æˆ˜ï¼šç”¨æˆ·ä»¬æäº¤äº†ä¸€äº›éå¸¸å¤æ‚ï¼Œå¹¶ä¸”æ¶‰åŠå¤šä¸ªå­¦ç§‘é¢†åŸŸçš„æç¤ºï¼Œä¾‹å¦‚å…³äºç²’å­ç‰©ç†ã€æœ¬ä½“ç©ºé—´ï¼ˆontological spacesï¼‰ä»¥åŠæŠ‘éƒç—‡äºšå‹çš„ç ”ç©¶ã€‚è¿™äº›æç¤ºé€šå¸¸éœ€è¦ AI è¿›ä¸€æ­¥æ¾„æ¸…æ‰èƒ½è¿›è¡Œæ·±å…¥ç ”ç©¶æˆ–åˆ†æï¼Œè¿™ä¹Ÿçªæ˜¾äº†ç²¾ç¡®è¾“å…¥å¯¹äºä¼˜åŒ– AI å›å¤çš„é‡è¦æ€§ã€‚
æŠ•èµ„å’Œç»æµé¢„æµ‹ï¼š è®¸å¤šäººéƒ½å¯¹åœ¨åé€šç”¨äººå·¥æ™ºèƒ½ï¼ˆpost-AGIï¼‰æ—¶ä»£ï¼Œå¦‚ä½•åˆ©ç”¨ AI è¿›è¡Œè‚¡å¸‚é¢„æµ‹å’Œç»æµåˆ†æè¡¨ç°å‡ºäº†æµ“åšçš„å…´è¶£ã€‚ç”¨æˆ·ä»¬å¾ˆæƒ³çŸ¥é“ï¼Œé€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å‘å±•ä¼šå¯¹è‚¡ç¥¨ä¼°å€¼ã€å›½å†…ç”Ÿäº§æ€»å€¼ï¼ˆGDPï¼‰å¢é•¿ä»¥åŠå€ºåˆ¸å¸‚åœºå¸¦æ¥ä»€ä¹ˆæ ·çš„å½±å“ã€‚è¿™ç±»é—®é¢˜æœ¬èº«å°±å¸¦æœ‰ä¸€å®šçš„æ¨æµ‹æ€§ï¼Œéœ€è¦ AI èƒ½å¤Ÿè€ƒè™‘åˆ°å¤šç§å¯èƒ½çš„æƒ…å†µå’Œå˜é‡ã€‚
å†œä¸šå’Œç¯å¢ƒç³»ç»Ÿï¼š è®¨è®ºä¸­è¿˜æåˆ°äº†åˆ›æ–°çš„å†œä¸šè€•ä½œæ–¹æ³•ï¼Œæ¯”å¦‚ã€Œä¸‰å§å¦¹æ³•ï¼ˆ3 sisters methodï¼‰ã€ã€‚æœ‰äººæå‡ºï¼Œå¯ä»¥åˆ©ç”¨ AI æ¥è¿›ä¸€æ­¥ä¼˜åŒ–è¿™ç§æ–¹æ³•ï¼Œé’ˆå¯¹ä¸åŒçš„æ°”å€™å’ŒåœŸå£¤æ¡ä»¶ï¼Œè®¾è®¡å‡ºæ›´é«˜æ•ˆçš„æ¤ç‰©å…±ç”Ÿç³»ç»Ÿã€‚è¿™åæ˜ äº†äººä»¬å¯¹äºåˆ©ç”¨ AI æ¥æ”¹å–„å¯æŒç»­å†œä¸šå®è·µçš„å¹¿æ³›å…´è¶£ã€‚
è‡´ OpenAIï¼šæˆ‘æ¯æœˆèŠ± 200 ç¾å…ƒè®¢é˜…æ·±åº¦ç ”ç©¶ï¼Œå¦‚æœèƒ½æ”¯æŒä¿å­˜ä¸º PDF/Markdown æ ¼å¼å°±æ›´å¥½äº†ï¼(å¾—åˆ†ï¼š229ï¼Œè¯„è®ºï¼š40)ï¼šä½œè€…åæ§½è¯´ï¼ŒOpenAI çš„æ·±åº¦ç ”ç©¶ï¼Œå³ä½¿æ¯æœˆæ”¶è´¹é«˜è¾¾ 200 ç¾å…ƒï¼Œç«Ÿç„¶è¿˜ä¸æ”¯æŒç›´æ¥å°†æŠ¥å‘Šä¿å­˜ä¸º PDF æˆ– Markdown æ ¼å¼ï¼Œå®åœ¨ä»¤äººå¤±æœ›ã€‚ä»–å»ºè®®äº†ä¸€ä¸ªå˜é€šæ–¹æ³•ï¼šå…ˆç‚¹å‡»ã€Œå¤åˆ¶ã€æŒ‰é’®è·å–åŸå§‹çš„ Markdown æ–‡æœ¬ï¼Œç„¶åå†ç²˜è´´åˆ° Notion è¿™æ ·çš„ç¬”è®°è½¯ä»¶ä¸­ã€‚
è®¸å¤šç”¨æˆ·éƒ½è¡¨è¾¾äº†å¯¹ OpenAI æ·±åº¦ç ”ç©¶çš„ä¸æ»¡ï¼Œè®¤ä¸ºå®ƒç¼ºå°‘ç›´æ¥å¯¼å‡ºä¸º PDF æˆ– Markdown çš„åŠŸèƒ½ã€‚å¤§å®¶æ™®éè®¤ä¸ºï¼ŒAI åº”è¯¥å¸®åŠ©äººä»¬å‡å°‘é‡å¤æ€§çš„å·¥ä½œï¼Œæ›´æ–¹ä¾¿åœ°ä¸å…¶ä»–åº”ç”¨è½¯ä»¶ï¼ˆæ¯”å¦‚ Pages å’Œ Wordï¼‰è¿›è¡Œæ•´åˆã€‚è€ƒè™‘åˆ°è¿™æ¬¾å·¥å…·æ¯æœˆ 200 ç¾å…ƒçš„é«˜æ˜‚ä»·æ ¼ï¼Œç¼ºå°‘è¿™äº›åŠŸèƒ½ç¡®å®æ˜¯ä¸€ä¸ªä¸å°çš„é—æ†¾ã€‚
æœ‰äººå»ºè®®ï¼Œå¯ä»¥å…ˆç”¨ã€Œå¤åˆ¶ã€æŒ‰é’®å¤åˆ¶ Markdown æ–‡æœ¬ï¼Œç„¶åç²˜è´´åˆ° Markdown ç¼–è¾‘å™¨ä¸­ï¼Œæˆ–è€…é€‰æ‹©ã€Œæ‰“å°ã€å¹¶ã€Œå¦å­˜ä¸º PDFã€ã€‚ä½†æ˜¯ï¼Œç”¨æˆ·ä»¬è§‰å¾—è¿™äº›æ‰‹åŠ¨æ“ä½œï¼Œå’Œ AI æœ¬åº”å…·å¤‡çš„èŠ‚çœæ—¶é—´å’Œç®€åŒ–ä»»åŠ¡çš„åˆè¡·æ˜¯èƒŒé“è€Œé©°çš„ã€‚
å¤§å®¶è¿˜å°± AI å·¥å…·çš„å‘½åå±•å¼€äº†æœ‰è¶£çš„è®¨è®ºï¼Œå°†æ·±åº¦ç ”ç©¶å’Œ Gemini æ·±åº¦ç ”ç©¶è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶ä¸”æœŸå¾…æœªæ¥èƒ½å‡ºç°åƒã€ŒMicrosoft Co-pilot - In to Deepã€è¿™æ ·çš„äº§å“åç§°ã€‚è¿™ä¹Ÿåæ˜ äº†äººä»¬å¯¹å½“å‰ AI èƒ½åŠ›çš„æ™®éä¸æ»¡ï¼Œä»¥åŠå¯¹æ›´é«˜çº§ç‰ˆæœ¬èƒ½å¤Ÿæä¾›æ›´æµç•…åŠŸèƒ½çš„æœŸå¾…ã€‚
ä¸»é¢˜ 3. ç”¨äºå¯è¿½è¸ªå¥åº·è¯Šæ–­çš„å¼€æº AI

How I Built an Open Source AI Tool to Find My Autoimmune Disease (After $100k and 30+ Hospital Visits) - Now Available for Anyone to Use (Score: 195, Comments: 27): The author shares their journey of building an open-source AI tool to aid in diagnosing autoimmune diseases after spending $100k and visiting 30+ hospitals without clear answers. The tool allows users to upload and standardize medical records, track lab result changes, and identify patterns using different AI models, including Deepseek and GPT4/Claude. They provide resources like Fasten Health for obtaining medical records and mention plans to migrate document parsing to run locally.
Data Security Concerns: Several commenters emphasize the critical importance of running the tool locally to avoid data breaches, especially given the sensitivity of medical records and the high value of such data on the dark market. Mithril was mentioned as a secure AI deployment option for handling medical information, highlighting the need for certifications like FISMA and HITRUST.
Fragmented Diagnosis to Discovery: The discussion includes a personal account of receiving multiple diagnoses like herniated disc and spinal curvature, which were later unified into a diagnosis of Ankylosing Spondylitis using the tool. A suggestion to consider EDS (Ehlers-Danlos Syndrome) was also made, indicating the tool's potential in refining and discovering complex medical conditions.
User Reactions: Strong reactions from users indicate surprise and concern about the potential for serious data breaches, with multiple comments expressing disbelief and highlighting the legal implications of mishandling sensitive medical data.
AI Discord Recap
A summary of Summaries of Summaries by Gemini 2.0 Flash Thinking

æˆ‘å¦‚ä½•æ„å»ºäº†ä¸€ä¸ªå¼€æº AI å·¥å…·æ¥è¯Šæ–­æˆ‘çš„è‡ªèº«å…ç–«æ€§ç–¾ç—…ï¼ˆAutoimmune Diseaseï¼‰ï¼ˆåœ¨èŠ±è´¹äº† 10 ä¸‡ç¾å…ƒå’Œå°±è¯Š 30 å¤šæ¬¡åï¼‰â€”â€” ç°åœ¨ï¼Œä»»ä½•äººéƒ½å¯ä»¥ä½¿ç”¨å®ƒï¼ ï¼ˆå¾—åˆ†ï¼š195ï¼Œè¯„è®ºï¼š27ï¼‰ï¼šä½œè€…åˆ†äº«äº†ä»–åœ¨èŠ±è´¹ 10 ä¸‡ç¾å…ƒã€è·‘äº† 30 å¤šå®¶åŒ»é™¢å´ä»æœªç¡®è¯Šåï¼Œæ„å»ºå¼€æº AI å·¥å…·ä»¥è¾…åŠ©è¯Šæ–­è‡ªèº«å…ç–«æ€§ç–¾ç—…çš„å†ç¨‹ã€‚è¿™æ¬¾å·¥å…·å…è®¸ç”¨æˆ·ä¸Šä¼ å¹¶æ ‡å‡†åŒ–åŒ»ç–—è®°å½•ï¼Œè¿½è¸ªåŒ–éªŒç»“æœçš„å˜åŒ–ï¼Œå¹¶åˆ©ç”¨åŒ…æ‹¬ Deepseek å’Œ GPT4/Claude åœ¨å†…çš„å¤šç§ AI æ¨¡å‹æ¥è¯†åˆ«æ½œåœ¨çš„ç–¾ç—…æ¨¡å¼ã€‚ä½œè€…è¿˜æä¾›äº† Fasten Health ç­‰èµ„æºï¼Œæ–¹ä¾¿ç”¨æˆ·è·å–åŒ»ç–—è®°å½•ï¼Œå¹¶è®¡åˆ’å°†æ–‡æ¡£è§£æåŠŸèƒ½è¿ç§»åˆ°æœ¬åœ°è¿è¡Œã€‚

æ•°æ®å®‰å…¨é—®é¢˜ï¼š å¤šä½è¯„è®ºå‘˜å¼ºè°ƒï¼Œå¿…é¡»åœ¨æœ¬åœ°è¿è¡Œè¯¥å·¥å…·ä»¥é¿å…æ•°æ®æ³„éœ²ã€‚è€ƒè™‘åˆ°åŒ»ç–—è®°å½•çš„æ•æ„Ÿæ€§ï¼Œä»¥åŠæ­¤ç±»æ•°æ®åœ¨åœ°ä¸‹é»‘å¸‚ä¸Šçš„é«˜ä»·å€¼ï¼Œè¿™ä¸€ç‚¹è‡³å…³é‡è¦ã€‚æœ‰äººæåˆ° Mithril æ˜¯ä¸€ç§å®‰å…¨çš„ AI éƒ¨ç½²æ–¹æ¡ˆï¼Œå¯ä»¥ç”¨äºå¤„ç†åŒ»ç–—ä¿¡æ¯ï¼Œå¹¶å¼ºè°ƒéœ€è¦è·å¾— FISMA å’Œ HITRUST ç­‰ç›¸å…³è®¤è¯ã€‚

ä»é›¶æ•£è¯Šæ–­åˆ°æœ€ç»ˆç¡®è¯Šï¼š è®¨è®ºä¸­ï¼Œæœ‰äººåˆ†äº«äº†è‡ªå·±è¢«è¯¯è¯Šä¸ºæ¤é—´ç›˜çªå‡ºå’Œè„ŠæŸ±ä¾§å¼¯çš„ç»å†ï¼Œæœ€ç»ˆï¼Œä»–ä»¬ä½¿ç”¨è¯¥å·¥å…·å°†æ‰€æœ‰ç—‡çŠ¶ä¸å¼ºç›´æ€§è„ŠæŸ±ç‚è”ç³»èµ·æ¥ï¼Œä»è€Œå¾—åˆ°äº†ç¡®è¯Šã€‚è¿˜æœ‰äººå»ºè®®è€ƒè™‘ EDSï¼ˆEhlers-Danlos Syndromeï¼ŒåŸƒå‹’æ–¯ - å½“æ´›æ–¯ç»¼åˆå¾ï¼‰ï¼Œè¿™è¡¨æ˜è¯¥å·¥å…·åœ¨æ”¹è¿›è¯Šæ–­å’Œå‘ç°å¤æ‚ç–¾ç—…æ–¹é¢çš„æ½œåŠ›ã€‚

ç”¨æˆ·åé¦ˆï¼š ç”¨æˆ·çš„å¼ºçƒˆååº”è¡¨æ˜ï¼Œä»–ä»¬å¯¹æ½œåœ¨çš„ä¸¥é‡æ•°æ®æ³„éœ²æ„Ÿåˆ°æƒŠè®¶å’Œæ‹…å¿§ã€‚è®¸å¤šè¯„è®ºè¡¨è¾¾äº†éš¾ä»¥ç½®ä¿¡ä¹‹æƒ…ï¼Œå¹¶å¼ºè°ƒäº†ä¸å½“å¤„ç†æ•æ„ŸåŒ»ç–—æ•°æ®å¯èƒ½å¸¦æ¥çš„æ³•å¾‹åæœã€‚

AI Discord æ‘˜è¦å›é¡¾
Gemini 2.0 Flash Thinking æ€»ç»“ç²¾è¦

Theme 1. Breakthroughs in Model Capabilities and Performance

Hibiki Achieves Real-Time Speech Translation Like a Human: Kyutai's Hibiki model delivers simultaneous speech-to-speech translation from ğŸ‡«ğŸ‡· to ğŸ‡¬ğŸ‡§, adapting pace to content and preserving speaker's voice. Early reports boast Hibiki's superior quality, naturalness, and speaker similarity, rivaling professional human interpreters in real-time communication.
Gemini 2.0 Flash Parses PDFs at Scale for Pennies: Gemini 2 Flash now efficiently parses large PDF documents for approximately $1 per 6000 tokens, marking a significant leap in document processing. This cost-effective solution unlocks new possibilities for applications requiring high-volume, high-accuracy text extraction from complex document formats.
Unsloth's GRPO Makes DeepSeek-R1 Reasoning Accessible on 7GB VRAM: Unsloth's latest GRPO update slashes memory usage by 80%, allowing users to reproduce DeepSeek-R1's reasoning with just 7GB VRAM. This breakthrough democratizes access to advanced reasoning models, enabling local experimentation with models like Llama 3.1 (8B) and Phi-4 (14B) even on resource-constrained systems.
Theme 2. Tooling and Framework Enhancements for AI Engineers

ä¸»é¢˜ 1. æ¨¡å‹èƒ½åŠ›ä¸æ€§èƒ½çš„é‡å¤§è¿›å±•

Hibiki å®ç°äº†åª²ç¾äººç±»çš„å®æ—¶è¯­éŸ³ç¿»è¯‘ï¼šKyutai å…¬å¸çš„ Hibiki æ¨¡å‹èƒ½å¤Ÿæä¾›ä» ğŸ‡«ğŸ‡· åˆ° ğŸ‡¬ğŸ‡§ çš„åŒå£°ä¼ è¯‘ï¼Œå®ƒå¯ä»¥æ ¹æ®å†…å®¹è°ƒæ•´è¯­é€Ÿï¼Œå¹¶ä¿ç•™è¯´è¯è€…çš„å£°éŸ³ã€‚æ—©æœŸæŠ¥å‘Šæ˜¾ç¤ºï¼ŒHibiki åœ¨è´¨é‡ã€è‡ªç„¶åº¦å’Œè¯´è¯äººç›¸ä¼¼åº¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œåœ¨å®æ—¶é€šä¿¡ä¸­èƒ½å¤Ÿä¸ä¸“ä¸šçš„äººå·¥ç¿»è¯‘ç›¸åª²ç¾ã€‚
Gemini 2.0 Flash ä»¥æä½æˆæœ¬å®ç° PDF å¤§è§„æ¨¡è§£æï¼š Gemini 2 Flash ç°åœ¨èƒ½å¤Ÿé«˜æ•ˆè§£æå¤§å‹ PDF æ–‡æ¡£ï¼Œæˆæœ¬çº¦ä¸ºæ¯ 6000 ä¸ªä»¤ç‰Œï¼ˆTokenï¼‰1 ç¾å…ƒï¼Œè¿™æ ‡å¿—ç€æ–‡æ¡£å¤„ç†æŠ€æœ¯çš„é‡å¤§é£è·ƒã€‚è¿™ç§ç»æµé«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆä¸ºéœ€è¦ä»å¤æ‚æ–‡æ¡£æ ¼å¼ä¸­è¿›è¡Œå¤§æ‰¹é‡ã€é«˜ç²¾åº¦æ–‡æœ¬æå–çš„åº”ç”¨å¼€å¯äº†æ–°çš„å¯èƒ½æ€§ã€‚
Unsloth çš„ GRPO ä½¿å¾— DeepSeek-R1 çš„æ¨ç†åœ¨ 7GB æ˜¾å­˜ï¼ˆVRAMï¼‰ä¸Šæˆä¸ºå¯èƒ½ï¼šUnsloth æœ€æ–°çš„ GRPO æ›´æ–°å°†å†…å­˜ä½¿ç”¨é‡å‡å°‘äº† 80%ï¼Œä½¿ç”¨æˆ·ä»…éœ€ 7GB æ˜¾å­˜ï¼ˆVRAMï¼‰å³å¯é‡ç° DeepSeek-R1 çš„æ¨ç†è¿‡ç¨‹ã€‚è¿™ä¸€çªç ´é™ä½äº†é«˜çº§æ¨ç†æ¨¡å‹çš„ä½¿ç”¨é—¨æ§›ï¼Œå³ä½¿åœ¨èµ„æºå—é™çš„ç³»ç»Ÿä¸Šï¼Œä¹Ÿèƒ½ä½¿ç”¨ Llama 3.1ï¼ˆ8Bï¼‰å’Œ Phi-4ï¼ˆ14Bï¼‰ç­‰æ¨¡å‹è¿›è¡Œæœ¬åœ°å®éªŒã€‚
ä¸»é¢˜ 2. é¢å‘ AI å·¥ç¨‹å¸ˆçš„å·¥å…·ä¸æ¡†æ¶ä¼˜åŒ–

GitHub Copilot Awakens as an Agent, Edits Code Like a Pro: GitHub Copilot introduces agent mode and general availability of Copilot Edits, enhancing developer workflow with smarter AI assistance. This update aims to provide more proactive and effective coding support, transforming Copilot into a more integrated and powerful development partner.
Windsurf IDE Supercharges with Gemini 2.0 Flash and Cascade Web Search: Windsurf now supports blazingly fast Gemini 2.0 Flash, consuming only 0.25 prompt credits, and Cascade gains automatic web search via @web, costing 1 flow action credit. These enhancements aim to boost developer productivity with faster models and integrated information retrieval within the IDE environment.
Cursor IDE Unveils GitHub Agents and Architect Feature for Productivity Boost: Cursor IDE rolls out new GitHub agents and an architect feature, aiming to significantly boost developer productivity and streamline complex projects. While users are enthusiastic about these additions, some report potential bugs in command execution within the Composer tool, signaling active development and refinement of these features.
Theme 3. Navigating Challenges in Model Performance and Infrastructure

GitHub Copilot è§‰é†’ä¸º AI æ™ºèƒ½ä½“ï¼Œåƒä¸“å®¶ä¸€æ ·ç¼–è¾‘ä»£ç ï¼šGitHub Copilot å¼•å…¥äº† AI æ™ºèƒ½ä½“ï¼ˆAI Agentï¼‰æ¨¡å¼å’Œ Copilot Edits çš„å…¨é¢å¯ç”¨æ€§ï¼Œé€šè¿‡æ›´æ™ºèƒ½çš„ AI è¾…åŠ©æ¥å¢å¼ºå¼€å‘äººå‘˜çš„å·¥ä½œæµç¨‹ã€‚æ­¤æ›´æ–°æ—¨åœ¨æä¾›æ›´ä¸»åŠ¨å’Œæœ‰æ•ˆçš„ç¼–ç æ”¯æŒï¼Œå°† Copilot è½¬å˜ä¸ºæ›´é›†æˆå’Œå¼ºå¤§çš„å¼€å‘ä¼™ä¼´ã€‚
Windsurf IDE é€šè¿‡ Gemini 2.0 Flash å’Œ Cascade Web Search å¤§å¹…æé€Ÿï¼šWindsurf ç°åœ¨æ”¯æŒæå¿«çš„ Gemini 2.0 Flashï¼Œæ¯æ¬¡æç¤ºä»…æ¶ˆè€— 0.25 ä¸ª prompt ä¿¡ç”¨ç‚¹æ•°ï¼ŒCascade é€šè¿‡ @web è·å¾—è‡ªåŠ¨ç½‘ç»œæœç´¢ï¼Œæ¯æ¬¡æ“ä½œæ¶ˆè€— 1 ä¸ª flow action ä¿¡ç”¨ç‚¹æ•°ã€‚è¿™äº›å¢å¼ºåŠŸèƒ½æ—¨åœ¨é€šè¿‡æ›´å¿«çš„æ¨¡å‹å’Œ IDE ç¯å¢ƒä¸­é›†æˆçš„ ** ä¿¡æ¯æ£€ç´¢ **ï¼ˆinformation retrievalï¼‰æ¥æé«˜å¼€å‘äººå‘˜çš„ç”Ÿäº§åŠ›ã€‚
Cursor IDE å‘å¸ƒ GitHub AI æ™ºèƒ½ä½“å’Œæ¶æ„å¸ˆåŠŸèƒ½ä»¥æé«˜ç”Ÿäº§åŠ›ï¼šCursor IDE æ¨å‡ºäº†æ–°çš„ GitHub AI æ™ºèƒ½ä½“å’Œæ¶æ„å¸ˆåŠŸèƒ½ï¼Œæ—¨åœ¨æ˜¾ç€æé«˜å¼€å‘äººå‘˜çš„ç”Ÿäº§åŠ›å¹¶ç®€åŒ–å¤æ‚çš„é¡¹ç›®ã€‚è™½ç„¶ç”¨æˆ·å¯¹è¿™äº›æ–°å¢åŠŸèƒ½å……æ»¡çƒ­æƒ…ï¼Œä½†ä¸€äº›ç”¨æˆ·æŠ¥å‘Šäº† Composer å·¥å…·ä¸­ ** å‘½ä»¤æ‰§è¡Œ **ï¼ˆcommand executionï¼‰æ–¹é¢çš„æ½œåœ¨é”™è¯¯ï¼Œè¡¨æ˜è¿™äº›åŠŸèƒ½æ­£åœ¨ç§¯æå¼€å‘å’Œå®Œå–„ä¸­ã€‚
Theme 3. åº”å¯¹æ¨¡å‹æ€§èƒ½å’ŒåŸºç¡€è®¾æ–½çš„æŒ‘æˆ˜

DeepInfra Provider Plagued by 50% Failure Rate, Users Report: DeepInfra provider is currently failing to return responses 50% of the time, causing zero token completions and significant processing delays for users, particularly in applications like SillyTavern. Community members are actively sharing observations and seeking solutions to these performance issues across different models and providers on OpenRouter.
LM Studio Users Face API Error Avalanche, Seek Debugging Guidance: LM Studio users are reporting a surge of errors like 'unknown error' and 'exit code: 18446744072635812000' when loading models, prompting calls for detailed system specs and API insights for effective debugging. State handling issues when connecting via API also highlight the need for clearer documentation and user support for API interactions.
Codeium Jetbrains Plugin Criticized for Unresponsiveness and Frequent Restarts: Users are voicing frustrations with the Codeium Jetbrains plugin, citing frequent failures to respond and the necessity for frequent restarts, impacting developer workflow. Some users are opting to switch back to Copilot for reliability, while others report specific errors in PhpStorm, indicating persistent instability in the plugin's performance.
Theme 4. Community Driven Innovations and Open Source Contributions

DeepInfra æä¾›å•†é­é‡ 50% å¤±è´¥ç‡ï¼Œç”¨æˆ·åé¦ˆï¼šDeepInfra æä¾›å•†ç›®å‰æœ‰ 50% çš„æ—¶é—´æ— æ³•è¿”å›å“åº”ï¼Œå¯¼è‡´é›¶ Token è¾“å‡ºå’Œç”¨æˆ·çš„å¤§é‡å¤„ç†å»¶è¿Ÿï¼Œå°¤å…¶æ˜¯åœ¨ SillyTavern è¿™æ ·çš„åº”ç”¨ç¨‹åºä¸­ã€‚ç¤¾åŒºæˆå‘˜æ­£åœ¨ç§¯æåˆ†äº«è§‚å¯Ÿç»“æœï¼Œå¹¶å¯»æ±‚é’ˆå¯¹ OpenRouter ä¸Šä¸åŒæ¨¡å‹å’Œæä¾›å•†çš„è¿™äº›æ€§èƒ½é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚
LM Studio ç”¨æˆ·é¢ä¸´å¤§é‡ API é”™è¯¯ï¼Œå¯»æ±‚è°ƒè¯•æŒ‡å¯¼ï¼šLM Studio ç”¨æˆ·æŠ¥å‘Šåœ¨åŠ è½½æ¨¡å‹æ—¶å‡ºç°å¤§é‡é”™è¯¯ï¼Œå¦‚ã€ŒæœªçŸ¥é”™è¯¯ã€å’Œã€Œé€€å‡ºä»£ç ï¼š18446744072635812000ã€ï¼Œå› æ­¤å¸Œæœ›å®˜æ–¹æä¾›è¯¦ç»†çš„ç³»ç»Ÿé…ç½®ä¿¡æ¯ä»¥åŠ API ç›¸å…³çš„æ·±å…¥ä¿¡æ¯ï¼Œä»¥ä¾¿è¿›è¡Œæœ‰æ•ˆçš„è°ƒè¯•ã€‚é€šè¿‡ API è¿æ¥æ—¶å‡ºç°çš„çŠ¶æ€ç®¡ç†é—®é¢˜ä¹Ÿè¡¨æ˜ï¼Œéœ€è¦æ›´æ¸…æ™°çš„æ–‡æ¡£å’Œç”¨æˆ·æ”¯æŒï¼Œæ–¹ä¾¿ç”¨æˆ·è¿›è¡Œ API äº¤äº’ã€‚
Codeium Jetbrains æ’ä»¶å› æ— å“åº”å’Œé¢‘ç¹é‡å¯è€Œå¤‡å—è¯Ÿç—…ï¼šç”¨æˆ·å¯¹ Codeium Jetbrains æ’ä»¶æ„Ÿåˆ°ä¸æ»¡ï¼Œè®¤ä¸ºå…¶ç»å¸¸æ— å“åº”ï¼Œéœ€è¦é¢‘ç¹é‡å¯ï¼Œå½±å“äº†å¼€å‘äººå‘˜çš„å·¥ä½œæ•ˆç‡ã€‚ä¸€äº›ç”¨æˆ·ä¸ºäº†ä¿è¯å¯é æ€§ï¼Œé€‰æ‹©åˆ‡æ¢å› Copilotï¼Œè¿˜æœ‰ç”¨æˆ·æŠ¥å‘Šäº†åœ¨ PhpStorm ä¸­å‡ºç°çš„ç‰¹å®šé”™è¯¯ï¼Œè¡¨æ˜è¯¥æ’ä»¶çš„æ€§èƒ½æŒç»­ä¸ç¨³å®šã€‚
ä¸»é¢˜ 4. ç¤¾åŒºé©±åŠ¨çš„åˆ›æ–°å’Œå¼€æºè´¡çŒ®

Independent Researchers Leverage JAX and TPUs for Low-Cost AI Research: Independent AI researchers are exploring realistic domains for AI/ML research, with recommendations to learn JAX for access to TPU Research Cloud, enabling resource-efficient experimentation. The community cites the OpenMoE GitHub repository as a prime example of impactful research in Mixture-of-Experts models achievable with limited resources.
Y CLI Project Emerges as OpenRouter Terminal Chat Alternative: Y CLI, a personal project, offers a terminal-based web chat alternative for OpenRouter, storing chat data locally in jsonl files and now supporting Deepseek-r1 reasoning. Developers are actively encouraged to contribute to Y CLI via its GitHub repository, fostering community-driven development and catering to terminal enthusiasts.
Hugging Face Community Clones DeepResearch for Open Access: HuggingFace researchers launched an open-source clone of DeepResearch, emphasizing the importance of agent frameworks and introducing the GAIA benchmark to foster community contributions. This initiative promotes transparency and collaborative development in AI agent technology, encouraging broader participation and innovation.
Theme 5. Ethical Debates and Business Model Scrutiny in AI

æœªæ‰¾åˆ°æ„è¯‘å†…å®¹

OpenAI's Profit-First Approach Sparks Community Debate and Skepticism: Members are debating the motivations of AI giants like OpenAI, criticizing their prioritization of profit over public good and questioning the competitiveness of smaller companies. Skepticism surrounds OpenAI's updated chain of thought feature, with doubts about its real purpose amidst concerns about corporate agendas dominating AI development.
AI Backlash Echoes Crypto Distrust, Fuels Ethical Concerns: Public distrust towards AI is linked to past negative experiences with cryptocurrency and NFTs, impacting the perception of AI technology and raising ethical concerns about AI development. Critics point to unlicensed AI training data and the potential for AI to disrupt labor markets, fueling broader societal anxieties about AI's ethical implications.
Stability AI's Subscription Costs and 'Private Images' Option Spark Debate: Members are questioning the 'private images' option in Stability AI's Max subscription, debating if it implicitly caters to NSFW content, while others compare cloud service costs to local electricity expenses. These discussions reflect varying user attitudes towards the entry costs and perceived utility of different AI models, highlighting the ongoing debate about the economics of AI services.
PART 1: High level Discord summaries
Unsloth AI (Daniel Han) Discord
Unsloth's GRPO now reasons with vLLM!: Unsloth's latest update on GRPO allows reproducing DeepSeek-R1's reasoning with as little as 7GB VRAM, also supporting models with reduced memory use using Colab.
Users can experiment with the latest features and notebook updates for performance enhancements, as well as training Llama 3.1 (8B) and Phi-4 (14B) models.
Unsloth Fine-Tunes R1 Distill Llama + Qwen!: Unsloth introduced support for fine-tuning distilled DeepSeek models, utilizing Llama and Qwen architectures and making model uploads available.
Unsloth also supports new models such as Mistral-Small-24B-2501 and Qwen2.5, which can be found in the Hugging Face collection.
Quantization cuts VRAM by 60%!: Recent discussions highlight effective use of BitsandBytes quantization, reducing VRAM usage by approximately 60% when selectively quantizing layers with further details available in Unslothâ€™s blog posts.
Participants discussed using multi-turn conversational datasets with GRPO, emphasizing retaining reasoning contexts during model training and improving AI model reasoning capabilities with well-formatted datasets.
OpenAI Prioritizes Profit: Members debated motivations of major AI players like OpenAI, criticizing profit prioritization over public good, along with concerns about smaller companies' competitiveness and potential alliance needs.
A user highlighted OpenAI's updates to the chain of thought feature, linking to the announcement, but responses showed skepticism about its real purpose.
Indie AI Researchers Tap TPUs with JAX!: Independent researchers seek realistic domains to start AI/ML research, where a member recommends learning JAX for TPU Research Cloud access, linking to the application form.
Members cited the OpenMoE GitHub repository as a relevant example of conducting research in Mixture-of-Experts models, and even pretraining small transformers on the TinyStories dataset.
Stability.ai (Stable Diffusion) Discord
Stability Welcomes New Community Chief: Maxfield, the new Chief Community Guy at Stability, introduced himself to improve community engagement, previously contributing at Civitai since 2022.
Acknowledging past engagement was lackluster, Maxfield plans to launch a feature request board and encourage researchers to share project updates for improved transparency.
Civitai Plagued by Download Errors: Users reported encountering Error 1101 when downloading models from Civitai, leading to community frustration over downtime.
The issues raised concerns about the accessibility and reliability of accessing models via Civitai.
Users Dissect Latent Space Intricacies: A user expressed confusion over the complexity of tools for swapping latent space parameters, suggesting a need for more user-friendly solutions.
Discussions touched on potential implementations for newer diffusion models and the challenges of adapting existing architectures.
AI Subscription Costs Spark Debate: Members questioned the 'private images' option in Stability's Max subscription, debating if it catered to NSFW content, while others compared cloud service costs to local electricity expenses.
The discussions highlighted varying attitudes towards the entry costs versus the utility of different AI models.
Engineers Seek AI Prompting Clarity: A user sought insights into prompting techniques for generative models, while others suggested external tools like brxce/stable-diffusion-prompt-generator to assist.
The conversation underscored the difficulty in adapting to different AI model requirements and generating satisfactory prompts, especially across platforms.
Codeium (Windsurf) Discord
Windsurf Adds Gemini 2.0 Flash: Windsurf now supports Gemini 2.0 Flash, consuming only 0.25 user prompt credits per message and flow action credits per tool call, as announced in a tweet.
While blazingly fast and efficient, Gemini 2.0 Flash is limited in tool calling ability but excels at answering codebase-related questions.
Windsurf Next Beta Arrives: Users can now access the latest features of Windsurf Next by downloading the beta from this link.
The beta allows early exploration of new AI capabilities with the flexibility to switch between Next and Stable versions.
Jetbrains Plugin Criticized by Users: Users reported frustration with the Codeium Jetbrains plugin, citing frequent failures to respond and the necessity for frequent restarts.
One user switched back to Copilot for reliability, while another reported an error in PhpStorm related to file access.
Users Report Windsurf Performance Issues: Users reported performance issues with Windsurf, particularly with models like O3-mini and Gemini Flash, which finish prematurely without complete suggestions.
One user expressed frustration over the need to continuously prompt the model to 'continue', raising concerns about wasted credits.
Cascade Learns Web Search: Cascade can now perform web searches automatically or via user commands like @web and @docs, costing 1 flow action credit, described in the Windsurf Editor Changelogs.
This functionality supports URL input and uses web context to improve responses, aiming to provide more accurate and comprehensive information.
aider (Paul Gauthier) Discord
Aider Users Find Port Error Fix: A user reported an invalid port error with Aider when loading model metadata, indicating a potential configuration issue.
Another member suggested overriding the default model metadata file as a workaround to resolve this error, ensuring the tool functions correctly.
Gemini's Unique Editing Needs: Users discussed inconsistencies with DeepSeek and Gemini models, noting Gemini's unique editing format (udiff) differs from other models.
Aider automatically uses udiff for Google models while maintaining different defaults for others, accommodating this variation.
Pen Testing AI Profitable, Risky: A member shared their project for pen testing using LLMs, creating a simulated hacking environment where two models collaborate.
Despite high token usage, professional pen tests can be extremely lucrative, suggesting a potential financial benefit.
HuggingFace Clones DeepResearch: HuggingFace researchers created an open-source clone of DeepResearch, as detailed in their blog post.
This initiative emphasizes the importance of agent frameworks and introduces the GAIA benchmark, to foster community contributions.
R1 Model Dumps Junk Tokens: A user reported commit messages filled with `` tokens when using R1 via Together.ai, seeking guidance on configuration.
Recommendations included configuring model settings to minimize these tokens in commit messages and keep commits clean.
OpenAI Discord
Gemini 2.0 Pro Sparking Excitement: Users are excited about Gemini 2.0 Pro with its 2 million token context, which facilitates complex interactions, but raised concerns about its usability compared to Google AI Studio.
Free alternatives offer extensive customization and might give users better results on certain tasks, and the community suggests weighing effort against perceived value of additional features in premium models.
DeepSeek Tangles with ChatGPT for Chess Title: A potential chess match between DeepSeek and ChatGPT is piquing user's interest given the models' limitations on reasoning, which promises to be highly amusing.
Humorous contrasts were drawn between the pricing of DeepSeekâ€™s $1 chess game versus OpenAI's $100 chess game, suggesting some prefer the cheaper, yet still challenging, game.
Gemini Flash 2.0 and Copilot Shine as Coding Tools: In discussions about coding, members recommended Gemini Flash 2.0 and Microsoft Copilot for their features and cost-effectiveness, particularly for advanced mathematics.
Users noted that Copilot offers a free trial, making it easier to explore without immediate financial commitment and allows engineers to 'try before they buy'.
Deep Research Chat Eagerly Awaited by Plus Users: Several members expressed eagerness for the Deep Research chat feature to be available for Plus users soon, noting their need for it in the coming days.
A member inquired if anyone had shared information about Deep Research chats, obviously looking for insights, and prompted others to express similar anticipation regarding the feature coming to Plus subscriptions.
Fine Tuning AI with Iterative Editing: A member suggested using Python to count words and iterate to ensure better response length, but noted this may impact creativity when attempting to control Response Length in AI responses.
Members also noted the importance of editing inputs using the edit button to sculpt the AI's output effectively by adjusting your input until satisfied before proceeding to ensure coherent context in the conversation.
Cursor IDE Discord
Cursor IDE Gets GitHub Agents, Architect Feature: Users are excited about the new GitHub agents and the architect feature in Cursor IDE, which aims to boost productivity.
However, some users reported a potential bug with running commands within the Composer tool after recent updates, as noted in the Cursor forum.
Gemini 2.0 Self-Learning Solid, But Not Top Dog: Users find Gemini 2.0 performs well for self-learning tasks due to its affordability and context management, some discussion mentioned it was solid but not superior to Sonnet for coding.
The community noted its effective context use makes it appealing for handling large codebases, potentially shaking up AI testing tools like Momentic.
Clipboard Comparison Tool Recommendations: The community is recommending a VSCode extension for clipboard comparisons, which allows users to compare against clipboard content as documented in Microsoft's VSCode documentation.
Users are also drawing comparisons between VSCode's local history and JetBrains' Timeline, suggesting Timeline offers greater efficiency, and the Partial Diff extension from the VSCode Marketplace.
MCP Server Configs Demand Better Context: A user is seeking assistance with MCP server configurations and accessing keys for Supabase, noting limited access with some keys and the github repo for mcp-starter.
The community is generally highlighting the need for improved context management within Cursor, particularly for managing complex projects, using the releases from daniel-lxs/mcp-starter.
Cursor's Context Crunch Spurs Debate: Concerns are surfacing about context limitations in Cursor, with some users preferring models like Cline or Google models for their larger context windows, perhaps because they are reading Andrej Karpathy's tweets on vibe coding.
There's ongoing debate on how context size impacts the effectiveness of AI models, specifically how larger context windows could boost performance in broader applications, and the role of model specific rules as discussed in the cursor forum.
Perplexity AI Discord
Focus Mode Gets the Chop: Users noticed the temporary removal of Focus Mode in Perplexity AI, sparking debate on the necessity to explicitly mention sources like Reddit in prompts.
Some users expressed concerns about the complication this adds to their ability to direct the AI's information sourcing effectively.
Decoding Model Use in Perplexity Pro: Users are trying to clarify if Pro mode fully uses models like Claude 3.5 end-to-end or integrates R1 for reasoning, suggesting a more complex, multi-model approach.
Insights indicate that undisclosed models conduct initial searches before handing off to chosen models for the final answer generation.
ByteDance Dives Deep with Deepfakes: ByteDance's release of new deepfake technology has ignited discussions on the ethical implications and potential for misuse within the AI community.
Community members are actively speculating on the ramifications of this technology, weighing its innovative possibilities against its capacity for harm.
Desire for Model Transparency Swells: Users are urging Perplexity AI for clearer communication about model specifications and updates, particularly regarding changes that impact functionality and performance.
Greater transparency is expected to diminish user confusion and improve interaction with the platformâ€™s AI functionalities.
Sonar Pro Devs on Hot Seat for Security: An urgent call went out for contact with the Sonar Pro reasoning developers due to the discovery of a security issue.
Users were directed to email api@perplexity.ai to address the vulnerability.
OpenRouter (Alex Atallah) Discord
DeepSeek Insurance gets even deeper: OpenRouter now insures DeepSeek R1 requests that receive no completion tokens, so you won't be charged even if the upstream provider does.
The completion rate for Standard DeepSeek R1 has improved from 60% to 96% over time, making it a more reliable option.
Kluster's Cancellation Catastrophe Corrected: A Kluster integration issue caused delayed completion tokens and unexpected charges due to failure to cancel timed-out requests.
This issue has since been addressed, resolving the problem of users being charged despite apparent timeouts on OpenRouter's end.
Qwen Quitting Quietly: Novita is deprecating their Qwen/Qwen-2-72B-Instruct model, with OpenRouter set to disable it around the same time.
Users should transition away from this model to avoid disruption when the model becomes unavailable.
Y CLI yearns for your attention: Y CLI, a personal project and web chat alternative, stores all chat data in single jsonl files and has added Deepseek-r1 reasoning content support, evidenced in this asciinema recording.
Developers are encouraged to contribute to Y CLI via its GitHub repository, with a call for fellow terminal fans.
DeepInfra Deeply Inconsistent: Users reported that DeepInfra is currently failing to return responses 50% of the time due to an increase in processing delays, often causing zero token completions when utilized with applications like SillyTavern.
The community is sharing observations about performance differences between models and providers, including suggestions for improvements.
LM Studio Discord
User face LM Studio API Error Avalanche: Users reported errors like 'unknown error' and 'exit code: 18446744072635812000' when loading models in LM Studio, needing system specs and API details for debugging.
One user struggled with state handling when connecting to local models via API, indicating the need for better guidance on API interactions.
Obsidian's Smart Connections Extension Causes Turmoil: Users faced errors connecting Obsidian's Smart Connections extension to LM Studio, citing conflicts with other extensions and missing required fields in API responses.
Troubleshooting involved uninstalling conflicting plugins and rebuilding caches, though ongoing errors persisted even after setting up a connection.
TheBloke Models Still a Standard: Members inquired about the safety and reliability of downloading AI models from TheBloke, even with his reduced community presence.
It was confirmed that TheBloke's models remain a standard in the industry, with users encouraged to monitor community channels for availability updates.
DDR5 6000 EXPO Timings are Conservative: A user found their DDR5 6000 EXPO timings to be conservative, observing a peak memory bandwidth of 72 during inference.
After completing 4 passes of memtest86, another member suggested trying TestMem5 for a more rigorous assessment of stability.
DeepSeek R1 Model Support GPU Acceleration?: Inquiries arose about GPU acceleration for the DeepSeek R1 Distill Qwen 7B model, with uncertainty about which models support GPU use.
It was clarified that only specific models like Llama have known support for acceleration, leaving some ambiguity for the DeepSeek model.
MCP (Glama) Discord
Home Assistant Gets Functional MCP Client: A user released a Home Assistant with MCP client/server support and plans to add an animated talking head avatar via met4citizen/TalkingHead for better user interaction.
The project is still in development, as they balance paid work with open source development. Also, there was curiosity about usage statistics of the Home Assistant MCP bridging with tools like Claude.
Goose MCP Client Honks Loudly: Users shared positive experiences using the Goose MCP Client in testing, highlighting its effectiveness.
A pull request to enhance its logging features, block/goose@162c4c5, is in progress, with a fix to include cached tokens in usage count logs in Goose.
Claude Grapples Image Display: A user reported challenges displaying images as tool results on Claude Desktop, encountering an input error.
The error has led to speculation that converting image results to embedded resources might be a potential workaround.
PulseMCP Boasts Use Cases: A new showcase of practical PulseMCP Use Cases debuted, featuring instructions and videos for using various client apps and servers, and launched the use-cases on PulseMCP.
It highlights uses of Gemini voice, Claude, and Cline for managing Notion, converting Figma designs, and creating knowledge graphs.
Mobile MCP options discussed: Members suggested that Sage supports iPhones, while options for Android users may require using web clients like LibreChat or MCP-Bridge.
This conversation underscores interest in extending MCP functionality beyond desktop environments.
Yannick Kilcher Discord
Gemini 2.0 Pro Creates SVGs: Members discussed that Gemini 2.0 Pro demonstrates impressive performance in creating SVGs, surpassing models such as o3-mini and R1, as noted in Simon Willison's blog.
Several members also observed its enhanced SQL query capabilities, hinting at significant progress by Google with Gemini Flash 2.0.
DeepSpeed Dataloader Batch-size Woes: A user reported confusion regarding the need to manually define batch_size in Dataloader while utilizing DeepSpeed's auto batch size configuration.
Another member proposed the integration of DeepSpeed tags into the Dataloader for optimization and suggested potential performance modifications for specific nodes.
Harmonic Loss Paper Lacks Punch: Community members expressed skepticism towards the harmonic loss paper, deeming it hastily assembled and failing to provide meaningful performance improvements despite its theoretical advantages.
One member indicated that the GitHub repository associated with the paper offers more valuable information than the paper itself.
Gemini 2.0 Flash leaves mark: Users trying the new Gemini 2.0 Flash model through LlamaIndex reported incredible speeds, although not as fast as Groq.
One user stated that the model struggled with returning valid JSON formats, concluding it may not be suitable for tasks needing reliability in output.
S1 Model emerges under $50: The S1 reasoning model was discussed, highlighting its performance compared to models like OpenAI's o1 but at a fraction of the cost, under $50.
The S1 model and its tools are available on GitHub and was developed through distillation from Gemini 2.0.
Eleuther Discord
Adobe Seeks LLM Agent Research Partners: A senior ML Engineer at Adobe is looking for collaboration on LLM agent research projects.
Interested individuals are invited to join discussions to explore potential partnerships.
Deepspeed Batch Size still required: When using Deepspeed with auto batch sizing, the batch_size needs to be specified for the data loader.
This requirement persists despite auto batch sizing configuration.
Thematic Generalization Benchmark Emerges: A member shared a GitHub repository detailing a thematic generalization benchmark for evaluating LLMs in category inference from examples and anti-examples.
The benchmark's correlation with the performance of SAE autointerp was questioned.
New Architectures are Cookin' at RWKV: The RWKV team is actively developing some new architectures, showing their proactive stance.
One user grappling with scaling issues has invited discourse about prospective collaboration.
MATS 8.0 Cohort Applications Now Open: Applications for the MATS 8.0 cohort are open until Feb 28, offering opportunities for paid full-time mechanistic interpretability research, apply here.
Previous mentees have significantly contributed, evidenced by their involvement in 10 top conference papers.
Nous Research AI Discord
Deep Research Excites Users: Members laud OpenAI's Deep Research for efficiently gathering relevant connections and sources, boosting their cognitive bandwidth.
One user highlighted its ability to explore obscure online communities and gather unexpected data.
AI Backlash Echoes Crypto Concerns: Public distrust towards AI stems from past issues with cryptocurrency and NFTs, impacting the perception of AI technology, according to some members.
Critics are concerned about AI training data being unlicensed and the disruptive effects of AI on labor markets, as articulated in Why Everyone Is Suddenly Mad at AI.
Purpose AI Agent in Legal Limbo: A user aims to develop a purpose-driven AI agent within a legal trust framework, aiming to pioneer legal discourse around AI personhood.
Feedback centered on the engineering complexity, including integrating fiscal management functions while emphasizing the potential for custom software solutions like the ones shown in I Built the Ultimate Team of AI Agents in n8n With No Code (Free Template).
Model Merging Mania: Members discussed strategies for merging AI models, sharing insights on improving model instruction tuning and reasoning performance.
Various fine-tuning methods were explored, highlighting the benefits of innovative techniques in AI training to enhance model performance with tools like Unsloth Documentation.
Synthetic Data Dream: A member seeks resources on synthetic data generation, focusing on seed-based approaches similar to Self-Instruct, after facing challenges with Magpie outputs.
They discovered the Awesome-LLM-Synthetic-Data GitHub repository, offering a list of resources on LLM-based synthetic data generation.
Interconnects (Nathan Lambert) Discord
Schulman Shuffles from Anthropic: Leading AI researcher and OpenAI co-founder John Schulman has left Anthropic after around five months, prompting speculation about his next career steps link.
Potential destinations mentioned include Deepseek and AI2, according to sources.
Copilot Becomes Agent: GitHub Copilot introduced agent mode, enhancing developer assistance along with general availability of Copilot Edits link.
This update seeks to provide more effective coding support through AI.
LRM Test-Time Scaling Terminology Troubles: Members questioned the term test-time scaling for Long-Range Models (LRMs), emphasizing that models decide their own output link.
It was pointed out that scaling occurs during the training phase, rendering the term misleading; a member called the whole concept fundamentally flawed.
Qwen Achieves Magical Results: Qwen 2.5 models are showing impressive results with minimal training data, as noted by members discussing their findings link.
Aran Komatsuzaki remarked that Qwen models seem to have a magical quality, achieving notably good performance with limited data.
Scale AI Faces Adaptation Challenge: Members recognized that adaptation is possible for Scale AI, but challenges remain due to current operational models and valuations link.
The consensus was a bleak outlook without significant changes to their approach amid a shifting landscape.
Notebook LM Discord
NotebookLM Mobile Users Limited to One Model: Users cannot change the model within the mobile version of NotebookLM, a limitation causing frustration for those expecting greater flexibility.
This restriction hinders the user experience on mobile devices, leading to confusion among users accustomed to managing models on the web platform.
Gemini Shines with Sheets, NotebookLM Stumbles: Members voiced concerns about using NotebookLM for analyzing spreadsheet data, suggesting tools like Gemini within Google Sheets are more suitable.
As Engadget reported, Gemini can use Python code to generate insights and charts, reinforcing NotebookLM's strength as primarily a text analysis tool.
Sliders Could Refine AI Creativity: A user proposed integrating sliders for tuning AI's creativity, similar to features in the Gemini API, inspired by discovering an exploit related to the AI's features.
This functionality would allow users to adjust parameters, offering greater control over the creative output of AI models.
NotebookLM Summarizes Legal Testimony at NY Budget Hearing: A user employed NotebookLM to capture testimony at the New York State Legislatureâ€™s Budget hearing on Environmental Conservation.
The user highlighted the challenge of sharing this extensive document due to licensing, while the notes are available here.
Max Headroom Glitches Back, Critiques AI: The iconic Max Headroom makes a return with a new video, showcasing a unique approach to AI interaction.
As seen on Youtube, the new content humorously critiques corporate AI practices, urging viewers to share and engage.
LLM Agents (Berkeley MOOC) Discord
Fall 2024 MOOC Certificates Finally Drop: The Fall 2024 MOOC certificates were released today at 8am PT, after the resolution of technical challenges.
Some participants were downgraded to the Trailblazer tier due to incomplete coursework, with no makeups offered.
Certificate Timeline Elusive: Members expressed uncertainty regarding the certificate issuance timeline, hoping for delivery within a week or two due to unforeseen technical issues being resolved.
A member noted discrepancies in certificate receipt, indicating a potential soft bounce issue affecting communications.
Quiz Availability Creates Confusion: Concerns arose over the availability of answers for Quiz-1 as Quiz-2 was launched, prompting members to seek clarification on the new policy regarding answer releases.
Community members clarified that score visibility for Quiz-1 was possible through the original submission link.
Certificate Tier Distribution: It was revealed that there are 301 Trailblazer, 138 Masters, 89 Ninjas, 11 Legends, and 7 Honorees amongst the participants.
Clarification was provided that only the honorary tier would be noted if both an honorary and a specific tier were achieved.
Course Experience Earns Praise: The community expressed gratitude for the support received during the course, especially acknowledging the team handling grading and certificate queries.
Participants shared enthusiasm for the course, with one member reflecting on their learning journey and the significance of their certificate for future endeavors.
GPU MODE Discord
NVIDIA Blackwell gets OpenAI Triton: The Triton compiler now supports the NVIDIA Blackwell architecture due to ongoing collaboration between NVIDIA and OpenAI, enhancing performance and programmability via cuDNN and CUTLASS.
This advancement enables developers to optimize matrix multiplication and attention mechanisms for modern AI workloads, improving efficiency and capabilities.
Minimize AI Research Costs: Members shared that independent researchers can conduct efficient work on LLMs and vision tasks while fine-tuning models on a limited budget, economizing AI research via stability with low-bit training weights.
The success of GPT-2 speedruns with Muon was highlighted as a prime example of impactful research using limited resources.
FP8 Attention requires Hadamard: A member observed that FP8 Attention for video models performed significantly better when utilizing the Hadamard Transform, drastically reducing error rates; the Flash Attention 3 paper suggests that this approach is crucial for operations in FP8.
Another member recommended using the fast-hadamard-transform repository to implement Hadamard before the attention mechanism for enhanced performance.
Reasoning Gym Embraces Sokoban Puzzles: A pull request was submitted to add Sokoban puzzles to reasoning-gym, demonstrating a new puzzle format for users to solve, including a graphic explanation of the puzzle setup along with example moves.
Members are also discussing collaboratively building a basic gym for the Rush Hour game integration into the reasoning-gym to encourage joint coding efforts.
Linear Attention faces Distillation Challenges: A member attempted to distill a small LLM to a linear attention model following a recipe from Lolcats but the model only produced repeating characters.
The member reached out for help specifically from the Lolcats team, highlighting the community support aspect often relied upon in AI model development.
Nomic.ai (GPT4All) Discord
O3 Remains Ahead Despite Pricing: Despite pricing concerns, O3 continues to outperform other models, with Llama 4 being anticipated as the next potential challenger, according to discussions in the general channel.
Links comparing DeepSeek-R1 vs o3 are available online and o3-mini vs DeepSeek-R1 are also available.
DeepSeek Constrained in Political Discussions: Users found that DeepSeek has greater limitations than ChatGPT and O3-mini in sensitive political discussions, often resulting in unexpected deletions or evasions.
This highlights potential constraints in language models when prompted with sensitive political topics.
DeepSeek's Cutoff Date Raises Questions: Reportedly, DeepSeek's knowledge cutoff date is July 2024, which raises questions about its current relevance given that we are now in 2025.
The Time Bandit method, for extracting information by leveraging temporal context, was discussed in relation to DeepSeek, with more details on its system prompt available online.
Torchtune Discord
GRPO Implementation Scores Big: A member reported a successful implementation of GRPO training, achieving training scores ranging from 10% to 40% on GSM8k.
While debugging, they noted challenges with deadlocks and memory management, and are planning improvements and opening the project for contributions.
Kolo Extends to Torchtune: Kolo officially announced support for Torchtune on their GitHub page.
The project delivers a comprehensive solution for fine-tuning and testing LLMs locally using the best available tools.
Llama 3.1 and Qwen 2.5 stumble on Configs: Members identified FileNotFoundError issues downloading and fine-tuning Llama 3.1 and Qwen 2.5 due to mismatched path configurations.
One member created a GitHub issue to address the incorrect default paths and propose fixes.
Hugging Face Fast Tokenizers Get Support: The community discussed the prospect of using Hugging Face fast tokenizers, with members indicating current limitations but ongoing progress.
A member mentioned that Evan is actively enabling support, as detailed in this GitHub pull request.
Full DPO Distributed PR Faces Hurdles: A user reported issues with GitHub checks on their Full DPO Distributed PR, with specific errors related to GPU and OOM issues.
The error, ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!, prompted the user to seek assistance from the community.
Modular (Mojo ğŸ”¥) Discord
Mojo Pivots from Python, Focuses on GPU: In a recent community meeting, Modular clarified that Mojo is not currently a superset of Python, but focuses on leveraging GPU and performance programming.
This shift emphasizes enhancing Mojo's efficiency in its designed applications rather than broadening its language framework.
Parser Revision Balances Branching Costs: A member suggested that the parser needs adjustment for handling multiple slices of data, weighing the costs of branching and noting that branching may be cheaper than significant data transfers.
This is a valid consideration for those not focusing on higher performance needs.
Msty Simplifies Local Model Access: A member introduced Msty, an OpenAI-compatible client that simplifies local model interactions compared to using Docker and other complex setups, highlighting its ease of use and features for accessing AI models seamlessly with Msty's Website.
The importance of offline usability and privacy with Msty was emphasized, suggesting it is highly favorable for users who wish to avoid complex configurations.
MAX Serve CLI Mimics Ollama's Features: Members discussed building a CLI similar to ollama on top of MAX Serve, noting that MAX Serve can already handle many functionalities offered by Ollama with a docker container.
The discussion highlighted the hope for better performance running local models compared to Ollama.
Community Reports OpenAI API Incompatibilities: A user reported missing features in the OpenAI completions API with max serve (v24.6), such as generation stopping at specified tokens, suggesting that they file issues on the GitHub repo to highlight these missing elements.
The group acknowledged ongoing issues with OpenAI API compatibility, particularly referencing the v1/models endpoint, and other missing functionalities like token stopping and prompt handling in this GitHub issue.
Latent Space Discord
Hibiki Champions Real-time Translation: Kyutai's Hibiki model achieves real-time speech-to-speech translation from ğŸ‡«ğŸ‡· to ğŸ‡¬ğŸ‡§, preserving the speaker's voice and adapting to context.
Early reports claim Hibiki excels in quality, naturalness, and speaker similarity, rivaling human interpreters.
Melanie Mitchell Raises Agent Concerns: @mmitchell_ai's latest paper argues against developing Fully Autonomous Agents, emphasizing ethical considerations.
The piece sparked debates within the AI community, acknowledging her balanced perspective amidst fervent discussions.
Mistral AI's Le Chat Enters the Scene: Mistral AI launched Le Chat, a versatile AI assistant tailored for daily personal and professional tasks, accessible on web and mobile.
The tool is set to redefine user interaction with AI, potentially impacting workflows and personal routines.
OpenAI Enhances o3-mini Capabilities: OpenAI rolled out enhanced chain of thought features in o3-mini and o3-mini-high (source), benefiting both free and paid subscribers.
The updates promise improved performance and a smoother user experience, reaffirming OpenAI's commitment to continuous service evolution.
PDF Parsing Achieves Breakthrough: PDF parsing is now efficiently solved at scale; Gemini 2 Flash can parse large documents for approximately $1 per 6000 tokens, according to @deedydas.
This advancement in processing complex documents unlocks new possibilities for applications needing high-caliber text extraction.
LlamaIndex Discord
Gemini 2.0 is now generally available: Gemini 2.0 from @google launched with day 0 support, developers can install the latest integration package with pip install llama-index-llms-gemini and read more in the announcement blog post.
The updated 2.0 Flash is available to all users in the Gemini app on desktop and mobile.
LlamaParse Tackles Complex Financials: Hanane D showcased the parsing of complex financial documents accurately and cost-effectively using LlamaParse 'Auto' mode, using @OpenAI embeddings as shared in this link.
Her demonstration highlights the advancements in parsing technology for extracting relevant insights from intricate data, charts and tables.
Embedding Print Troubles LlamaIndex: A member requested deletion of the embedding print from the LlamaIndex documentation due to excessive space usage and readability issues, see GitHub issue.
Another member offered to create a Pull Request (PR) to address the embedding print removal.
MLOps @Chipro Discord
LLMs Classify Well, But Noise Makes Them Falter: Members discussed that although LLMs are effective for classification, noisy data requires additional techniques like dense embeddings and autoencoder rerankers to improve performance.
This suggests the necessity of a more intricate strategy when handling challenging data scenarios.
Latency Concerns Dampen LLM Enthusiasm: The discussion revealed that although LLMs classify effectively, their suitability might diminish in scenarios with strict latency requirements due to their processing limits.
The suitability of LLMs depends on the specific latency constraints of a given application.
Business Requirements Highlight ML Misfits: A member noted that a missed opportunity occurred in properly framing the business requirements during the transition to an ML solution.
It should have been evident from the onset that if low-latency is paramount, traditional LLMs might not be the ideal choice.
Cohere Discord
Cohere Fine-Tuning Limits Spark Concern: A user encountered a BadRequestError (Status code: 400) in Cohere, indicating the training configuration surpassed the maximum of 250 training steps, with a batch size limit of 16.
A member questioned if this restricts fine-tuning to 4000 examples, highlighting that this limitation wasn't previously in place.
AIML System Design Interview Questions Requested: A member inquired about system design interview questions specific to AI/ML in the Cohere channel.
Another member acknowledged the request and indicated it would be collected, implying team collaboration on this topic.
Gorilla LLM (Berkeley Function Calling) Discord
Request for Canonical System Prompts Arises: A member requested clarification on the canonical system prompts for fine-tuned tool-using models, noting the Gorilla paper lacked this detail.
The goal is to ensure the models reliably return responses or JSON for function calls, suggesting a need for standardized prompt engineering practices.
Hugging Face Datasets Seek Transformation: A member aimed to streamline experimentation by transforming data and using datasets.map on Hugging Face, signalling a move towards more flexible data manipulation.
This highlights ongoing efforts to improve the usability and accessibility of datasets for research and development purposes.
Dataset Format Issue with Hugging Face: A member reported a dataset format mismatch within Hugging Face, where .json files actually contained jsonl data, leading to compatibility problems.
The suggested solution involves renaming the file suffix to .jsonl and adjusting the dataset config files to align with the actual data format.
DSPy Discord
Paper Posted on DSPy: A member shared a link to a paper on DSPy.
The paper was shared in the #papers channel.
Member Asks About Git Repo: In the #examples channel, a member inquired about the availability of a Git repo for their work, indicating interest in accessing related code or resources.
The member did not specify which project it was referring to.
Colab Notebook Surfaces: In response to the Git repo query, a member provided a link to a Colab notebook.
Accessing the notebook requires signing in and it is likely related to the DSPy discussion.
The tinygrad (George Hotz) Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.

æœªæ‰¾åˆ°æ„è¯‘å†…å®¹

The AI21 Labs (Jamba) Discord has no new messages. If this guild has been quiet for too long, let us know and we will remove it.

PART 2: Detailed by-Channel summaries and links
Unsloth AI (Daniel Han) â–· #general (516 messagesğŸ”¥ğŸ”¥ğŸ”¥):
GRPO and vLLM integration, DeepSeek models and fine-tuning, Quantization techniques, Multi-turn conversational datasets, AI ethics and data privacy

AI21 Labsï¼ˆJambaï¼‰çš„ Discord é¢‘é“ç›®å‰æ²¡æœ‰æ–°æ¶ˆæ¯ã€‚å¦‚æœè¯¥é¢‘é“é•¿æ—¶é—´å¤„äºé™é»˜çŠ¶æ€ï¼Œè¯·å‘ŠçŸ¥æˆ‘ä»¬ï¼Œæˆ‘ä»¬å°†ç§»é™¤å®ƒã€‚

ç¬¬äºŒéƒ¨åˆ†ï¼šå„é¢‘é“è¯¦ç»†æ‘˜è¦åŠé“¾æ¥
Unsloth AIï¼ˆDaniel Hanï¼‰â–· #generalï¼ˆ516 æ¡æ¶ˆæ¯ğŸ”¥ğŸ”¥ğŸ”¥)ï¼š
GRPO å’Œ vLLM é›†æˆã€DeepSeek æ¨¡å‹åŠå¾®è°ƒã€é‡åŒ–æŠ€æœ¯ã€å¤šè½®å¯¹è¯æ•°æ®é›†ã€AI ä¼¦ç†ä¸æ•°æ®éšç§

GRPO with vLLM Support: The latest update on GRPO reasoning allows users to reproduce DeepSeek-R1's reasoning with reduced memory use, now supporting models with as little as 7GB VRAM.
Users are encouraged to experiment and test the latest features and notebook updates for enhanced performance.
Fine-Tuning DeepSeek Models: Unsloth has introduced support for fine-tuning distilled DeepSeek models, with future notebooks planned for guiding users in this process.
These distilled models utilize Llama and Qwen architectures, allowing compatibility with Unsloth.
Quantization Insights: Recent discussions highlight the effective use of BitsandBytes quantization, with a significant reduction in VRAM usage by approximately 60% when selectively quantizing layers.
Users have shown interest in further reading about the details of this quantization technique in Unslothâ€™s blog posts.
Conversational Dataset Handling: Participants discussed the use of multi-turn conversational datasets with GRPO, emphasizing the nuances of retaining reasoning contexts during model training.
There was a consensus that a well-formatted dataset can enhance the reasoning capabilities of AI models.
Ethics and AI Development: A debate emerged about data privacy and the implications of closed-source models versus open-source alternatives in AI development.
Users expressed concerns about the direction of AI and the importance of building models aligned with user values rather than corporate agendas.
Links mentioned:
DownloadMoreRAM.com - CloudRAM 2.0: no description found
Run DeepSeek-R1 Dynamic 1.58-bit: DeepSeek R-1 is the most powerful open-source reasoning model that performs on par with OpenAI's o1 model.Run the 1.58-bit Dynamic GGUF version by Unsloth.
Llama 3.2 - a unsloth Collection: no description found
unsloth/SmolLM2-135M-Instruct-GGUF Â· Hugging Face: no description found
Unsloth - Dynamic 4-bit Quantization: Unsloth's Dynamic 4-bit Quants selectively avoids quantizing certain parameters. This greatly increases accuracy while maintaining similar VRAM use to BnB 4bit.
Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search: PaperGithubHuggingfaceIntroductionSince the release of OpenAI's o1, significant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. T...
Tweet from Unsloth AI (@UnslothAI): You can now reproduce DeepSeek-R1's reasoning on your own local device!Experience the "Aha" moment with just 7GB VRAM.Unsloth reduces GRPO training memory use by 80%.15GB VRAM can transfor...
Travis Neil Primrose GIF - Travis Neil primrose Neil - Discover & Share GIFs: Click to view the GIF
GitHub - cognitivecomputations/stablemax-orthogonal: Contribute to cognitivecomputations/stablemax-orthogonal development by creating an account on GitHub.
black-forest-labs/FLUX.1-dev Â· Hugging Face: no description found
Stanford's s1-32B Model Outperforms OpenAI's o1-Preview by 27% on AIME24 Math Questions Using 1,000 Diverse Questions | DeepNewz: Stanford researchers have introduced a new approach called Simple Test-Time Scaling (s1), which enhances reasoning performance in language models. The s1-32B model, fine-tuned on a dataset of 1,000 di...
[Fixing] More finetuning support Â· Issue #1561 Â· unslothai/unsloth: Support sequence classification Flex Attention for Gemma and others Variable sequence length and auto unpadding / padding Tool Calling Refactor and merge xformers, SDPA, flash-attn, flex-attention
llama.cpp GGUF breaks [FIXED] Â· Issue #1376 Â· unslothai/unsloth: As of 3rd December 2024 - fixed. Please update Unsloth via pip install --upgrade --no-deps --no-cache-dir unsloth
Unsloth AI (Daniel Han) â–· #announcements (1 messages):
Reasoning in Unsloth, DeepSeek-R1, Model Fine-Tuning, New Model Support

GRPO ä¸ vLLM æ”¯æŒï¼šGRPO æ¨ç†åŠŸèƒ½è¿æ¥æ–°è¿›å±•ï¼Œç”¨æˆ·ç°åœ¨å¯ä»¥ç”¨æ›´å°‘çš„å†…å­˜å¤ç° DeepSeek-R1 çš„æ¨ç†è¿‡ç¨‹ï¼Œå³ä½¿æ˜¯ VRAM ä»…æœ‰ 7GB çš„æ¨¡å‹ä¹Ÿèƒ½èƒœä»»ã€‚
æˆ‘ä»¬é¼“åŠ±å¤§å®¶ç§¯æä½“éªŒæœ€æ–°çš„åŠŸèƒ½å’Œæ›´æ–°çš„ Notebookï¼Œä»¥è·å¾—æ›´å‡ºè‰²çš„æ€§èƒ½ã€‚
DeepSeek æ¨¡å‹å¾®è°ƒï¼šUnsloth å·²ç»å¼€å§‹æ”¯æŒå¯¹ç²¾ç‚¼è¿‡çš„ DeepSeek æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä¸”è®¡åˆ’æ¨å‡º Notebookï¼ŒæŒ‡å¯¼ç”¨æˆ·å®Œæˆæ•´ä¸ªæµç¨‹ã€‚
è¿™äº›ç²¾ç‚¼æ¨¡å‹é‡‡ç”¨äº† Llama å’Œ Qwen çš„æ¶æ„ï¼Œå› æ­¤å¯ä»¥å®Œç¾å…¼å®¹ Unslothã€‚
é‡åŒ–æ´è§ï¼šæœ€è¿‘çš„è®¨è®ºæ˜¾ç¤ºï¼ŒBitsandBytes é‡åŒ–æŠ€æœ¯æ•ˆæœæ˜¾è‘—ã€‚é€šè¿‡å¯¹ç‰¹å®šå±‚è¿›è¡Œé€‰æ‹©æ€§é‡åŒ–ï¼ŒVRAM çš„ä½¿ç”¨é‡å¯ä»¥å‡å°‘å¤§çº¦ 60%ï¼
å¦‚æœæ‚¨æƒ³æ·±å…¥äº†è§£ Unsloth åšå®¢ä¸­å…³äºè¿™ç§é‡åŒ–æŠ€æœ¯çš„æ›´å¤šç»†èŠ‚ï¼Œæ¬¢è¿è¿›ä¸€æ­¥é˜…è¯»ã€‚
å¯¹è¯æ•°æ®é›†å¤„ç†ï¼šæœ‰å¼€å‘è€…è®¨è®ºäº†å¦‚ä½•å°†å¤šè½®å¯¹è¯æ•°æ®é›†åº”ç”¨äº GRPOï¼Œé‡ç‚¹åœ¨äºå¦‚ä½•åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä¿ç•™æ¨ç†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
å¤§å®¶ä¸€è‡´è®¤ä¸ºï¼Œç²¾å¿ƒè®¾è®¡çš„æ•°æ®é›†èƒ½å¤Ÿæ˜¾è‘—æå‡ AI æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚
ä¼¦ç†ä¸ AI å¼€å‘ï¼šå…³äºæ•°æ®éšç§ï¼Œä»¥åŠé—­æºæ¨¡å‹å’Œå¼€æºæ›¿ä»£æ–¹æ¡ˆåœ¨ AI å¼€å‘ä¸­çš„å½±å“ï¼Œä¸€åœºæ¿€çƒˆçš„è®¨è®ºæ­£åœ¨è¿›è¡Œã€‚
ç”¨æˆ·è¡¨è¾¾äº†å¯¹ AI å‘å±•æ–¹å‘çš„å…³æ³¨ï¼Œå¹¶å¼ºè°ƒæ„å»ºç¬¦åˆç”¨æˆ·ä»·å€¼è§‚ã€è€Œéä»…ä»…æœåŠ¡äºå…¬å¸åˆ©ç›Šçš„ AI æ¨¡å‹è‡³å…³é‡è¦ã€‚

ç›¸å…³é“¾æ¥ï¼š
DownloadMoreRAM.com - CloudRAM 2.0ï¼šæš‚æ— æè¿°
Run DeepSeek-R1 Dynamic 1.58-bitï¼šDeepSeek R-1 æ˜¯ç›®å‰æœ€å¼ºå¤§çš„å¼€æºæ¨ç†æ¨¡å‹ä¹‹ä¸€ï¼Œæ€§èƒ½ä¸ OpenAI çš„ o1 æ¨¡å‹ä¸ç›¸ä¸Šä¸‹ã€‚å¿«æ¥ä½“éªŒ Unsloth æä¾›çš„ 1.58-bit åŠ¨æ€ GGUF ç‰ˆæœ¬ã€‚
Llama 3.2 - a unsloth Collectionï¼šæš‚æ— æè¿°
unsloth/SmolLM2-135M-Instruct-GGUFÂ·Hugging Faceï¼šæš‚æ— æè¿°
Unsloth - Dynamic 4-bit Quantizationï¼šUnsloth çš„åŠ¨æ€ 4-bit é‡åŒ–æŠ€æœ¯å¯ä»¥é€‰æ‹©æ€§åœ°é¿å…å¯¹æŸäº›å‚æ•°è¿›è¡Œé‡åŒ–ï¼Œä»è€Œåœ¨ä¿æŒä¸ BnB 4bit ç›¸ä¼¼ VRAM ä½¿ç”¨ç‡çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜å‡†ç¡®æ€§ã€‚
Satoriï¼šReinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Searchï¼šPaperGithubHuggingfaceIntroductionSince the release of OpenAI's o1ï¼Œsignificant efforts have been made within the research community to enhance open-source LLMs with advanced reasoning capabilities. T...
Tweet from Unsloth AIï¼ˆ@UnslothAI)ï¼šYou can now reproduce DeepSeek-R1's reasoning on your own local device!Experience theã€ŒAhaã€moment with just 7GB VRAM.Unsloth reduces GRPO training memory use by 80%.15GB VRAM can transfor...
Travis Neil Primrose GIF - Travis Neil primrose Neil - Discover & Share GIFsï¼šç‚¹å‡»æŸ¥çœ‹ GIF
GitHub - cognitivecomputations/stablemax-orthogonalï¼šé€šè¿‡åœ¨ GitHub ä¸Šåˆ›å»ºä¸€ä¸ªå¸æˆ·ï¼Œå‚ä¸ cognitivecomputations/stablemax-orthogonal é¡¹ç›®çš„å¼€å‘ã€‚
black-forest-labs/FLUX.1-devÂ·Hugging Faceï¼šæš‚æ— æè¿°
Stanford's s1-32B Model Outperforms OpenAI's o1-Preview by 27% on AIME24 Math Questions Using 1,000 Diverse Questions | DeepNewzï¼šæ–¯å¦ç¦å¤§å­¦çš„ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§åä¸ºç®€å•æµ‹è¯•æ—¶ç¼©æ”¾ï¼ˆSimple Test-Time Scalingï¼Œs1ï¼‰çš„æ–°æ–¹æ³•ï¼Œèƒ½å¤Ÿæå‡è¯­è¨€æ¨¡å‹çš„æ¨ç†æ€§èƒ½ã€‚åœ¨åŒ…å« 1,000 ä¸ªä¸åŒé—®é¢˜çš„ AIME24 æ•°å­¦é¢˜æ•°æ®é›†ä¸Šï¼Œç»è¿‡å¾®è°ƒçš„ s1-32B æ¨¡å‹çš„æ€§èƒ½è¶…è¶Šäº† OpenAI çš„ o1-Preview è¾¾ 27%ã€‚
[Fixing] More finetuning supportÂ·Issue #1561Â·unslothai/unslothï¼šä¿®å¤äº†æ›´å¤šå¾®è°ƒæ”¯æŒçš„é—®é¢˜ï¼šåŒ…æ‹¬ Gemma ç­‰æ¨¡å‹çš„åºåˆ—åˆ†ç±»ã€Flex Attentionã€å¯å˜åºåˆ—é•¿åº¦å’Œè‡ªåŠ¨å¡«å…… / å–æ¶ˆå¡«å……ï¼Œä»¥åŠå·¥å…·è°ƒç”¨ï¼Œå¹¶é‡æ„å’Œåˆå¹¶äº† xformersã€SDPAã€flash-attnã€flex-attention ç­‰åŠŸèƒ½ã€‚
llama.cpp GGUF breaks [FIXED]Â·Issue #1376Â·unslothai/unslothï¼šæˆªè‡³ 2024 å¹´ 12 æœˆ 3 æ—¥ï¼Œæ­¤é—®é¢˜å·²ä¿®å¤ã€‚è¯·ä½¿ç”¨ pip install --upgrade --no-deps --no-cache-dir unsloth å‘½ä»¤æ›´æ–° Unslothã€‚
Unsloth AIï¼ˆDaniel Hanï¼‰â–· #announcementsï¼ˆ1 messages):
- Unsloth ä¸­çš„æ¨ç†
- DeepSeek-R1
- æ¨¡å‹å¾®è°ƒ
- æ–°æ¨¡å‹æ”¯æŒ

Unsloth introduces Reasoning with R1: Unsloth has unveiled reasoning capabilities with the release of R1, which can be trained locally or for free on Colab. The approach allows users to reproduce R1-Zero's insights with just 7GB of VRAM.
Additionally, the Colab notebooks provide resources for both Llama 3.1 (8B) and Phi-4 (14B) models.
DeepSeek-R1 boosts accuracy: The new R1 Dynamic 1.58-bit model has been introduced, promising greater accuracy compared to standard bits. More details and a tutorial can be found in the DeepSeek-R1 blog.
Additionally, users can now fine-tune R1 Distill Llama + Qwen models, with model uploads available.
Support for Mistral and Qwen models.: Unsloth has added support for new models such as Mistral-Small-24B-2501 and Qwen2.5, which can be found in the Hugging Face collection.
Users can also explore models with 1M context on Hugging Face.
Links mentioned:
Google Colab: no description found
Google Colab: no description found
Unsloth Documentation: no description found
unsloth (Unsloth AI): no description found
Unsloth Documentation: no description found
Unsloth AI (Daniel Han) â–· #off-topic (38 messagesğŸ”¥):
Model Merging, DeepSeek V3, User Benefit vs. Corporate Profit, OpenAI developments, Societal Value in AI

Unsloth å‘å¸ƒ R1ï¼Œå±•ç°å…¨æ–°æ¨ç†èƒ½åŠ›ï¼šUnsloth é€šè¿‡å‘å¸ƒ R1 æ­ç¤ºäº†å…¶æ¨ç†èƒ½åŠ›ï¼ŒR1 æ—¢å¯ä»¥åœ¨æœ¬åœ°è®­ç»ƒï¼Œä¹Ÿå¯ä»¥åœ¨ Colab ä¸Šå…è´¹ä½¿ç”¨ã€‚è¯¥æ–¹æ³•å…è®¸ç”¨æˆ·ä»…ç”¨ 7GB çš„æ˜¾å­˜ï¼ˆVRAMï¼‰å³å¯å¤ç° R1-Zero çš„å®éªŒç»“æœã€‚
æ­¤å¤–ï¼ŒColab notebooks ä¸º Llama 3.1ï¼ˆ8Bï¼‰å’Œ Phi-4ï¼ˆ14Bï¼‰æ¨¡å‹æä¾›äº†ç›¸å…³èµ„æºã€‚
DeepSeek-R1 æå‡ç²¾åº¦ï¼šæ–°æ¨å‡ºçš„ R1 Dynamic 1.58-bit æ¨¡å‹ï¼Œç›¸è¾ƒäºæ ‡å‡†ä½ï¼ˆbitsï¼‰æ¨¡å‹ï¼Œèƒ½å¤Ÿå®ç°æ›´é«˜çš„ç²¾åº¦ã€‚æ›´å¤šè¯¦æƒ…å’Œä½¿ç”¨æ•™ç¨‹è¯·å‚è€ƒ DeepSeek-R1 åšå®¢ã€‚
æ­¤å¤–ï¼Œç”¨æˆ·ç°åœ¨å¯ä»¥å¯¹ R1 Distill Llama + Qwen æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¹¶ä¸Šä¼ æ¨¡å‹ã€‚
æ”¯æŒ Mistral å’Œ Qwen æ¨¡å‹ï¼šUnsloth æ–°å¢äº†å¯¹ Mistral-Small-24B-2501 å’Œ Qwen2.5 ç­‰æ–°å‹å·çš„æ”¯æŒï¼Œè¿™äº›æ¨¡å‹å¯ä»¥åœ¨ Hugging Face é›†åˆä¸­æ‰¾åˆ°ã€‚
ç”¨æˆ·è¿˜å¯ä»¥åœ¨ Hugging Face ä¸Šæ¢ç´¢å…·æœ‰ 100 ä¸‡ï¼ˆ1Mï¼‰Token ä¸Šä¸‹æ–‡é•¿åº¦çš„æ¨¡å‹ã€‚
ç›¸å…³é“¾æ¥ï¼š
Google Colabï¼šæœªæ‰¾åˆ°æè¿°
Google Colabï¼šæœªæ‰¾åˆ°æè¿°
Unsloth æ–‡æ¡£ï¼šæœªæ‰¾åˆ°æè¿°
unslothï¼ˆUnsloth AI)ï¼šæœªæ‰¾åˆ°æè¿°
Unsloth æ–‡æ¡£ï¼šæœªæ‰¾åˆ°æè¿°
Unsloth AIï¼ˆDaniel Hanï¼‰â–· #off-topicï¼ˆ38 messagesğŸ”¥)ï¼š
æ¨¡å‹åˆå¹¶ã€DeepSeek V3ã€ç”¨æˆ·åˆ©ç›Šä¸å…¬å¸åˆ©æ¶¦çš„æƒè¡¡ã€OpenAI å‘å±•ã€AI çš„ç¤¾ä¼šä»·å€¼

Discussion on Model Merging Limitations: A user expressed doubt about their ability to reproduce DeepSeek V3 despite having sufficient funding, stating, 'I am not smart enough.'
Another member shared their commitment to address this challenge, mentioning the need to spearhead conversations around solutions.
Concerns About AI Corporations: There was a debate about the motivations of major AI players like OpenAI, with one member criticizing that companies prioritize profit over public good, stating, 'those that put profit over people win at this point'.
Discussion led to questioning whether smaller companies could compete effectively under such conditions, suggesting that alliances may be necessary for survival.
OpenAI's Latest Updates: A user highlighted an announcement from OpenAI about updates to the chain of thought feature for various user tiers, linking to the update here.
Responses to the update included skepticism about it providing any real purpose, with comments like 'just general ones'.
The Value of AI and User Focus: A user argued that most competitors fail to focus on the end userâ€™s needs, highlighting that 'no competitor has actually focused on user directly.'
They proposed that emphasizing societal value could yield greater benefits than current corporate models.
Federated Collective Training Approach: Members discussed potential paths forward in the AI space, including the idea of federated collective ways to train models and challenge existing monopolies.
Opinions varied on the feasibility of surpassing larger entities with superior intelligence or through cooperative methodologies, touching on a Marxist perspective of claiming production.
Links mentioned:
Tweet from OpenAI (@OpenAI): Updated chain of thought in OpenAI o3-mini for free and paid users, and in o3-mini-high for paid users.
GitHub - SakanaAI/evolutionary-model-merge: Official repository of Evolutionary Optimization of Model Merging Recipes: Official repository of Evolutionary Optimization of Model Merging Recipes - SakanaAI/evolutionary-model-merge
Unsloth AI (Daniel Han) â–· #help (188 messagesğŸ”¥ğŸ”¥):
Unsloth model training, GRPO and reward functions, Model merging issues, Continued pretraining with LoRA, Adapter performance comparison

å…³äºæ¨¡å‹åˆå¹¶å±€é™æ€§çš„è®¨è®ºï¼šä¸€ä½ç”¨æˆ·è¡¨è¾¾äº†å¯¹å…¶å¤ç° DeepSeek V3 èƒ½åŠ›çš„æ€€ç–‘ï¼Œå³ä½¿ä»–ä»¬æœ‰è¶³å¤Ÿçš„èµ„é‡‘ï¼Œå¹¶è¡¨ç¤ºï¼Œã€Œæˆ‘ä¸å¤Ÿèªæ˜ã€ã€‚
å¦ä¸€ä½æˆå‘˜åˆ†äº«äº†ä»–ä»¬è‡´åŠ›äºåº”å¯¹è¿™ä¸€æŒ‘æˆ˜çš„å†³å¿ƒï¼Œå¹¶æåˆ°éœ€è¦å¸¦å¤´è®¨è®ºè§£å†³æ–¹æ¡ˆã€‚
å…³äº AI å…¬å¸çš„æ‹…å¿§ï¼šå…³äºåƒ OpenAI è¿™æ ·çš„å¤§å‹ AI å…¬å¸çš„åŠ¨æœºå­˜åœ¨äº‰è®ºï¼Œä¸€ä½æˆå‘˜æ‰¹è¯„è¿™äº›å…¬å¸ä¼˜å…ˆè€ƒè™‘åˆ©æ¶¦è€Œä¸æ˜¯å…¬å…±åˆ©ç›Šï¼Œå¹¶è¡¨ç¤ºã€Œé‚£äº›æŠŠåˆ©æ¶¦ç½®äºäººæ°‘ä¹‹ä¸Šçš„äººç›®å‰è·èƒœã€ã€‚
è®¨è®ºå¼•å‘äº†å¯¹å°å‹å…¬å¸åœ¨è¿™ç§æ¡ä»¶ä¸‹æ˜¯å¦èƒ½æœ‰æ•ˆç«äº‰çš„è´¨ç–‘ï¼Œè¡¨æ˜ä¸ºäº†ç”Ÿå­˜å¯èƒ½éœ€è¦ç»“ç›Ÿã€‚
OpenAI çš„æœ€æ–°æ›´æ–°ï¼šä¸€ä½ç”¨æˆ·å¼ºè°ƒäº† OpenAI å…³äºä¸ºå„ç§ç”¨æˆ·å±‚çº§æ›´æ–°æ€ç»´é“¾ï¼ˆchain of thoughtï¼‰åŠŸèƒ½çš„å…¬å‘Šï¼Œå¹¶åœ¨æ­¤å¤„é“¾æ¥åˆ°è¯¥æ›´æ–°ã€‚
å¯¹è¯¥æ›´æ–°çš„å›åº”åŒ…æ‹¬å¯¹å…¶æä¾›ä»»ä½•å®é™…ç›®çš„çš„æ€€ç–‘ï¼Œè¯„è®ºå¦‚ã€Œåªæ˜¯ä¸€äº›æ¯”è¾ƒç¬¼ç»Ÿçš„æ›´æ–°ã€ã€‚
AI çš„ä»·å€¼å’Œç”¨æˆ·å…³æ³¨ï¼šä¸€ä½ç”¨æˆ·è®¤ä¸ºï¼Œå¤§å¤šæ•°ç«äº‰è€…æœªèƒ½å…³æ³¨æœ€ç»ˆç”¨æˆ·çš„éœ€æ±‚ï¼Œå¹¶å¼ºè°ƒã€Œæ²¡æœ‰ç«äº‰è€…çœŸæ­£ç›´æ¥å…³æ³¨ç”¨æˆ·ã€ã€‚
ä»–ä»¬æå‡ºï¼Œå¼ºè°ƒç¤¾ä¼šä»·å€¼å¯èƒ½æ¯”å½“å‰çš„ä¼ä¸šæ¨¡å¼äº§ç”Ÿæ›´å¤§çš„åˆ©ç›Šã€‚
è”é‚¦é›†ä½“è®­ç»ƒæ–¹æ³•ï¼šæˆå‘˜è®¨è®ºäº† AI é¢†åŸŸæ½œåœ¨çš„å‰è¿›é“è·¯ï¼ŒåŒ…æ‹¬è”é‚¦é›†ä½“è®­ç»ƒæ¨¡å‹ï¼ˆfederated collective ways to train modelsï¼‰å’ŒæŒ‘æˆ˜ç°æœ‰å„æ–­çš„æƒ³æ³•ã€‚
å¯¹äºæ˜¯å¦å¯ä»¥é€šè¿‡å“è¶Šçš„æ™ºèƒ½æˆ–é€šè¿‡åˆä½œæ–¹æ³•è¶…è¶Šå¤§å‹å®ä½“ï¼Œæ„è§ä¸ä¸€ï¼Œæ¶‰åŠå£°ç§°ç”Ÿäº§çš„é©¬å…‹æ€ä¸»ä¹‰è§‚ç‚¹ã€‚
æåŠçš„é“¾æ¥ï¼š
æ¥è‡ª OpenAIï¼ˆ@OpenAIï¼‰çš„æ¨æ–‡ï¼šä¸ºå…è´¹å’Œä»˜è´¹ç”¨æˆ·æ›´æ–°äº† OpenAI o3-mini ä¸­çš„æ€ç»´é“¾ï¼Œå¹¶ä¸ºä»˜è´¹ç”¨æˆ·æ›´æ–°äº† o3-mini-high ä¸­çš„æ€ç»´é“¾ã€‚
GitHub - SakanaAI/evolutionary-model-merge:ã€Œæ¨¡å‹åˆå¹¶é…æ–¹è¿›åŒ–ä¼˜åŒ–ã€çš„å®˜æ–¹å­˜å‚¨åº“ï¼šæ¨¡å‹åˆå¹¶é…æ–¹è¿›åŒ–ä¼˜åŒ–çš„å®˜æ–¹å­˜å‚¨åº“ - SakanaAI/evolutionary-model-merge
Unsloth AIï¼ˆDaniel Hanï¼‰â–· #helpï¼ˆ188 æ¡æ¶ˆæ¯ğŸ”¥ğŸ”¥):
Unsloth æ¨¡å‹è®­ç»ƒï¼ŒGRPO å’Œå¥–åŠ±å‡½æ•°ï¼Œæ¨¡å‹åˆå¹¶é—®é¢˜ï¼Œä½¿ç”¨ LoRA ç»§ç»­é¢„è®­ç»ƒï¼Œé€‚é…å™¨æ€§èƒ½æ¯”è¾ƒ

Unsloth model training strategies discussed: Users shared their experiences with unsloth, emphasizing the impact of pretraining and instruction data on model performance.
One user noted that their model performed better before additional finetuning, highlighting the challenges with further training.
GRPO and reward functions effectiveness: There was a discussion on teaching models to follow specific formats using GRPO, with a focus on reward functions for guiding output.
Participants suggested that combining a reward function with supervised fine-tuning (SFT) could enhance the training outcome.
Challenges with model merging: A user expressed concerns about merging LoRA adapters with base models, revealing that post-merging training led to poor outputs.
It was suggested to keep using LoRA adapters for training instead of merging until fully confident in model performance.
Issues with continued pretraining using adapters: Participants encountered issues where the adapter failed to continue training the lm_head and embed tokens during resumed training.
This raised questions about whether merging would solve these issues or if training could proceed effectively with just the adapter.
Optimizations and praise for Unsloth: Users expressed admiration for the optimizations in the Unsloth framework, mentioning the efficiency improvements.
The release of RL features was highlighted as a significant enhancement that many users were looking forward to.
Links mentioned:
Run DeepSeek-R1 Dynamic 1.58-bit: DeepSeek R-1 is the most powerful open-source reasoning model that performs on par with OpenAI's o1 model.Run the 1.58-bit Dynamic GGUF version by Unsloth.
bartowski/Qwen2.5-Math-1.5B-Instruct-GGUF Â· Hugging Face: no description found
Hugging Face: The AI community building the future. Hugging Face has 287 repositories available. Follow their code on GitHub.
Unsloth Documentation: no description found
Unsloth Notebooks | Unsloth Documentation: Below is a list of all our notebooks:
GitHub - unslothai/unsloth: Finetune Llama 3.3, DeepSeek-R1, Mistral, Phi-4 & Gemma 2 LLMs 2-5x faster with 70% less memory: Finetune Llama 3.3, DeepSeek-R1, Mistral, Phi-4 & Gemma 2 LLMs 2-5x faster with 70% less memory - unslothai/unsloth
Tutorial: How to Finetune Llama-3 and Use In Ollama | Unsloth Documentation: Beginner's Guide for creating a customized personal assistant (like ChatGPT) to run locally on Ollama
Google Colab: no description found
Unsloth AI (Daniel Han) â–· #showcase (1 messages):
yaska0971: Name strings tooooooooooo long. Please shorten it

å…³äº Unsloth æ¨¡å‹è®­ç»ƒç­–ç•¥çš„è®¨è®ºï¼šç”¨æˆ·åˆ†äº«äº†ä»–ä»¬ä½¿ç”¨ Unsloth çš„ç»éªŒï¼Œå¼ºè°ƒäº†é¢„è®­ç»ƒå’ŒæŒ‡ä»¤æ•°æ®å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚
ä¸€ä½ç”¨æˆ·æŒ‡å‡ºï¼Œä»–ä»¬çš„æ¨¡å‹åœ¨è¿›è¡Œé¢å¤–çš„å¾®è°ƒä¹‹å‰è¡¨ç°æ›´å¥½ï¼Œè¿™çªæ˜¾äº†è¿›ä¸€æ­¥è®­ç»ƒæ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚
GRPO å’Œå¥–åŠ±å‡½æ•°çš„æœ‰æ•ˆæ€§ï¼šè®¨è®ºé›†ä¸­åœ¨ä½¿ç”¨ GRPO æ•™å¯¼æ¨¡å‹éµå¾ªç‰¹å®šæ ¼å¼ï¼Œä»¥åŠä½¿ç”¨å¥–åŠ±å‡½æ•°æ¥æŒ‡å¯¼è¾“å‡ºã€‚
å‚ä¸è€…è®¤ä¸ºï¼Œå°†å¥–åŠ±å‡½æ•°ä¸ç›‘ç£å¼å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼ŒSFTï¼‰ç›¸ç»“åˆå¯ä»¥å¢å¼ºè®­ç»ƒæ•ˆæœã€‚
æ¨¡å‹åˆå¹¶çš„æŒ‘æˆ˜ï¼šä¸€ä½ç”¨æˆ·è¡¨è¾¾äº†å¯¹å°† LoRA é€‚é…å™¨ä¸åŸºç¡€æ¨¡å‹åˆå¹¶çš„æ‹…å¿§ï¼Œè¡¨æ˜åˆå¹¶åçš„è®­ç»ƒå¯¼è‡´äº†è¾ƒå·®çš„è¾“å‡ºã€‚
å»ºè®®åœ¨å¯¹æ¨¡å‹æ€§èƒ½æœ‰å……åˆ†æŠŠæ¡ä¹‹å‰ï¼Œç»§ç»­ä½¿ç”¨ LoRA é€‚é…å™¨è¿›è¡Œè®­ç»ƒï¼Œè€Œä¸æ˜¯è¿›è¡Œåˆå¹¶ã€‚
ä½¿ç”¨é€‚é…å™¨ç»§ç»­é¢„è®­ç»ƒçš„é—®é¢˜ï¼šå‚ä¸è€…é‡åˆ°äº†é€‚é…å™¨åœ¨æ¢å¤è®­ç»ƒæœŸé—´æ— æ³•ç»§ç»­è®­ç»ƒ `lm_head`ï¼ˆè¯­è¨€æ¨¡å‹å¤´éƒ¨ï¼Œç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ª Tokenï¼‰å’ŒåµŒå…¥ Token çš„é—®é¢˜ã€‚
è¿™å¼•å‘äº†å…³äºåˆå¹¶æ˜¯å¦èƒ½è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ–è€…æ˜¯å¦å¯ä»¥ä»…ä½¿ç”¨é€‚é…å™¨æœ‰æ•ˆåœ°è¿›è¡Œè®­ç»ƒçš„é—®é¢˜ã€‚
å¯¹ Unsloth çš„ä¼˜åŒ–å’Œèµæ‰¬ï¼šç”¨æˆ·è¡¨è¾¾äº†å¯¹ Unsloth æ¡†æ¶ä¸­ä¼˜åŒ–çš„èµèµï¼Œæåˆ°äº†æ•ˆç‡çš„æé«˜ã€‚
RL åŠŸèƒ½çš„å‘å¸ƒè¢«è®¤ä¸ºæ˜¯ä¸€é¡¹é‡è¦çš„æ”¹è¿›ï¼Œå—åˆ°äº†è®¸å¤šç”¨æˆ·çš„æœŸå¾…ã€‚
æåˆ°çš„é“¾æ¥ï¼š
Run DeepSeek-R1 Dynamic 1.58-bitï¼šDeepSeek R-1 æ¨¡å‹æ˜¯æœ€å¼ºå¤§çš„å¼€æºæ¨ç†æ¨¡å‹ï¼Œå…¶æ€§èƒ½ä¸ OpenAI çš„ o1 æ¨¡å‹ç›¸å½“ã€‚è¿è¡Œ Unsloth æä¾›çš„ 1.58-bit åŠ¨æ€ GGUF ç‰ˆæœ¬ã€‚
bartowski/Qwen2.5-Math-1.5B-Instruct-GGUFÂ·Hugging Faceï¼šæœªæ‰¾åˆ°æè¿°
Hugging Faceï¼šæ„å»ºæœªæ¥çš„ AI ç¤¾åŒºã€‚Hugging Face æ‹¥æœ‰ 287 ä¸ªå¯ç”¨çš„ä»£ç ä»“åº“ã€‚åœ¨ GitHub ä¸Šå…³æ³¨ä»–ä»¬çš„ä»£ç ã€‚
Unsloth Documentationï¼šæœªæ‰¾åˆ°æè¿°
Unsloth Notebooks | Unsloth Documentationï¼šä»¥ä¸‹æ˜¯æˆ‘ä»¬æ‰€æœ‰ Notebook çš„åˆ—è¡¨ï¼š
GitHub - unslothai/unslothï¼šä½¿ç”¨ 70% æ›´å°‘çš„å†…å­˜ï¼Œä»¥ 2-5 å€çš„é€Ÿåº¦å¾®è°ƒ Llama 3.3ã€DeepSeek-R1ã€Mistralã€Phi-4 & Gemma 2 LLMsï¼š ä½¿ç”¨ 70% æ›´å°‘çš„å†…å­˜ï¼Œä»¥ 2-5 å€çš„é€Ÿåº¦å¾®è°ƒ Llama 3.3ã€DeepSeek-R1ã€Mistralã€Phi-4 & Gemma 2 LLMs - unslothai/unsloth
Tutorialï¼šHow to Finetune Llama-3 and Use In Ollama | Unsloth Documentationï¼šåˆ›å»ºè‡ªå®šä¹‰ä¸ªäººåŠ©ç†ï¼ˆå¦‚ ChatGPTï¼‰ä»¥åœ¨ Ollama ä¸Šæœ¬åœ°è¿è¡Œçš„åˆå­¦è€…æŒ‡å—
Google Colabï¼šæœªæ‰¾åˆ°æè¿°
Unsloth AIï¼ˆDaniel Hanï¼‰â–· #showcaseï¼ˆ1 messages):
yaska0971ï¼šåç§°å­—ç¬¦ä¸²å¤ªé•¿ï¼Œè¯·ç¼©çŸ­

Unsloth AI (Daniel Han) â–· #research (6 messages):
Realistic AI Research Domains, TPU Research Cloud with JAX, OpenMoE Project, Pretraining Small Transformers

Exploring Realistic AI Research Avenues: An independent researcher seeks realistic domains to start AI/ML research without significant funding, citing limitations on pretraining large models.
Suggestions for areas of focus include comparative studies, creating new data generation models, or improving prompts for LLMs.
Value of Learning JAX for TPU Access: A member recommends learning JAX for access to the TPU Research Cloud, suggesting it could be quite beneficial.
They provided a link to the TPU Research Cloud application form.
OpenMoE: An Example from the Community: A member highlighted the OpenMoE GitHub repository as a relevant example of someone conducting research in the field of Mixture-of-Experts models.
The repository is led by a researcher who has progressed to working at DeepMind, demonstrating success in this area.
Pretraining Small Transformers on TinyStories: Another member suggested pretraining small transformers on the TinyStories dataset as a potential research option.
This could offer new pathways for independent projects without needing extensive resources.
Links mentioned:
Paper page - Exponentially Faster Language Modelling: no description found
GitHub - XueFuzhao/OpenMoE: A family of open-sourced Mixture-of-Experts (MoE) Large Language Models: A family of open-sourced Mixture-of-Experts (MoE) Large Language Models - XueFuzhao/OpenMoE
Accelerate your research with the TPU Research Cloud: Please complete the following questions for your application to be considered.
Stability.ai (Stable Diffusion) â–· #announcements (1 messages):
Maxfield Introduction, Community Engagement Initiatives, Feature Request Board, Showcasing Researcher Progress

Unsloth AIï¼ˆDaniel Hanï¼‰â–· #researchï¼ˆ6 æ¡æ¶ˆæ¯):
ç°å®çš„ AI ç ”ç©¶é¢†åŸŸï¼Œä½¿ç”¨ JAX çš„ TPU ç ”ç©¶äº‘å¹³å°ï¼ŒOpenMoE é¡¹ç›®ï¼Œé¢„è®­ç»ƒå°å‹ Transformers

æ¢ç´¢ç°å®çš„ AI ç ”ç©¶é€”å¾„ï¼šä¸€ä½ç‹¬ç«‹ç ”ç©¶å‘˜æ­£åœ¨å¯»æ‰¾å®é™…å¯è¡Œçš„ç ”ç©¶æ–¹å‘ï¼Œä»¥ä¾¿åœ¨èµ„é‡‘æœ‰é™çš„æƒ…å†µä¸‹å¼€å±• AI/ML ç ”ç©¶ã€‚ä»–ä»¬è®¤ä¸ºï¼Œé¢„è®­ç»ƒå¤§å‹æ¨¡å‹å­˜åœ¨è¯¸å¤šé™åˆ¶ï¼Œå› æ­¤éœ€è¦å¦è¾Ÿè¹Šå¾„ã€‚
å»ºè®®çš„ç ”ç©¶æ–¹å‘åŒ…æ‹¬ï¼šå¯¹æ¯”ç ”ç©¶ã€åˆ›å»ºæ–°çš„æ•°æ®ç”Ÿæˆæ¨¡å‹ï¼Œæˆ–è€…æ”¹è¿›å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼ŒLLMï¼‰çš„æç¤ºï¼ˆPromptï¼‰ã€‚
å­¦ä¹  JAX ä»¥ä¾¿è®¿é—® TPU çš„ä»·å€¼ï¼šä¸€ä½æˆå‘˜å»ºè®®å­¦ä¹  JAXï¼Œä»¥ä¾¿èƒ½å¤Ÿè®¿é—® TPU ç ”ç©¶äº‘å¹³å°ï¼Œå¹¶è®¤ä¸ºè¿™å°†å¸¦æ¥å¾ˆå¤§çš„ç›Šå¤„ã€‚
ä»–ä»¬è¿˜æä¾›äº† TPU ç ”ç©¶äº‘å¹³å°ç”³è¯·è¡¨çš„é“¾æ¥ã€‚
OpenMoEï¼šæ¥è‡ªç¤¾åŒºçš„ç¤ºä¾‹ï¼šä¸€ä½æˆå‘˜æ¨èäº† OpenMoE GitHub ä»“åº“ï¼Œè®¤ä¸ºå®ƒå±•ç¤ºäº†ä¸€ä¸ªåœ¨æ··åˆä¸“å®¶ï¼ˆMixture-of-Expertsï¼ŒMoEï¼‰æ¨¡å‹é¢†åŸŸè¿›è¡Œç ”ç©¶çš„ä¼˜ç§€æ¡ˆä¾‹ã€‚
è¯¥ä»“åº“ç”±ä¸€ä½å·²ç»è¿›å…¥ DeepMind å·¥ä½œçš„ç ”ç©¶å‘˜ä¸»å¯¼ï¼Œè¿™å……åˆ†è¯æ˜äº†åœ¨è¯¥é¢†åŸŸå–å¾—æˆåŠŸçš„å¯èƒ½æ€§ã€‚
åœ¨ TinyStories ä¸Šé¢„è®­ç»ƒå°å‹ Transformersï¼ˆTransformer)ï¼šå¦ä¸€ä½æˆå‘˜å»ºè®®ï¼Œå¯ä»¥åœ¨ TinyStories æ•°æ®é›†ä¸Šé¢„è®­ç»ƒå°å‹ Transformerï¼Œä½œä¸ºä¸€ç§æ½œåœ¨çš„ç ”ç©¶æ–¹å‘ã€‚TinyStories æ˜¯ä¸€ä¸ªåŒ…å«å¤§é‡æçŸ­ç¯‡æ•…äº‹çš„æ•°æ®é›†ï¼Œéå¸¸é€‚åˆç”¨äºè®­ç»ƒå°å‹è¯­è¨€æ¨¡å‹ã€‚
è¿™ä¸ºç‹¬ç«‹é¡¹ç›®å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œè€Œä¸”ä¸éœ€è¦å¤§é‡çš„èµ„æºæŠ•å…¥ã€‚
æåˆ°çš„é“¾æ¥ï¼š
è®ºæ–‡é¡µé¢ - æŒ‡æ•°çº§åŠ é€Ÿè¯­è¨€å»ºæ¨¡ï¼šæœªæ‰¾åˆ°æè¿°
GitHub - XueFuzhao/OpenMoEï¼šA family of open-sourced Mixture-of-Expertsï¼ˆMoEï¼‰Large Language Modelsï¼šä¸€ä¸ªå¼€æºçš„æ··åˆä¸“å®¶ï¼ˆMoEï¼‰å¤§è¯­è¨€æ¨¡å‹å®¶æ— - XueFuzhao/OpenMoE
ä½¿ç”¨ TPU ç ”ç©¶äº‘å¹³å°åŠ é€Ÿæ‚¨çš„ç ”ç©¶ï¼šè¯·å®Œæˆä»¥ä¸‹é—®é¢˜ï¼Œä»¥ä¾¿æ‚¨çš„ç”³è¯·è¢«è€ƒè™‘ã€‚
Stability.aiï¼ˆStable Diffusionï¼‰â–· #announcementsï¼ˆ1 æ¡æ¶ˆæ¯):
Maxfield ä»‹ç»ï¼Œç¤¾åŒºå‚ä¸è®¡åˆ’ï¼ŒåŠŸèƒ½è¯·æ±‚æ¿ï¼Œå±•ç¤ºç ”ç©¶äººå‘˜çš„è¿›å±•

Maxfield, the New Community Chief, Introduces Himself: Maxfield, the new Chief Community Guy at Stability, introduces himself and expresses his commitment to improving community engagement after heavy involvement in AI media generation since 2022.
He highlights that he previously contributed at Civitai and acknowledges that community engagement has been lackluster lately.
Two New Initiatives to Boost Engagement: Maxfield announces two initiatives aimed at improving communication and sharing community interests with features like a feature request board for suggestions on models and tools.
He emphasizes that the goal is to ensure that the community's voices are heard and that initiatives will cater to hobbyists and professionals alike.
Encouraging Creators to Share Progress: Maxfield plans to promote transparency by encouraging Stability's researchers and creators to share updates on their projects and developments.
He believes that the fantastic work being done shouldn't be kept a secret and should be shared more widely with the community.
Stability.ai (Stable Diffusion) â–· #general-chat (459 messagesğŸ”¥ğŸ”¥ğŸ”¥):
Stability AI Updates, Model Compatibility, AI Prompting Techniques, Community Dynamics, AI Subscriptions and Costs

Maxfieldï¼Œæ–°ä»»ç¤¾åŒºä¸»ç®¡ï¼Œè‡ªæˆ‘ä»‹ç»ï¼šStability çš„æ–°ä»»ç¤¾åŒºè´Ÿè´£äºº Maxfield è‡ªæˆ‘ä»‹ç»ï¼Œå¹¶è¡¨ç¤ºè‡ª 2022 å¹´ä»¥æ¥æ·±åº¦å‚ä¸ AI ç”Ÿæˆåª’ä½“åï¼Œä»–è‡´åŠ›äºæ”¹å–„ç¤¾åŒºçš„å‚ä¸åº¦ã€‚
ä»–å¼ºè°ƒï¼Œä»–ä¹‹å‰æ›¾åœ¨ Civitai åšå‡ºè¿‡è´¡çŒ®ï¼Œå¹¶æ‰¿è®¤æœ€è¿‘ç¤¾åŒºçš„å‚ä¸åº¦ä¸€ç›´ä¸å°½å¦‚äººæ„ã€‚
ä¸¤é¡¹æ—¨åœ¨æé«˜å‚ä¸åº¦çš„æ–°ä¸¾æªï¼šMaxfield å®£å¸ƒäº†ä¸¤é¡¹æ—¨åœ¨æ”¹å–„æ²Ÿé€šå’Œåˆ†äº«ç¤¾åŒºå…´è¶£çš„ä¸¾æªï¼Œå…¶ä¸­åŒ…æ‹¬ä¸€ä¸ªåŠŸèƒ½å»ºè®®æ¿ï¼Œç”¨äºæ¥æ”¶å…³äºæ¨¡å‹å’Œå·¥å…·çš„å»ºè®®ã€‚
ä»–å¼ºè°ƒï¼Œç›®æ ‡æ˜¯ç¡®ä¿ç¤¾åŒºçš„å£°éŸ³è¢«å¬åˆ°ï¼Œå¹¶ä¸”è¿™äº›ä¸¾æªå°†åŒæ—¶æ»¡è¶³ä¸šä½™çˆ±å¥½è€…å’Œä¸“ä¸šäººå£«çš„éœ€æ±‚ã€‚
é¼“åŠ±åˆ›ä½œè€…åˆ†äº«è¿›å±•ï¼šMaxfield è®¡åˆ’é€šè¿‡é¼“åŠ± Stability çš„ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…åˆ†äº«å…¶é¡¹ç›®å’Œå¼€å‘çš„æœ€æ–°è¿›å±•æ¥æé«˜é€æ˜åº¦ã€‚
ä»–è®¤ä¸ºï¼Œæ­£åœ¨åšçš„å‡ºè‰²å·¥ä½œä¸åº”è¯¥ä¿å¯†ï¼Œè€Œåº”è¯¥ä¸ç¤¾åŒºæ›´å¹¿æ³›åœ°åˆ†äº«ã€‚
Stability.aiï¼ˆStable Diffusionï¼‰â–· #general-chatï¼ˆ459 messagesğŸ”¥ğŸ”¥ğŸ”¥)ï¼š
Stability AI æ›´æ–°ã€æ¨¡å‹å…¼å®¹æ€§ã€AI æç¤ºï¼ˆPromptingï¼‰æŠ€æœ¯ã€ç¤¾åŒºåŠ¨æ€ã€AI è®¢é˜…å’Œæˆæœ¬ï¼ˆè¿™äº›æ˜¯ Stability.ai çš„ Stable Diffusion é¡¹ç›®åœ¨ #general-chat é¢‘é“ä¸­è®¨è®ºçš„çƒ­é—¨è¯é¢˜ï¼‰ã€‚

Discussion on Stability AI features and subscription costs: Members discussed the 'private images' option in the Max subscription, questioning if it implied NSFW content, with some sharing their experiences with the service.
Others highlighted the costs involved in using cloud services for model training, comparing them to local electricity costs.
Issues with downloading from Civitai: Several users reported encountering Error 1101 when trying to download models from Civitai, indicating possible server issues.
The community shared their frustrations over the downtime and difficulties accessing models.
Latent space tools and model training: A user expressed confusion over the complexity of tools for swapping latent space parameters, indicating a need for more intuitive solutions.
Discussions included potential implementations for newer diffusion models and challenges with running existing architectures.
General attitudes towards different AI models: Participants shared their experiences and opinions regarding various AI models, including Stability AI and Midjourney, reflecting mixed feelings about subscription models and community dynamics.
There was a contemplation of the value of entry costs against the utility of using specific AI models.
AI prompting techniques and tools: A user sought insights into prompting techniques for generative models, while others suggested using external tools to generate prompts.
Discussion included the difficulty of adjusting to different AI model requirements and generating satisfactory prompts.
Links mentioned:
brxce/stable-diffusion-prompt-generator: Stable Diffusion Prompt Generator
Creep Hands GIF - Creep Hands Adventure Time - Discover & Share GIFs: Click to view the GIF
Multimodal AI Agents - Hackathon Â· Luma: Gen AI AgentsCreatorsCorner, collaborating with Google Deepmind, Weights & Biases, Together.ai, Stytch, Senso, LlamaIndex and others enthusiasticallyâ€¦
GitHub - NeuralNotW0rk/LoRAW: Flexible LoRA Implementation to use with stable-audio-tools: Flexible LoRA Implementation to use with stable-audio-tools - NeuralNotW0rk/LoRAW
Expanding the frontiers of AI creativity - PurpleSmartAI: no description found
Codeium (Windsurf) â–· #announcements (3 messages):
Gemini 2.0 Flash, Windsurf Next Beta, Windsurf 1.2.6 Patch Fixes, Cascade Web Search

å…³äº Stability AI çš„åŠŸèƒ½å’Œè®¢é˜…è´¹ç”¨ï¼šæœ‰æˆå‘˜æé—®ï¼ŒMax è®¢é˜…ä¸­çš„ã€Œç§æœ‰å›¾åƒã€é€‰é¡¹æ˜¯å¦æ„å‘³ç€æ¶‰åŠä¸é€‚å®œå·¥ä½œåœºæ‰€è§‚çœ‹çš„å†…å®¹ï¼ˆNSFWï¼‰ï¼Œå¹¶åˆ†äº«äº†å„è‡ªçš„ä½¿ç”¨ä½“éªŒã€‚
ä¹Ÿæœ‰äººå¯¹æ¯”äº†äº‘æœåŠ¡å’Œæœ¬åœ°ç”µåŠ›åœ¨æ¨¡å‹è®­ç»ƒä¸Šçš„æˆæœ¬ã€‚
Civitai ä¸‹è½½é—®é¢˜ï¼šå¤šä½ç”¨æˆ·æŠ¥å‘Šç§°ï¼Œä» Civitai ä¸‹è½½æ¨¡å‹æ—¶é‡åˆ° Error 1101 é”™è¯¯ï¼Œè¿™å¯èƒ½è¡¨æ˜æœåŠ¡å™¨å­˜åœ¨é—®é¢˜ã€‚
ç¤¾ç¾¤æˆå‘˜æ™®éå¯¹æœåŠ¡ä¸­æ–­å’Œæ¨¡å‹è®¿é—®å›°éš¾è¡¨ç¤ºä¸æ»¡ã€‚
æ½œåœ¨ç©ºé—´å·¥å…·ä¸æ¨¡å‹è®­ç»ƒï¼šæœ‰ç”¨æˆ·åæ˜ ï¼Œç”¨äºäº¤æ¢æ½œåœ¨ç©ºé—´å‚æ•°çš„å·¥å…·è¿‡äºå¤æ‚ï¼Œå¸Œæœ›èƒ½æœ‰æ›´ç›´è§‚çš„è§£å†³æ–¹æ¡ˆã€‚
è®¨è®ºå†…å®¹è¿˜åŒ…æ‹¬æ–°å‹æ‰©æ•£æ¨¡å‹çš„æ½œåœ¨åº”ç”¨ï¼Œä»¥åŠç°æœ‰æ¶æ„æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚
å¯¹ä¸åŒ AI æ¨¡å‹çš„çœ‹æ³•ï¼šå‚ä¸è€…åˆ†äº«äº†ä»–ä»¬å¯¹åŒ…æ‹¬ Stability AI å’Œ Midjourney åœ¨å†…çš„å„ç§ AI æ¨¡å‹çš„ç»éªŒå’Œçœ‹æ³•ï¼Œè¡¨è¾¾äº†å¯¹è®¢é˜…æ¨¡å¼å’Œç¤¾ç¾¤ç”Ÿæ€çš„å¤æ‚æƒ…æ„Ÿã€‚
å¤§å®¶ä¹Ÿåœ¨æ€è€ƒï¼Œä½¿ç”¨ç‰¹å®š AI æ¨¡å‹çš„å®é™…ä»·å€¼æ˜¯å¦ä¸å‰æœŸæŠ•å…¥çš„æˆæœ¬ç›¸ç¬¦ã€‚
AI æç¤ºè¯æŠ€å·§ä¸å·¥å…·ï¼šæœ‰ç”¨æˆ·å¸Œæœ›äº†è§£ç”Ÿæˆå¼æ¨¡å‹ï¼ˆGenerative AIï¼‰çš„æç¤ºè¯ï¼ˆPromptï¼‰æŠ€å·§ï¼Œå¦ä¸€äº›äººåˆ™å»ºè®®ä½¿ç”¨å¤–éƒ¨å·¥å…·æ¥è¾…åŠ©ç”Ÿæˆã€‚
è®¨è®ºä¸­æåˆ°äº†ï¼Œé’ˆå¯¹ä¸åŒ AI æ¨¡å‹è°ƒæ•´æç¤ºè¯ï¼Œä»¥åŠç”Ÿæˆé«˜è´¨é‡æç¤ºè¯çš„éš¾åº¦ã€‚
ç›¸å…³é“¾æ¥ï¼š
brxce/stable-diffusion-prompt-generatorï¼šStable Diffusion Prompt Generator
Creep Hands GIF - Creep Hands Adventure Time - Discover & Share GIFsï¼šç‚¹å‡»æŸ¥çœ‹ GIF
Multimodal AI Agents - HackathonÂ·Lumaï¼šGen AI AgentsCreatorsCornerï¼Œä¸ Google Deepmindï¼ŒWeights & Biasesï¼ŒTogether.aiï¼ŒStytchï¼ŒSensoï¼ŒLlamaIndex ç­‰æœºæ„åˆä½œä¸¾åŠçš„é»‘å®¢æ¾æ´»åŠ¨ã€‚
GitHub - NeuralNotW0rk/LoRAWï¼šFlexible LoRA Implementation to use with stable-audio-toolsï¼šNeuralNotW0rk/LoRAW
Expanding the frontiers of AI creativity - PurpleSmartAIï¼šæ— æè¿°ä¿¡æ¯
Codeiumï¼ˆWindsurfï¼‰â–· #announcementsï¼ˆ3 messages):
Gemini 2.0 Flashï¼ŒWindsurf Next Betaï¼ŒWindsurf 1.2.6 Patch Fixesï¼ŒCascade Web Search

Gemini 2.0 Flash speeds up coding: The new Gemini 2.0 Flash is now available on Windsurf, consuming only 0.25 user prompt credits per message and flow action credits per tool call.
Blazingly fast and efficient, it is limited in tool calling ability but excels at answering codebase-related questions.
Join Windsurf Next for early features: Users can gain early access to the latest features of Windsurf Next by downloading the beta from this link.
The beta enables exploration of new AI capabilities while allowing users to switch between Next and Stable versions as needed.
Windsurf 1.2.6 Patch addresses credit issues: The latest Windsurf 1.2.6 Patch fixes problems with partial credits during the transition to flex credits, detailed in the full changelog.
This patch enhances user experience by ensuring smoother credit transitions for actions.
Cascade's new web search capabilities: Cascade can now perform web searches automatically or via user commands like @web and @docs, making it versatile for obtaining real-time information.
This functionality includes URL input support and incorporates web context for improved responses, with a web search costing 1 flow action credit.
Links mentioned:
Tweet from Windsurf (@windsurf_ai): Gemini 2.0 Flash is now available in Windsurf!From our testing, Flash is:âš¡ blazingly fastğŸ’ª efficient - only consumes 0.25X creditsğŸ§  limited in its tool calling ability, but great for questions about...
Thank you for downloading Windsurf Editor: Tomorrow's editor, today. Windsurf Editor is the first AI agent-powered IDE that keeps developers in the flow. Available today on Mac, Windows, and Linux.
Windsurf Next Launch: Introducing Windsurf Next, our opt-in prerelease version of Windsurf.
Windsurf Editor Changelogs | Windsurf Editor and Codeium extensions: Latest updates and changes for the Windsurf Editor.
Codeium (Windsurf) â–· #discussion (31 messagesğŸ”¥):
Codeium Jetbrains Plugin Issues, DeepSeek Feature Request, Function Length Display in CodeLens, Educational Email Discounts, Version Updates and Bug Reports

Gemini 2.0 Flash åŠ é€Ÿç¼–ç ï¼š æ–°çš„ Gemini 2.0 Flash ç°å·²åœ¨ Windsurf ä¸Šæ¨å‡ºï¼Œæ¯ä¸ªæ¶ˆæ¯ä»…æ¶ˆè€— 0.25 ä¸ªç”¨æˆ·æç¤ºç§¯åˆ†ï¼ˆCreditsï¼‰ï¼Œæ¯æ¬¡å·¥å…·è°ƒç”¨æ¶ˆè€—æµç¨‹æ“ä½œç§¯åˆ†ï¼ˆCreditsï¼‰ã€‚
é€Ÿåº¦æå¿«ä¸”é«˜æ•ˆï¼Œå®ƒçš„å·¥å…·è°ƒç”¨èƒ½åŠ›æœ‰é™ï¼Œä½†æ“…é•¿å›ç­”ä¸ä»£ç åº“ç›¸å…³çš„é—®é¢˜ã€‚
åŠ å…¥ Windsurf Next ä»¥è·å–æ—©æœŸåŠŸèƒ½ï¼š ç”¨æˆ·å¯ä»¥é€šè¿‡æ­¤é“¾æ¥ä¸‹è½½ Beta ç‰ˆæ¥æå‰è®¿é—® Windsurf Next çš„æœ€æ–°åŠŸèƒ½ã€‚
Beta ç‰ˆæœ¬æˆ·æˆ·æˆ·æˆ·æˆ·ä½¿ç”¨æˆ·å¯ä»¥æ¢ç´¢æ–°çš„ AI åŠŸèƒ½ï¼ŒåŒæ—¶å…è®¸ç”¨æˆ·æ ¹æ®éœ€è¦åœ¨ Next å’Œç¨³å®šç‰ˆæœ¬ä¹‹é—´åˆ‡æ¢ã€‚
Windsurf 1.2.6 è¡¥ä¸è§£å†³äº†ç§¯åˆ†é—®é¢˜ï¼š æœ€æ–°çš„ Windsurf 1.2.6 è¡¥ä¸ä¿®å¤äº†åœ¨è¿‡æ¸¡åˆ°çµæ´»ç§¯åˆ†ï¼ˆCreditsï¼‰æœŸé—´çš„éƒ¨åˆ†ç§¯åˆ†ï¼ˆCreditsï¼‰é—®é¢˜ï¼Œè¯¦ç»†ä¿¡æ¯è¯·å‚è§å®Œæ•´çš„æ›´æ–°æ—¥å¿—ã€‚
æ­¤è¡¥ä¸é€šè¿‡ç¡®ä¿æ“ä½œçš„ç§¯åˆ†ï¼ˆCreditsï¼‰è½¬æ¢æ›´åŠ æµç•…ï¼Œä»è€Œå¢å¼ºç”¨æˆ·ä½“éªŒã€‚
Cascade çš„æ–° Web æœç´¢åŠŸèƒ½ï¼š Cascade ç°åœ¨å¯ä»¥è‡ªåŠ¨æˆ–é€šè¿‡ç”¨æˆ·å‘½ä»¤ï¼ˆä¾‹å¦‚ @web å’Œ @docsï¼‰æ‰§è¡Œ Web æœç´¢ï¼Œä½¿å…¶èƒ½å¤Ÿçµæ´»åœ°è·å–å®æ—¶ä¿¡æ¯ã€‚
æ­¤åŠŸèƒ½åŒ…æ‹¬ URL è¾“å…¥æ”¯æŒï¼Œå¹¶ç»“åˆäº† Web ä¸Šä¸‹æ–‡ä»¥æ”¹è¿›å“åº”ï¼Œæ¯æ¬¡ Web æœç´¢æ¶ˆè€— 1 ä¸ªæµç¨‹æ“ä½œç§¯åˆ†ï¼ˆCreditsï¼‰ã€‚
æåˆ°çš„é“¾æ¥ï¼š
æ¥è‡ª Windsurfï¼ˆ@windsurf_aiï¼‰çš„æ¨æ–‡ï¼š Gemini 2.0 Flash ç°å·²åœ¨ Windsurf ä¸­æä¾›ï¼æ ¹æ®æˆ‘ä»¬çš„æµ‹è¯•ï¼ŒFlash æ˜¯ï¼šâš¡ é€Ÿåº¦æå¿«ğŸ’ª é«˜æ•ˆ - ä»…æ¶ˆè€— 0.25X ç§¯åˆ†ï¼ˆCreditsï¼‰ğŸ§  å·¥å…·è°ƒç”¨èƒ½åŠ›æœ‰é™ï¼Œä½†éå¸¸é€‚åˆå›ç­”æœ‰å…³... çš„é—®é¢˜æ„Ÿè°¢æ‚¨ä¸‹è½½ Windsurf Editorï¼š æ˜æ—¥çš„ç¼–è¾‘å™¨ï¼Œä»Šæ—¥å¯ç”¨ã€‚Windsurf Editor æ˜¯ç¬¬ä¸€ä¸ªç”± AI æ™ºèƒ½ä½“ï¼ˆAI Agentï¼‰é©±åŠ¨çš„ IDEï¼Œå¯å¸®åŠ©å¼€å‘äººå‘˜ä¿æŒå·¥ä½œæ•ˆç‡ã€‚ä»Šå¤©åœ¨ Macã€Windows å’Œ Linux ä¸Šæä¾›ã€‚
Windsurf Next å¯åŠ¨ï¼š æ¨å‡º Windsurf Nextï¼Œè¿™æ˜¯æˆ‘ä»¬çš„ Windsurf çš„å¯é€‰é¢„å‘å¸ƒç‰ˆæœ¬ã€‚
Windsurf Editor æ›´æ–°æ—¥å¿— | Windsurf Editor å’Œ Codeium æ‰©å±•ï¼š Windsurf Editor çš„æœ€æ–°æ›´æ–°å’Œæ›´æ”¹ã€‚
Codeiumï¼ˆWindsurfï¼‰â–· #discussionï¼ˆ31 messagesğŸ”¥)ï¼š
Codeium Jetbrains æ’ä»¶é—®é¢˜ï¼ŒDeepSeek åŠŸèƒ½è¯·æ±‚ï¼ŒCodeLens ä¸­çš„å‡½æ•°é•¿åº¦æ˜¾ç¤ºï¼Œæ•™è‚²ç”µå­é‚®ä»¶æŠ˜æ‰£ï¼Œç‰ˆæœ¬æ›´æ–°å’Œé”™è¯¯æŠ¥å‘Š

Codeium Jetbrains Plugin Faces Criticism: Users expressed frustration with the Codeium Jetbrains plugin, stating that it often fails to respond and requires frequent restarts, with one switching back to Copilot for reliability.
An error related to file access was reported in PhpStorm, indicating ongoing problems with the plugin's performance.
Request for DeepSeek Feature Implementation: A user requested the addition of the DeepSeek feature to Codeium, emphasizing its potential benefits.
Another member encouraged this request, prompting users to officially submit feature requests through designated channels.
Codelens and Function Lengths Inquiry: A member asked if it's possible to show the length of a function, which led to the identification of the feature as Codelens.
However, the specific implementation of adding logic to Codelens within VSCode remains unclear among users.
Clarification on Educational Email Discounts: Discussion arose about educational email eligibility for discounts, with users clarifying that emails must end with .edu to qualify.
Concerns were raised about the detection methods used, with some speculating that eligibility is specific to US educational institutions.
Credits Usage in Codeium Models: A user inquired whether using the Codeium Premier model in VSCode requires credits, to which it was confirmed that chat features do not consume credits.
It was clarified that none of the models in the extension are connected to credits, ensuring users can utilize them freely.
Codeium (Windsurf) â–· #windsurf (345 messagesğŸ”¥ğŸ”¥):
Issues with Windsurf Performance, Gemini Flash vs Sonnet, Usage of Multiple AI Models, Windsurf Installation and Login Problems, User Experience with Cascading Files

Codeium Jetbrains æ’ä»¶é¢ä¸´æ‰¹è¯„ï¼š ç”¨æˆ·å¯¹ Codeium Jetbrains æ’ä»¶è¡¨ç¤ºä¸æ»¡ï¼Œç§°å…¶ç»å¸¸æ— æ³•å“åº”ï¼Œéœ€è¦é¢‘ç¹é‡å¯ï¼Œä¸€ä½ç”¨æˆ·å› å…¶å¯é æ€§é—®é¢˜è€Œåˆ‡æ¢å› Copilotã€‚
åœ¨ PhpStorm ä¸­æŠ¥å‘Šäº†ä¸€ä¸ªä¸æ–‡ä»¶è®¿é—®ç›¸å…³çš„é”™è¯¯ï¼Œè¡¨æ˜è¯¥æ’ä»¶çš„æ€§èƒ½å­˜åœ¨æŒç»­é—®é¢˜ã€‚
è¯·æ±‚å®ç° DeepSeek åŠŸèƒ½ï¼š ä¸€ä½ç”¨æˆ·è¯·æ±‚å‘ Codeium æ·»åŠ  DeepSeek åŠŸèƒ½ï¼Œå¼ºè°ƒäº†å®ƒçš„æ½œåœ¨å¥½å¤„ã€‚
å¦ä¸€ä½æˆå‘˜é¼“åŠ±äº†è¿™ä¸€è¯·æ±‚ï¼Œå»ºè®®ç”¨æˆ·é€šè¿‡æŒ‡å®šæ¸ é“æ­£å¼æäº¤åŠŸèƒ½è¯·æ±‚ã€‚
Codelens å’Œå‡½æ•°é•¿åº¦è¯¢é—®ï¼š ä¸€ä½æˆå‘˜è¯¢é—®æ˜¯å¦å¯ä»¥æ˜¾ç¤ºå‡½æ•°çš„é•¿åº¦ï¼Œè¿™å¯¼è‡´è¯¥åŠŸèƒ½è¢«è¯†åˆ«ä¸º Codelensã€‚
ç„¶è€Œï¼Œåœ¨ VSCode ä¸­å‘ Codelens æ·»åŠ é€»è¾‘çš„å…·ä½“å®ç°å¯¹äºç”¨æˆ·æ¥è¯´ä»ç„¶ä¸æ¸…æ¥šã€‚
å…³äºæ•™è‚²é‚®ç®±æŠ˜æ‰£çš„è¯´æ˜ï¼š å‡ºç°äº†å…³äºæ•™è‚²é‚®ç®±æ˜¯å¦æœ‰èµ„æ ¼è·å¾—æŠ˜æ‰£çš„è®¨è®ºï¼Œç”¨æˆ·æ¾„æ¸…è¯´ï¼Œé‚®ç®±å¿…é¡»ä»¥ .edu ç»“å°¾æ‰æœ‰èµ„æ ¼ã€‚
æœ‰äººå¯¹ä½¿ç”¨çš„æ£€æµ‹æ–¹æ³•æå‡ºäº†è´¨ç–‘ï¼Œä¸€äº›äººæ¨æµ‹èµ„æ ¼ä»…é™äºç¾å›½æ•™è‚²æœºæ„ã€‚
Codeium æ¨¡å‹ä¸­çš„ Credits ä½¿ç”¨æƒ…å†µï¼š ä¸€ä½ç”¨æˆ·è¯¢é—®åœ¨ VSCode ä¸­ä½¿ç”¨ Codeium Premier æ¨¡å‹æ˜¯å¦éœ€è¦ Credits ï¼Œå¯¹æ­¤ï¼Œæœ‰äººç¡®è®¤èŠå¤©åŠŸèƒ½ä¸æ¶ˆè€— Creditsã€‚
æ®æ¾„æ¸…ï¼Œæ‰©å±•ç¨‹åºä¸­çš„ä»»ä½•æ¨¡å‹éƒ½æœªè¿æ¥åˆ° Credits ï¼Œä»è€Œç¡®ä¿ç”¨æˆ·å¯ä»¥è‡ªç”±ä½¿ç”¨å®ƒä»¬ã€‚
Codeiumï¼ˆWindsurfï¼‰â–· #windsurfï¼ˆ345 æ¡æ¶ˆæ¯ğŸ”¥ğŸ”¥)ï¼š
Windsurf æ€§èƒ½é—®é¢˜ã€Gemini Flash ä¸ Sonnetã€å¤šä¸ª AI æ¨¡å‹çš„ä½¿ç”¨ã€Windsurf å®‰è£…å’Œç™»å½•é—®é¢˜ã€ä½¿ç”¨çº§è”æ–‡ä»¶çš„ä½“éªŒ

Issues with Windsurf Performance: Many users reported performance issues with Windsurf, particularly noting problems with calls to models like O3-mini and Gemini Flash that finish prematurely without providing complete suggestions.
One user expressed frustration about the need to continuously prompt the model to 'continue', leading to concerns over wasted credits.
Gemini Flash vs Sonnet: Some users are comparing Gemini Flash and Sonnet, with Gemini Flash noted for its faster speeds and lower costs, but still being behind Sonnet in terms of overall quality.
As per discussions, Claude remains a preferred option for many due to its higher performance metrics on coding challenges.
Usage of Multiple AI Models: There are discussions about which AI models to use depending on the task, with some users advocating for DeepSeek for debugging and Claude for better quality outputs.
Windsurf's agentic capabilities with different models were debated, indicating variability in performance based on the selected model.
Windsurf Installation and Login Problems: A user reported difficulties logging into the Windsurf IDE despite having a Pro subscription, facing issues with trial activation and authentication.
Another instance noted error messages about version mismatches after attempting to reinstall the IDE.
User Experience with Cascading Files: Users expressed the tediousness of manually adding multiple files to the Cascade chat in their Angular projects, seeking better methods for integration.
Suggestions were made for using right-click options to copy file paths and include them more efficiently in discussions.
Links mentioned:
no title found: no description found
Windsurf Next Changelogs | Windsurf Editor and Codeium extensions: Latest updates and changes for the Windsurf Next extension.
Feature Requests | Codeium: Give feedback to the Codeium team so we can make more informed product decisions. Powered by Canny.
Open VSX Registry: no description found
Auto commit message | Feature Requests | Codeium: Generate Commit Messages from Committed File Context
Roll-over of Pro Credits | Feature Requests | Codeium: Unused Premium User Prompt Credits and Premium Flow Action Credits roll-over to the next month
Tweet from Kevin Hou (@kevinhou22): we love docs! ğŸ“– I'm working on improving / adding more @ docs shortcuts to @windsurf_ailmk what you want and I'll add as many as I can... ğŸ§µalso shoutout @mintlify for auto-hosting all docs w...
Reddit - Dive into anything: no description found
aider (Paul Gauthier) â–· #general (337 messagesğŸ”¥ğŸ”¥):
Hiring Update, Aider Error Handling, DeepSeek and Gemini Models, LLM Editing Formats, Pen Testing with LLMs

Windsurf æ€§èƒ½é—®é¢˜ï¼šè®¸å¤šç”¨æˆ·æŠ¥å‘Šäº† Windsurf çš„æ€§èƒ½é—®é¢˜ï¼Œç‰¹åˆ«æŒ‡å‡ºåœ¨è°ƒç”¨ O3-mini å’Œ Gemini Flash ç­‰æ¨¡å‹æ—¶ï¼Œæ¨¡å‹ä¼šè¿‡æ—©ç»“æŸï¼Œæ— æ³•ç»™å‡ºå®Œæ•´çš„å»ºè®®ã€‚
æœ‰ç”¨æˆ·åé¦ˆï¼Œéœ€è¦ä¸æ–­æç¤ºæ¨¡å‹ã€Œç»§ç»­ã€æ‰èƒ½è·å¾—å®Œæ•´ç»“æœï¼Œæ‹…å¿ƒæµªè´¹é¢åº¦ã€‚
Gemini Flash vs Sonnetï¼šä¸€äº›ç”¨æˆ·æ­£åœ¨æ¯”è¾ƒ Gemini Flash å’Œ Sonnetã€‚Gemini Flash çš„ä¼˜åŠ¿åœ¨äºé€Ÿåº¦æ›´å¿«ã€æˆæœ¬æ›´ä½ï¼Œä½†æ•´ä½“è´¨é‡ä»ç„¶ä¸åŠ Sonnetã€‚
è®¨è®ºæ˜¾ç¤ºï¼Œç”±äº Claude åœ¨ä»£ç ç¼–å†™æŒ‘æˆ˜ä¸­è¡¨ç°æ›´ä½³ï¼Œå› æ­¤ä»ç„¶æ˜¯è®¸å¤šç”¨æˆ·çš„é¦–é€‰ã€‚
ä½¿ç”¨å¤šä¸ª AI æ¨¡å‹ï¼šç”¨æˆ·é’ˆå¯¹ä¸åŒä»»åŠ¡åº”è¯¥ä½¿ç”¨å“ªäº› AI æ¨¡å‹å±•å¼€äº†è®¨è®ºã€‚ä¸€äº›ç”¨æˆ·æ¨èä½¿ç”¨ DeepSeek è¿›è¡Œä»£ç è°ƒè¯•ï¼Œè€Œ Claude åˆ™æ›´é€‚åˆç”Ÿæˆé«˜è´¨é‡çš„è¾“å‡ºã€‚
Windsurf çš„ AI æ™ºèƒ½ä½“ï¼ˆAI Agentï¼‰ä¸ä¸åŒæ¨¡å‹ç»“åˆåçš„èƒ½åŠ›è¡¨ç°å—åˆ°äº†è®¨è®ºï¼Œè¿™è¡¨æ˜æ¨¡å‹é€‰æ‹©ä¼šå½±å“æœ€ç»ˆçš„æ€§èƒ½ã€‚
Windsurf å®‰è£…å’Œç™»å½•é—®é¢˜ï¼šæœ‰ç”¨æˆ·æŠ¥å‘Šç§°ï¼Œå³ä½¿å·²è®¢é˜… Pro ç‰ˆæœ¬ï¼Œä¹Ÿæ— æ³•æ­£å¸¸ç™»å½• Windsurf IDEï¼Œé‡åˆ°äº†è¯•ç”¨æ¿€æ´»å’Œèº«ä»½éªŒè¯é—®é¢˜ã€‚
å¦æœ‰ç”¨æˆ·åœ¨å°è¯•é‡æ–°å®‰è£… IDE åï¼Œæ”¶åˆ°äº†ç‰ˆæœ¬ä¸åŒ¹é…çš„é”™è¯¯æç¤ºã€‚
ä½¿ç”¨å±‚å æ–‡ä»¶æ—¶çš„ä½“éªŒï¼šç”¨æˆ·åé¦ˆï¼Œåœ¨ Angular é¡¹ç›®ä¸­æ‰‹åŠ¨å°†å¤šä¸ªæ–‡ä»¶æ·»åŠ åˆ° Cascade èŠå¤©éå¸¸ç¹çï¼Œå¸Œæœ›èƒ½å¤Ÿæœ‰æ›´å¥½çš„é›†æˆæ–¹å¼ã€‚
æœ‰äººå»ºè®®ä½¿ç”¨å³é”®èœå•å¤åˆ¶æ–‡ä»¶è·¯å¾„ï¼Œä»è€Œæ›´é«˜æ•ˆåœ°åœ¨è®¨è®ºä¸­å¼•ç”¨æ–‡ä»¶ã€‚
é“¾æ¥ï¼š
Windsurf Next Changelogs | Windsurf Editor and Codeium extensionsï¼šWindsurf Next æ‰©å±•çš„æœ€æ–°æ›´æ–°å’Œå˜æ›´æ—¥å¿—ã€‚
Feature Requests | Codeiumï¼šå‘ Codeium å›¢é˜Ÿæä¾›åé¦ˆï¼Œä»¥å¸®åŠ©æˆ‘ä»¬åšå‡ºæ›´æ˜æ™ºçš„äº§å“å†³ç­–ã€‚è¯¥åŠŸèƒ½ç”± Canny æä¾›æ”¯æŒã€‚
Open VSX Registry:
Auto commit message | Feature Requests | Codeiumï¼šä»å·²æäº¤çš„æ–‡ä»¶ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆæäº¤ä¿¡æ¯
Roll-over of Pro Credits | Feature Requests | Codeiumï¼šæœªä½¿ç”¨çš„é«˜çº§ç”¨æˆ·æç¤ºé¢åº¦å’Œé«˜çº§æµç¨‹æ“ä½œé¢åº¦å¯æ»šåŠ¨è‡³ä¸‹ä¸ªæœˆ
Tweet from Kevin Houï¼ˆ@kevinhou22)ï¼šwe love docs! ğŸ“– I'm working on improving /adding more @ docs shortcuts to @windsurf_ailmk what you want and I'll add as many as I can... ğŸ§µalso shoutout @mintlify for auto-hosting all docs w...
Reddit - Dive into anything:
aiderï¼ˆPaul Gauthierï¼‰â–· #generalï¼ˆ337 messagesğŸ”¥ğŸ”¥):
Hiring Updateï¼ŒAider Error Handlingï¼ŒDeepSeek and Gemini Modelsï¼ŒLLM Editing Formatsï¼ŒPen Testing with LLMs

Congrats on the Job Offer!: A member announced they received a job offer, highlighting a significant pay increase and less downtime compared to their current role.
They expressed excitement about the new opportunity and the potential for increased earnings to fund their AI projects.
Common Aider Error Encountered: A user reported an issue with Aider indicating an invalid port error while trying to load model metadata.
Another member suggested overriding the default model metadata file as a workaround to resolve this error.
Discussion on DeepSeek and Gemini Models: Users discussed DeepSeek's recent inconsistencies and the performance of the Gemini models, particularly mentioning difficulties with the 1206 model.
Specifically, they noted that Google's models by default use a unique editing format (udiff) which differs from other models.
Understanding LLM Editing Formats: Users spoke about the various edit formats used by Aider, distinguishing between standard diff formats and Aider's own UDIFF syntax.
They clarified that Aider automatically uses udiff for Google models while maintaining different defaults for others.
Pen Testing Project with LLMs: One member shared their project creating a pen testing setup using LLMs, highlighting how two models work together in a simulated hacking environment.
Despite high token usage, they mentioned the potential financial benefits, as professional pen tests can be extremely lucrative.
Links mentioned:
Perplexity AI (pplx-api) | liteLLM: https://www.perplexity.ai
Linting and testing: Automatically fix linting and testing errors.
Edit formats: Aider uses various â€œedit formatsâ€ to let LLMs edit source files.
Sonar Reasoning - API, Providers, Stats: Sonar Reasoning is a reasoning model provided by Perplexity based on [DeepSeek R1](/deepseek/deepseek-r1).It allows developers to utilize long chain of thought with built-in web search. Run Sonar Reas...
DeepClaude: no description found
LiteLLM: LiteLLM handles loadbalancing, fallbacks and spend tracking across 100+ LLMs. all in the OpenAI format
Advanced model settings: Configuring advanced settings for LLMs.
Vercel's Guillermo Rauch on AI and the Future of Coding - Ep. 47: Read Dan Shipper's essay on the allocation economy: https://every.to/chain-of-thought/the-knowledge-economy-is-over-welcome-to-the-allocation-economyGuillerm...
Autonomous AI in Action ğŸ’ª | Live Codestream with Aider & Deepseek v3 ğŸ§ : In this experimeny, Deepseek v3 (via Aider) is in charge of building a project with minimal human intervention. The AI is working on a Summarizer app that mo...
Add tree-sitter-hcl-tags.scm for terraform repomap generation Â· Issue #3159 Â· Aider-AI/aider: This is a first pass at a tree-sitter-hcl-tags.scm to enable repomap for terraform repositories. I'm able to use it locally after adding .tf extensions to grep-ast and identifying them as hcl. Usi...
OpenAI compatible APIs: aider is AI pair programming in your terminal
Aider's benchmark is explicitly not about using R1 thinking tokens (and says that using them did worse) Â· Issue #13 Â· getAsterisk/deepclaude: Hey deepclaude folks, I'm a bit confused about why you are prominently citing aider's R1+Sonnet benchmarking results. The blog article and twitter post about these results explicitly state tha...
SDK not that good Â· Issue #2052 Â· Aider-AI/aider: Hi, I really love your toolâ€”I'm using it, and I think it's great. However, when I try to wrap it in Python, it's not as easy as I expected. While the documentation shows how to use coder.r...
GitHub - jj-vcs/jj: A Git-compatible VCS that is both simple and powerful: A Git-compatible VCS that is both simple and powerful - jj-vcs/jj
GitHub - lumina-ai-inc/chunkr: Vision model based document ingestion: Vision model based document ingestion. Contribute to lumina-ai-inc/chunkr development by creating an account on GitHub.
Bug: Creating files named with the file extension only without the filename. Â· Issue #2879 Â· Aider-AI/aider: It suggests the correct filenames but then it will generate a file named php instead of install.php or sql instead of migration.sql
GitHub Â· Build and ship software on a single, collaborative platform: Join the world's most widely adopted, AI-powered developer platform where millions of developers, businesses, and the largest open source community build software that advances humanity.
Aider creates files using random strings as filenames Â· Issue #3139 Â· Aider-AI/aider: Issue Using o3-mini to prompt and it's been using very weird filenames like 2. New file for modular integration of the embedding worker New file (empty file) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ I think...
aider (Paul Gauthier) â–· #questions-and-tips (23 messagesğŸ”¥):
Aider Support for Agents, Staging Changes in Aider, Commit Messages with R1, Model Configuration Issues, Architect Mode Functionality

æ­å–œè·å¾—å·¥ä½œæœºä¼šï¼ï¼šä¸€ä½æˆå‘˜åˆ†äº«äº†è‡ªå·±æ‹¿åˆ°æ–°å·¥ä½œ offer çš„å–œè®¯ï¼Œå¹¶è¡¨ç¤ºæ–°å·¥ä½œçš„è–ªèµ„å¾…é‡å¤§å¹…æå‡ï¼Œå·¥ä½œå‹åŠ›ä¹Ÿæ›´å°ã€‚
ä»–ä»¬å¯¹æ–°çš„æœºä¼šæ„Ÿåˆ°éå¸¸å…´å¥‹ï¼Œå¹¶å¸Œæœ›èƒ½é€šè¿‡å¢åŠ æ”¶å…¥æ¥æ”¯æŒä»–ä»¬çš„ AI é¡¹ç›®ç ”å‘ã€‚
Aider å¸¸è§é”™è¯¯ï¼šä¸€ä½ç”¨æˆ·åé¦ˆ Aider å‡ºç°é—®é¢˜ï¼Œæç¤ºåŠ è½½æ¨¡å‹å…ƒæ•°æ®æ—¶ç«¯å£æ— æ•ˆã€‚
å¦ä¸€ä½æˆå‘˜å»ºè®®ï¼Œå¯ä»¥å°è¯•è¦†ç›–é»˜è®¤æ¨¡å‹å…ƒæ•°æ®æ–‡ä»¶ä½œä¸ºä¸´æ—¶è§£å†³æ–¹æ¡ˆã€‚
å…³äº DeepSeek å’Œ Gemini æ¨¡å‹çš„è®¨è®ºï¼šç”¨æˆ·è®¨è®ºäº† DeepSeek è¿‘æœŸè¡¨ç°ä¸ç¨³å®šï¼Œä»¥åŠ Gemini æ¨¡å‹çš„æ€§èƒ½é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ 1206 æ¨¡å‹ã€‚
å…·ä½“æ¥è¯´ï¼Œä»–ä»¬æåˆ° Google çš„æ¨¡å‹é»˜è®¤é‡‡ç”¨ä¸€ç§ç‰¹æ®Šçš„ç¼–è¾‘æ ¼å¼ï¼ˆudiffï¼‰ï¼Œä¸å…¶ä»–æ¨¡å‹æœ‰æ‰€åŒºåˆ«ã€‚
äº†è§£ LLM ç¼–è¾‘æ ¼å¼ï¼šç”¨æˆ·è®¨è®ºäº† Aider ä½¿ç”¨çš„å„ç§ç¼–è¾‘æ ¼å¼ï¼ŒåŒ…æ‹¬æ ‡å‡† diff æ ¼å¼å’Œ Aider è‡ªæœ‰çš„ UDIFF è¯­æ³•ã€‚
ä»–ä»¬æ¾„æ¸…è¯´ï¼ŒAider ä¼šè‡ªåŠ¨å¯¹ Google æ¨¡å‹ä½¿ç”¨ udiff æ ¼å¼ï¼Œè€Œå¯¹å…¶ä»–æ¨¡å‹åˆ™ä¿æŒä¸åŒçš„é»˜è®¤è®¾ç½®ã€‚
åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ¸—é€æµ‹è¯•é¡¹ç›®ï¼šä¸€ä½æˆå‘˜åˆ†äº«äº†ä»–ä»¬åˆ©ç”¨ LLM åˆ›å»ºæ¸—é€æµ‹è¯•ç¯å¢ƒçš„é¡¹ç›®ï¼Œå±•ç¤ºäº†ä¸¤ä¸ªæ¨¡å‹å¦‚ä½•åœ¨æ¨¡æ‹Ÿé»‘å®¢ç¯å¢ƒä¸­ååŒå·¥ä½œã€‚
å°½ç®¡ Token æ¶ˆè€—é‡å¾ˆå¤§ï¼Œä½†è€ƒè™‘åˆ°ä¸“ä¸šæ¸—é€æµ‹è¯•çš„é«˜æ”¶ç›Šï¼Œè¯¥é¡¹ç›®ä»å…·æœ‰æ½œåœ¨çš„ç»æµä»·å€¼ã€‚
æåˆ°çš„é“¾æ¥ï¼š
Perplexity AIï¼ˆpplx-apiï¼‰| liteLLMï¼šhttps://www.perplexity.ai
Linting and testingï¼šAutomatically fix linting and testing errors.
Edit formatsï¼šAider uses variousã€Œedit formatsã€to let LLMs edit source files.
Sonar Reasoning - APIï¼ŒProvidersï¼ŒStatsï¼šSonar Reasoning is a reasoning model provided by Perplexity based on [DeepSeek R1](/deepseek/deepseek-r1).It allows developers to utilize long chain of thought with built-in web search. Run Sonar Reas...
DeepClaudeï¼šno description found
LiteLLMï¼šLiteLLM handles loadbalancingï¼Œfallbacks and spend tracking across 100+ LLMs. all in the OpenAI format
Advanced model settingsï¼šConfiguring advanced settings for LLMs.
Vercel's Guillermo Rauch on AI and the Future of Coding - Ep. 47ï¼šRead Dan Shipper's essay on the allocation economyï¼šhttps://every.to/chain-of-thought/the-knowledge-economy-is-over-welcome-to-the-allocation-economyGuillerm...
Autonomous AI in Action ğŸ’ª | Live Codestream with Aider & Deepseek v3 ğŸ§ ï¼šIn this experimenyï¼ŒDeepseek v3ï¼ˆvia Aiderï¼‰is in charge of building a project with minimal human intervention. The AI is working on a Summarizer app that mo...
Add tree-sitter-hcl-tags.scm for terraform repomap generationÂ·Issue #3159Â·Aider-AI/aiderï¼šThis is a first pass at a tree-sitter-hcl-tags.scm to enable repomap for terraform repositories. I'm able to use it locally after adding .tf extensions to grep-ast and identifying them as hcl. Usi...
OpenAI compatible APIsï¼šaider is AI pair programming in your terminal
Aider's benchmark is explicitly not about using R1 thinking tokensï¼ˆand says that using them did worse)Â·Issue #13Â·getAsterisk/deepclaudeï¼šHey deepclaude folksï¼ŒI'm a bit confused about why you are prominently citing aider's R1+Sonnet benchmarking results. The blog article and twitter post about these results explicitly state tha...
SDK not that goodÂ·Issue #2052Â·Aider-AI/aiderï¼šHiï¼ŒI really love your toolâ€”I'm using itï¼Œand I think it's great. Howeverï¼Œwhen I try to wrap it in Pythonï¼Œit's not as easy as I expected. While the documentation shows how to use coder.r...
GitHub - jj-vcs/jjï¼šA Git-compatible VCS that is both simple and powerfulï¼šA Git-compatible VCS that is both simple and powerful - jj-vcs/jj
GitHub - lumina-ai-inc/chunkrï¼šVision model based document ingestionï¼šVision model based document ingestion. Contribute to lumina-ai-inc/chunkr development by creating an account on GitHub.
Bugï¼šCreating files named with the file extension only without the filename.Â·Issue #2879Â·Aider-AI/aiderï¼šIt suggests the correct filenames but then it will generate a file named php instead of install.php or sql instead of migration.sql
GitHubÂ·Build and ship software on a singleï¼Œcollaborative platformï¼šJoin the world's most widely adoptedï¼ŒAI-powered developer platform where millions of developersï¼Œbusinessesï¼Œand the largest open source community build software that advances humanity.
Aider creates files using random strings as filenamesÂ·Issue #3139Â·Aider-AI/aiderï¼šIssue Using o3-mini to prompt and it's been using very weird filenames like 2. New file for modular integration of the embedding worker New fileï¼ˆempty fileï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ I think...
aiderï¼ˆPaul Gauthierï¼‰â–· #questions-and-tipsï¼ˆ23 messagesğŸ”¥):
Aider Support for Agentsï¼ŒStaging Changes in Aiderï¼ŒCommit Messages with R1ï¼ŒModel Configuration Issuesï¼ŒArchitect Mode Functionality

Aider Might Support Agents Soon: A user inquired whether Aider will support agents at any point, indicating a potential development or update in the tool's capabilities.
This reflects ongoing interest in expanding Aider's functionality to enhance user experience.
Staging Changes Not Yet Available: One user asked if there's a way to stage changes instead of committing them directly in Aider, suggesting a desire for more granular control.
This indicates user interest in workflow features that may simplify the versioning process.
Troubles with Tokens in Commits: A user shared concerns about their commit messages filled with tokens from using R1 via Together.ai, seeking tips on configuration.
Recommendations included configuring model settings appropriately to minimize these tokens in commit messages.
Help with Internal OpenWeb UI Instance: A user requested guidance on using a JWT API key from an internal OpenWeb UI instance with Aider, noting that standard API keys weren't available.
This highlights the challenges with internal tools that restrict direct API access, complicating integration efforts.
Concerns about Architect Mode: A user expressed confusion over Architect mode's behavior, stating it doesn't allow them to dictate the next steps as intended.
Others noted that using the /ask command can achieve desired control without needing to adjust the mode.
Link mentioned: Reasoning models: How to configure reasoning model settings from secondary providers.

Aider å¯èƒ½ä¼šå¾ˆå¿«æ”¯æŒ AI æ™ºèƒ½ä½“ï¼ˆAI Agent)ï¼šä¸€ä½ç”¨æˆ·è¯¢é—® Aider æ˜¯å¦ä¼šåœ¨æœªæ¥æ”¯æŒ AI æ™ºèƒ½ä½“ï¼Œæš—ç¤ºè¯¥å·¥å…·çš„åŠŸèƒ½å¯èƒ½è¿æ¥æ–°çš„å¼€å‘æˆ–æ›´æ–°ã€‚
è¿™åæ˜ å‡ºç”¨æˆ·æŒç»­å¸Œæœ›æ‰©å±• Aider çš„åŠŸèƒ½ï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒã€‚
æš‚ä¸æ”¯æŒæš‚å­˜æ›´æ”¹ï¼šæœ‰ç”¨æˆ·æé—®æ˜¯å¦å¯ä»¥æš‚å­˜æ›´æ”¹ï¼Œè€Œä¸æ˜¯ç›´æ¥åœ¨ Aider ä¸­æäº¤ï¼Œè¡¨æ˜ä»–ä»¬å¸Œæœ›å¯¹ç‰ˆæœ¬æ§åˆ¶æœ‰æ›´ç²¾ç»†çš„æ§åˆ¶ã€‚
è¿™è¡¨æ˜ç”¨æˆ·å¯¹èƒ½ç®€åŒ–ç‰ˆæœ¬æ§åˆ¶æµç¨‹çš„å·¥ä½œæµåŠŸèƒ½å¾ˆæ„Ÿå…´è¶£ã€‚
æäº¤ä¿¡æ¯ä¸­ Token çš„é—®é¢˜ï¼šä¸€ä½ç”¨æˆ·åé¦ˆï¼Œåœ¨ä½¿ç”¨ Together.ai å’Œ R1 æ¨¡å‹æäº¤ä»£ç æ—¶ï¼Œæäº¤ä¿¡æ¯ä¸­å‡ºç°äº†å¤§é‡çš„ Tokenï¼Œå¸Œæœ›è·å–é…ç½®æ–¹é¢çš„å»ºè®®ã€‚
å¾—åˆ°çš„å»ºè®®æ˜¯ï¼Œé€‚å½“è°ƒæ•´æ¨¡å‹é…ç½®ï¼Œå¯ä»¥æœ€å¤§ç¨‹åº¦å‡å°‘æäº¤ä¿¡æ¯ä¸­å‡ºç°çš„ Tokenã€‚
å…³äºå†…éƒ¨ OpenWeb UI å®ä¾‹çš„å¸®åŠ©ï¼šä¸€ä½ç”¨æˆ·è¯·æ±‚åœ¨ä½¿ç”¨ Aider è¿æ¥å†…éƒ¨ OpenWeb UI å®ä¾‹æ—¶ï¼Œå¦‚ä½•ä½¿ç”¨ JWT API å¯†é’¥ï¼Œå› ä¸ºæ ‡å‡†çš„ API å¯†é’¥æ— æ³•è·å–ã€‚
è¿™çªæ˜¾äº†é‚£äº›é™åˆ¶ç›´æ¥ API è®¿é—®çš„å†…éƒ¨å·¥å…·æ‰€å¸¦æ¥çš„é›†æˆæŒ‘æˆ˜ã€‚
å¯¹æ¶æ„å¸ˆæ¨¡å¼çš„æ‹…å¿§ï¼šæœ‰ç”¨æˆ·å¯¹æ¶æ„å¸ˆæ¨¡å¼çš„è¡¨ç°æ„Ÿåˆ°å›°æƒ‘ï¼Œä»–ä»¬è¡¨ç¤ºè¯¥æ¨¡å¼ä¼¼ä¹å¹¶ä¸å…è®¸ç”¨æˆ·åƒé¢„æœŸé‚£æ ·æŒ‡å®šä¸‹ä¸€æ­¥æ“ä½œã€‚
å…¶ä»–ç”¨æˆ·æŒ‡å‡ºï¼Œå¯ä»¥ä½¿ç”¨ `/ask` å‘½ä»¤æ¥è¾¾åˆ°æ§åˆ¶ç›®çš„ï¼Œè€Œæ— éœ€è°ƒæ•´æ¨¡å¼ã€‚
ç›¸å…³é“¾æ¥ï¼šæ¨ç†æ¨¡å‹ï¼ˆReasoning Model)ï¼šå¦‚ä½•é…ç½®æ¥è‡ªäºŒçº§ä¾›åº”å•†çš„æ¨ç†æ¨¡å‹è®¾ç½®ã€‚

aider (Paul Gauthier) â–· #links (2 messages):
Gemini 2.0, Open Deep Research, HuggingFace, Agent frameworks

Gemini 2.0 Launches on LMSYS: The latest model, Gemini 2.0, has been featured on LMSYS showcasing its advancements in capabilities.
This launch aims to enhance the discussion around next-gen models in the AI community.
HuggingFace Unveils Open Deep Research Clone: HuggingFace researchers have created an open-source clone of DeepResearch, detailed in their blog post.
This initiative emphasizes the importance of agent frameworks and sets out the GAIA benchmark, paving the way for community contributions.
Link mentioned: Open-source DeepResearch â€“ Freeing our search agents: no description found

Gemini 2.0ï¼Œå¼€æ”¾å¼æ·±åº¦ç ”ç©¶ï¼ŒHuggingFaceï¼Œæ™ºèƒ½ä½“æ¡†æ¶ï¼ˆAgent frameworks)

Gemini 2.0 åœ¨ LMSYS ä¸Šå‘å¸ƒï¼šæœ€æ–°çš„æ¨¡å‹ Gemini 2.0 å·²ç»å‡ºç°åœ¨ LMSYS ä¸Šï¼Œå±•ç¤ºäº†å…¶åœ¨å„é¡¹èƒ½åŠ›ä¸Šçš„è¿›æ­¥ã€‚
æœ¬æ¬¡å‘å¸ƒæ—¨åœ¨ä¿ƒè¿› AI ç¤¾åŒºå›´ç»•ä¸‹ä¸€ä»£æ¨¡å‹å±•å¼€æ›´æ·±å…¥çš„è®¨è®ºã€‚
HuggingFace å‘å¸ƒå¼€æ”¾å¼ DeepResearch å…‹éš†ç‰ˆæœ¬ï¼šHuggingFace çš„ç ”ç©¶äººå‘˜åˆ›å»ºäº†ä¸€ä¸ª DeepResearch çš„å¼€æºå…‹éš†ç‰ˆæœ¬ï¼Œå¹¶åœ¨å…¶åšå®¢æ–‡ç« ä¸­è¿›è¡Œäº†è¯¦ç»†ä»‹ç»ã€‚
æ­¤ä¸¾æªå¼ºè°ƒäº†æ™ºèƒ½ä½“æ¡†æ¶ï¼ˆAgent frameworksï¼‰çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºäº† GAIA åŸºå‡†ï¼Œä¸ºç¤¾åŒºè´¡çŒ®åŠ›é‡é“ºå¹³äº†é“è·¯ã€‚
æåˆ°çš„é“¾æ¥ï¼šå¼€æº DeepResearch â€“ é‡Šæ”¾æˆ‘ä»¬çš„æœç´¢æ™ºèƒ½ä½“ï¼šæœªæ‰¾åˆ°æè¿°

OpenAI â–· #ai-discussions (276 messagesğŸ”¥ğŸ”¥):
Gemini 2.0 Pro, OpenAI vs DeepSeek, AI for Coding, Chatbot Aggregators, AI Model Comparisons

Gemini 2.0 Pro Excitement: Users expressed excitement about the capabilities of Gemini 2.0 Pro, highlighting its 2 million token context which allows for complex interactions and creative writing tasks.
However, concerns were raised regarding its usability compared to free alternatives like Google AI Studio, which offers extensive customization.
The Great Chatbot Showdown: A potential chess match between DeepSeek and ChatGPT sparked interest, with users speculating on the outcomes given the models' limitations on reasoning.
There was a humorous contrast drawn between the pricing of DeepSeekâ€™s $1 chess game versus OpenAI's $100 chess game.
AI for Coding Recommendations: In discussions about coding, members recommended Gemini Flash 2.0 and Microsoft Copilot for their features and cost-effectiveness, particularly for advanced mathematics.
Users noted that Copilot offers a free trial, making it easier to explore without immediate financial commitment.
AI Automated Coding Solutions: Conversations shifted towards finding an automated way to code with minimal dependencies, seeking solutions that reduce terminal time.
The goal was to discover the most agentic approach for coding tasks using AI, emphasizing user-friendly environments.
AI Model Comparisons and Experiences: Users shared mixed experiences with various AI models, noting that Gemini 2.0 and Sonnet 3.5 performed better in user tasks but had different features.
The consensus was that task requirements greatly influenced model choice, with attention to both capabilities and costs.
Link mentioned: Fire Writing GIF - Fire writing - Discover & Share GIFs: Click to view the GIF

OpenAI â–· #ai-discussionsï¼ˆ276 æ¡æ¶ˆæ¯ğŸ”¥ğŸ”¥)ï¼š
Gemini 2.0 Proã€OpenAI vs DeepSeekã€AI ç¼–ç ã€èŠå¤©æœºå™¨äººèšåˆå™¨ã€AI æ¨¡å‹æ¯”è¾ƒ

Gemini 2.0 Pro ä»¤äººå…´å¥‹ï¼šç”¨æˆ·å¯¹ Gemini 2.0 Pro çš„åŠŸèƒ½è¡¨ç¤ºå…´å¥‹ï¼Œå¼ºè°ƒå…¶ 200 ä¸‡ Token çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œè¿™å…è®¸å¤æ‚çš„äº¤äº’å’Œåˆ›é€ æ€§çš„å†™ä½œä»»åŠ¡ã€‚
ç„¶è€Œï¼Œäººä»¬å¯¹å…¶å¯ç”¨æ€§æå‡ºäº†æ‹…å¿§ï¼Œä¸ Google AI Studio ç­‰å…è´¹æ›¿ä»£å“ç›¸æ¯”ï¼Œåè€…æä¾›äº†å¹¿æ³›çš„è‡ªå®šä¹‰ã€‚
èŠå¤©æœºå™¨äººå·…å³°å¯¹å†³ï¼šDeepSeek å’Œ ChatGPT ä¹‹é—´æ½œåœ¨çš„å›½é™…è±¡æ£‹æ¯”èµ›å¼•èµ·äº†äººä»¬çš„å…´è¶£ï¼Œç”¨æˆ·æ¨æµ‹äº†é‰´äºè¿™äº›æ¨¡å‹åœ¨æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œå…¶ç»“æœå¦‚ä½•ã€‚
äººä»¬å¹½é»˜åœ°å¯¹æ¯”äº† DeepSeek çš„ 1 ç¾å…ƒå›½é™…è±¡æ£‹æ¸¸æˆä¸ OpenAI çš„ 100 ç¾å…ƒå›½é™…è±¡æ£‹æ¸¸æˆçš„ä»·æ ¼ã€‚
AI ç¼–ç æ¨èï¼šåœ¨å…³äºç¼–ç çš„è®¨è®ºä¸­ï¼Œæˆå‘˜æ¨èäº† Gemini Flash 2.0 å’Œ Microsoft Copilotï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰åŠŸèƒ½å’Œæˆæœ¬æ•ˆç›Šï¼Œç‰¹åˆ«æ˜¯å¯¹äºé«˜ç­‰æ•°å­¦ã€‚
ç”¨æˆ·æŒ‡å‡ºï¼ŒCopilot æä¾›å…è´¹è¯•ç”¨ï¼Œå¯ä»¥æ›´è½»æ¾åœ°è¿›è¡Œæ¢ç´¢ï¼Œæ— éœ€ç«‹å³æ‰¿æ‹…ç»æµè´Ÿæ‹…ã€‚
AI è‡ªåŠ¨åŒ–ç¼–ç è§£å†³æ–¹æ¡ˆï¼šå¯¹è¯è½¬å‘å¯»æ‰¾ä¸€ç§ä»¥æœ€å°‘çš„ä¾èµ–æ€§è¿›è¡Œè‡ªåŠ¨åŒ–ç¼–ç çš„æ–¹æ³•ï¼Œå¯»æ±‚å‡å°‘ç»ˆç«¯æ—¶é—´çš„è§£å†³æ–¹æ¡ˆã€‚
ç›®æ ‡æ˜¯æ‰¾åˆ°ä½¿ç”¨ AI è¿›è¡Œç¼–ç ä»»åŠ¡çš„æœ€æ™ºèƒ½åŒ–çš„æ–¹æ³•ï¼Œå¼ºè°ƒç”¨æˆ·å‹å¥½çš„ç¯å¢ƒã€‚
AI æ¨¡å‹æ¯”è¾ƒå’Œç»éªŒï¼šç”¨æˆ·åˆ†äº«äº†ä½¿ç”¨å„ç§ AI æ¨¡å‹çš„ä¸åŒä½“éªŒï¼ŒæŒ‡å‡º Gemini 2.0 å’Œ Sonnet 3.5 åœ¨ç”¨æˆ·ä»»åŠ¡ä¸­è¡¨ç°æ›´å¥½ï¼Œä½†å…·æœ‰ä¸åŒçš„åŠŸèƒ½ã€‚
å…±è¯†æ˜¯ä»»åŠ¡è¦æ±‚æå¤§åœ°å½±å“äº†æ¨¡å‹é€‰æ‹©ï¼Œå¹¶æ³¨æ„äº†èƒ½åŠ›å’Œæˆæœ¬ã€‚
æåˆ°çš„é“¾æ¥ï¼šFire Writing GIF - Fire writing - Discover & Share GIFsï¼šç‚¹å‡»æŸ¥çœ‹è¯¥ GIF å›¾ç‰‡

OpenAI â–· #gpt-4-discussions (5 messages):
Deep Research chat for Plus users

Anticipation for Deep Research in Plus Version: Several members expressed eagerness for the Deep Research chat feature to be available for Plus users soon.
Members specifically noted their need for it, hoping for a release in the coming days.
Conversation on not knowing details: A member remarked about a previous comment stating, 'I never knew,' indicating a lack of information on the topic.
Another member responded with skepticism, saying, 'It's not, what do you mean?'
Request for Deep Research chat sharing: A member inquired if anyone had shared information about Deep Research chats, obviously looking for insights.
This inquiry prompted others to express similar anticipation regarding the feature coming to Plus subscriptions.
OpenAI â–· #prompt-engineering (6 messages):
Response Length Control, Undesired Behavior in AI Models, Input Influencing Output

OpenAI â–· #gpt-4-discussionsï¼ˆ5 æ¡æ¶ˆæ¯):
Plus ç”¨æˆ·ä¸“ç”¨çš„æ·±åº¦ç ”ç©¶èŠå¤©å¯¹ Plus ç‰ˆæœ¬ä¸­æ·±åº¦ç ”ç©¶åŠŸèƒ½çš„æœŸå¾…ï¼š ä¸€äº›æˆå‘˜è¡¨ç¤ºéå¸¸å¸Œæœ›ã€Œæ·±åº¦ç ”ç©¶ã€èŠå¤©åŠŸèƒ½ à¤œà¤²à¥à¤¦ï¼ˆå°½å¿«ï¼‰ å‘ Plus ç”¨æˆ·å¼€æ”¾ã€‚
ä»–ä»¬ç‰¹åˆ«æåˆ°å¯¹è¯¥åŠŸèƒ½çš„éœ€æ±‚ï¼Œå¹¶æœŸå¾…åœ¨æœªæ¥å‡ å¤©å†…ä¸Šçº¿ã€‚
å…³äºä¿¡æ¯ç¼ºå¤±çš„è®¨è®ºï¼š æœ‰æˆå‘˜å¼•ç”¨ä¹‹å‰çš„è¯„è®ºã€Œæˆ‘ä»æ¥ä¸çŸ¥é“ã€ï¼Œè¡¨ç¤ºå¯¹ç›¸å…³ä¿¡æ¯ä¸äº†è§£ã€‚
å¦ä¸€ä½æˆå‘˜å¯¹æ­¤è¡¨ç¤ºæ€€ç–‘ï¼Œåé—®é“ï¼šã€Œä¸æ˜¯å§ï¼Œä½ è¿™è¯ä»€ä¹ˆæ„æ€ï¼Ÿã€
å…³äºã€Œæ·±åº¦ç ”ç©¶ã€èŠå¤©åˆ†äº«çš„è¯·æ±‚ï¼š æœ‰æˆå‘˜è¯¢é—®æ˜¯å¦æœ‰äººåˆ†äº«è¿‡å…³äºã€Œæ·±åº¦ç ”ç©¶ã€èŠå¤©çš„ä¿¡æ¯ï¼Œ æ˜¾ç„¶å¸Œæœ›äº†è§£æ›´å¤šã€‚
è¿™ä¸€è¯¢é—®ä¹Ÿå¼•å‘äº†å…¶ä»–ç”¨æˆ·å¯¹äº Plus è®¢é˜…ä½•æ—¶èƒ½ç”¨ä¸Šè¯¥åŠŸèƒ½çš„æœŸå¾…ã€‚
OpenAI â–· #prompt-engineeringï¼ˆ6 æ¡æ¶ˆæ¯):
å“åº”é•¿åº¦æ§åˆ¶ã€AI æ¨¡å‹ä¸­çš„ä¸è‰¯è¡Œä¸ºã€å½±å“è¾“å‡ºçš„è¾“å…¥

Strategies to Ensure Optimal Response Length: One member suggested using Python to count words and iterate to ensure better response length, though this may impact creativity.
They noted that more input generally results in more output but acknowledged the challenge of character counting without external assistance.
Editing for Desired Output: Another member emphasized the importance of editing inputs using the edit button to sculpt the AI's output effectively.
They advised adjusting your input until satisfied before proceeding to ensure coherent context in the conversation.
Undesired Behaviors Persist: A member expressed frustration with AI models that repeat undesired behaviors unless actively controlled.
They highlighted the need for strategies to manage and mitigate these behaviors for a better user experience.
OpenAI â–· #api-discussions (6 messages):
Controlling AI Response Length, Managing Undesired AI Behavior

ç¡®ä¿æœ€ä½³å“åº”é•¿åº¦çš„ç­–ç•¥ï¼šä¸ºäº†ç¡®ä¿æœ€ä½³çš„å“åº”é•¿åº¦ï¼Œæœ‰æˆå‘˜å»ºè®®ä½¿ç”¨ Python ç»Ÿè®¡è¯æ•°å¹¶è¿›è¡Œè¿­ä»£ï¼Œå°½ç®¡è¿™å¯èƒ½ä¼šå½±å“ç”Ÿæˆå¼ AIï¼ˆGenerative AIï¼‰çš„åˆ›é€ æ€§ã€‚
ä»–ä»¬æŒ‡å‡ºï¼Œé€šå¸¸æƒ…å†µä¸‹ï¼Œè¾“å…¥è¶Šå¤šï¼Œè¾“å‡ºä¹Ÿè¶Šå¤šï¼Œä½†åŒæ—¶ä¹Ÿæ‰¿è®¤åœ¨æ²¡æœ‰å¤–éƒ¨å·¥å…·çš„å¸®åŠ©ä¸‹ç»Ÿè®¡å­—ç¬¦æ•°æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚
ä¸ºäº†è·å¾—æœŸæœ›çš„è¾“å‡ºï¼šå¦ä¸€ä½æˆå‘˜å¼ºè°ƒï¼Œåˆ©ç”¨ç¼–è¾‘åŠŸèƒ½ä¿®æ”¹è¾“å…¥ï¼Œå¯¹äºæœ‰æ•ˆåœ°å¡‘é€  AI çš„è¾“å‡ºè‡³å…³é‡è¦ã€‚
ä»–ä»¬å»ºè®®åœ¨ç»§ç»­å¯¹è¯ä¹‹å‰ï¼Œåå¤è°ƒæ•´è¾“å…¥ï¼Œç›´åˆ°è·å¾—æ»¡æ„çš„ç»“æœï¼Œä»¥ç¡®ä¿å¯¹è¯çš„ä¸Šä¸‹æ–‡è¿è´¯ã€‚
ä¸æœŸæœ›çš„è¡Œä¸ºæŒç»­å‡ºç°ï¼šæœ‰æˆå‘˜è¡¨ç¤ºï¼Œé™¤éè¿›è¡Œç§¯æçš„å¹²é¢„ï¼Œå¦åˆ™ AI æ¨¡å‹ä¼šé‡å¤å‡ºç°ä¸æœŸæœ›çš„è¡Œä¸ºï¼Œè¿™è®©ä»–ä»¬æ„Ÿåˆ°æ²®ä¸§ã€‚
ä»–ä»¬å¼ºè°ƒï¼Œéœ€è¦åˆ¶å®šç›¸åº”çš„ç­–ç•¥æ¥ç®¡ç†å’Œå‡è½»è¿™äº›ä¸æœŸæœ›çš„è¡Œä¸ºï¼Œä»è€Œæå‡ç”¨æˆ·ä½“éªŒã€‚
OpenAI â–· #api-discussionsï¼ˆ6 æ¡æ¶ˆæ¯)ï¼š
æ§åˆ¶ AI å“åº”é•¿åº¦ï¼Œç®¡ç†ä¸éœ€è¦çš„ AI è¡Œä¸º

Methods to Ensure AI Response Length: A member suggested using Python to count words and iterate, though warned it might impact creativity. They noted that more content typically results in longer responses, but counting characters is difficult for AI without assistance.
Editing to Sculpt AI Output: Another member mentioned that using the edit button can help control undesired behavior, allowing users to refine prompts until satisfied. This method sets the stage for the next interaction in the 'context' or 'conversation' chain.
Cursor IDE â–· #general (282 messagesğŸ”¥ğŸ”¥):
Cursor IDE Updates, Gemini 2.0 Performance, Clipboard Comparison Tools, MCP Server Configurations, Context Limitations in AI Models

ç¡®ä¿ AI å›å¤é•¿åº¦çš„æ–¹æ³•ï¼šæœ‰æˆå‘˜å»ºè®®ä½¿ç”¨ Python ç¼–ç¨‹æ¥ç»Ÿè®¡å­—æ•°å¹¶è¿›è¡Œè°ƒæ•´ï¼Œä¸è¿‡ä»–ä»¬ä¹Ÿæé†’è¯´ï¼Œè¿™ç§æ–¹æ³•å¯èƒ½ä¼šé™åˆ¶ AI çš„åˆ›é€ æ€§ã€‚ä»–ä»¬è¿˜æåˆ°ï¼Œé€šå¸¸è¾“å…¥çš„å†…å®¹è¶Šå¤šï¼ŒAI çš„å›å¤ä¹Ÿè¶Šé•¿ï¼›ä½†å¦‚æœæ²¡æœ‰é¢å¤–çš„è¾…åŠ©ï¼ŒAI æœ¬èº«å¾ˆéš¾è®¡ç®—å­—ç¬¦æ•°é‡ã€‚
é€šè¿‡ç¼–è¾‘æ¥ä¼˜åŒ– AI çš„è¾“å‡ºï¼šå¦ä¸€ä½æˆå‘˜æåˆ°ï¼Œå¯ä»¥é€šè¿‡ç¼–è¾‘æŒ‰é’®æ¥æ§åˆ¶ AI äº§ç”Ÿçš„ä¸è‰¯è¡Œä¸ºï¼Œç”¨æˆ·å¯ä»¥åå¤ä¿®æ”¹æç¤ºè¯­ï¼Œç›´åˆ°è·å¾—æ»¡æ„çš„ç»“æœã€‚è¿™ç§åšæ³•ä¹Ÿä¸ºåç»­çš„ã€Œä¸Šä¸‹æ–‡ã€æˆ–ã€Œå¯¹è¯ã€é“¾ä¸­çš„äº’åŠ¨æ‰“ä¸‹äº†åŸºç¡€ã€‚
Cursor IDE â–· #generalï¼ˆ282 messagesğŸ”¥ğŸ”¥)ï¼šå…³äº Cursor IDE çš„è®¨è®ºï¼Œä¸»é¢˜åŒ…æ‹¬ï¼šCursor IDE æ›´æ–°ã€Gemini 2.0 æ€§èƒ½è¡¨ç°ã€å‰ªè´´æ¿æ¯”è¾ƒå·¥å…·ã€MCP æœåŠ¡å™¨é…ç½®ï¼Œä»¥åŠ AI æ¨¡å‹ä¸­å­˜åœ¨çš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ç­‰ã€‚

Cursor IDE Updates and Features: Users shared insights on Cursor IDE updates, specifically the introduction of GitHub agents and discussion about the architect feature that could further enhance productivity.
There was also mention of challenges with running commands within the Composer tool, indicating a potential bug after recent updates.
Gemini 2.0 Performance Evaluation: Several users expressed satisfaction with Gemini 2.0, noting its solid performance for self-learning tasks, although some feel it isn't superior to Sonnet for coding.
There were discussions about the model's affordability and effective context use, contributing to its appeal for large codebases.
Clipboard Comparison Tools Suggestions: Participants provided suggestions for clipboard comparison tools, highlighting the VSCode extension that allows comparisons against the clipboard content.
Users compared capabilities of VSCode's local history and suggested using tools like Timeline for better efficiency akin to JetBrains.
MCP Server Configurations and Documentation: A user sought help with MCP server configurations and accessing necessary keys for Supabase, shared that some keys provide limited access.
The community discussed configurations and the need for better context management in Cursor, particularly for complex projects.
Context Limitations in AI Models: Concerns were raised about context limitations in Cursor, with a preference for models like Cline or Google models that provide larger contexts.
The impact of context size on the effectiveness of AI models was debated, specifically how larger context windows could enhance performance in broader applications.
Links mentioned:
LiveBench: no description found
AI Testing Tool | Automated AI Testing - Momentic: Supercharge your QA process with Momentic's advanced AI tools for automated testing. Ship faster with reliable, AI-driven tests.
Partial Diff - Visual Studio Marketplace: Extension for Visual Studio Code - Compare (diff) text selections within a file, across files, or to the clipboard
no title found: no description found
O3-mini is LIVE! What version are we getting?: First off, shoutout to the Cursor team for pushing this out so quickly! The team stating that their devs still prefer Sonnet for most tasks (which surprised them). (source: x.com) According to the O...
Model-specific rules: The new cursor rules system is great, it would also be good if we could have rules for specific models.
New Diff is shown between each line of the previewed code: Describe the Bug Whatâ€™s new is shown between each line of the previewed code Steps to Reproduce just ask Cursor Expected Behavior Normal code preview Screenshots / Screen Recordings Operating Sy...
Tweet from Andrej Karpathy (@karpathy): There's a new kind of coding I call "vibe coding", where you fully give in to the vibes, embrace exponentials, and forget that the code even exists. It's possible because the LLMs (e.g...
Releases Â· daniel-lxs/mcp-starter: Contribute to daniel-lxs/mcp-starter development by creating an account on GitHub.
Release v0.1.0 Â· daniel-lxs/mcp-starter: Initial Release
Document "Compare active file with clipboard" functionality Â· Issue #7284 Â· microsoft/vscode-docs: Introduced in VS Code 1.19, you can compare the active file with the contents of the clipboard. Command: File: Compare Active File with Clipboard (workbench.files.action.compareWithClipboard) Keybi...
GitHub - robert-at-pretension-io/mcp: code: code. Contribute to robert-at-pretension-io/mcp development by creating an account on GitHub.
GitHub - daniel-lxs/mcp-perplexity: Contribute to daniel-lxs/mcp-perplexity development by creating an account on GitHub.
GitHub - daniel-lxs/mcp-starter: Contribute to daniel-lxs/mcp-starter development by creating an account on GitHub.
Perplexity AI â–· #general (247 messagesğŸ”¥ğŸ”¥):
Perplexity AI Focus Mode, Query Handling in Perplexity Pro, R1 vs. Other Models, Performance Issues with Deepseek, User Concerns regarding Model Specifications

Cursor IDE æ›´æ–°ä¸ç‰¹æ€§ï¼šç”¨æˆ·ä»¬åˆ†äº«äº†å¯¹ Cursor IDE æ›´æ–°çš„çœ‹æ³•ï¼Œç‰¹åˆ«æ˜¯å…³äº GitHub æ™ºèƒ½ä½“çš„å¼•å…¥ï¼Œä»¥åŠä¸€é¡¹å¯èƒ½è¿›ä¸€æ­¥æå‡æ•ˆç‡çš„æ¶æ„ç‰¹æ€§çš„è®¨è®ºã€‚
æœ‰ç”¨æˆ·æåˆ°åœ¨ä½¿ç”¨ Composer å·¥å…·æ—¶è¿è¡Œå‘½ä»¤æ—¶é‡åˆ°äº†é—®é¢˜ï¼Œæš—ç¤ºæœ€è¿‘çš„æ›´æ–°å¯èƒ½å¼•å…¥äº†ä¸€ä¸ª Bugã€‚

Gemini 2.0 æ€§èƒ½è¯„ä¼°ï¼šä¸å°‘ç”¨æˆ·å¯¹ Gemini 2.0 çš„è¡¨ç°æ„Ÿåˆ°æ»¡æ„ï¼Œè®¤ä¸ºå®ƒåœ¨è‡ªä¸»å­¦ä¹ æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ä¹Ÿæœ‰äººè§‰å¾—åœ¨ä»£ç ç¼–å†™æ–¹é¢ï¼Œå®ƒä¸å¦‚ Sonnetã€‚
è®¨è®ºè¿˜æ¶‰åŠ Gemini 2.0 çš„æ€§ä»·æ¯”ä»¥åŠå…¶æœ‰æ•ˆåˆ©ç”¨ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ï¼Œè¿™äº›éƒ½å¢å¼ºäº†å®ƒå¯¹å¤§å‹ä»£ç åº“çš„å¸å¼•åŠ›ã€‚

å‰ªè´´æ¿æ¯”è¾ƒå·¥å…·æ¨èï¼šå¤§å®¶æ¨èäº†ä¸€äº›å‰ªè´´æ¿æ¯”è¾ƒå·¥å…·ï¼Œå…¶ä¸­ç‰¹åˆ«æåˆ°äº† VSCode çš„ä¸€æ¬¾æ‰©å±•ï¼Œå®ƒå¯ä»¥ç›´æ¥ä¸å‰ªè´´æ¿ä¸­çš„å†…å®¹è¿›è¡Œæ¯”è¾ƒã€‚
æœ‰ç”¨æˆ·å°† VSCode çš„æœ¬åœ°å†å²è®°å½•åŠŸèƒ½ä¸å…¶ä»–å·¥å…·è¿›è¡Œäº†å¯¹æ¯”ï¼Œå¹¶å»ºè®®ä½¿ç”¨åƒ Timeline è¿™æ ·çš„å·¥å…·ï¼Œä»¥ä¾¿è·å¾—ç±»ä¼¼ JetBrains çš„é«˜æ•ˆä½“éªŒã€‚

MCP æœåŠ¡å™¨é…ç½®ä¸æ–‡æ¡£ï¼šä¸€ä½ç”¨æˆ·å¯»æ±‚å…³äº MCP æœåŠ¡å™¨é…ç½®çš„å¸®åŠ©ï¼Œä»¥åŠå¦‚ä½•è·å– Supabase æ‰€éœ€çš„å¯†é’¥ã€‚ä»–æåˆ°ï¼ŒæŸäº›å¯†é’¥çš„è®¿é—®æƒé™å—é™ã€‚
ç¤¾åŒºæˆå‘˜è®¨è®ºäº†ç›¸å…³é…ç½®ï¼Œå¹¶æŒ‡å‡º Cursor åœ¨ä¸Šä¸‹æ–‡ç®¡ç†æ–¹é¢éœ€è¦æ”¹è¿›ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤æ‚é¡¹ç›®æ—¶ã€‚

AI æ¨¡å‹ä¸­çš„ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼šæœ‰ç”¨æˆ·è¡¨è¾¾äº†å¯¹ Cursor ä¸­ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶çš„æ‹…å¿§ï¼Œä»–ä»¬æ›´å€¾å‘äºä½¿ç”¨åƒ Cline æˆ– Google è¿™æ ·èƒ½æä¾›æ›´é•¿ä¸Šä¸‹æ–‡çš„æ¨¡å‹ã€‚
è®¨è®ºèšç„¦äºä¸Šä¸‹æ–‡é•¿åº¦å¯¹ AI æ¨¡å‹æ•ˆæœçš„å½±å“ï¼Œç‰¹åˆ«æ˜¯æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£å¦‚ä½•åœ¨æ›´å¹¿æ³›çš„åº”ç”¨åœºæ™¯ä¸­æå‡æ€§èƒ½ã€‚

ç›¸å…³é“¾æ¥ï¼š

LiveBenchï¼šæš‚æ— æè¿°

AI Testing Tool | Automated AI Testing - Momenticï¼šä½¿ç”¨ Momentic å¼ºå¤§çš„ AI å·¥å…·è¿›è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•ï¼Œæå‡ä½ çš„ QA æµç¨‹ã€‚é€šè¿‡å¯é çš„ AI é©±åŠ¨æµ‹è¯•ï¼Œæ›´å¿«åœ°å‘å¸ƒäº§å“ã€‚

Partial Diff - Visual Studio Marketplaceï¼šVisual Studio Code æ‰©å±• - æ¯”è¾ƒæ–‡ä»¶å†…éƒ¨ã€æ–‡ä»¶ä¹‹é—´æˆ–ä¸å‰ªè´´æ¿çš„æ–‡æœ¬é€‰æ‹©ã€‚

æœªæ‰¾åˆ°æ ‡é¢˜ï¼šæš‚æ— æè¿°

O3-mini is LIVE! ä½ ä»¬ç”¨çš„æ˜¯å“ªä¸ªç‰ˆæœ¬ï¼Ÿï¼šé¦–å…ˆï¼Œå‘ Cursor å›¢é˜Ÿå¿«é€Ÿå‘å¸ƒ O3-mini è‡´æ•¬ï¼å›¢é˜Ÿè¡¨ç¤ºï¼Œä»–ä»¬çš„å¼€å‘äººå‘˜åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸­ä»ç„¶æ›´å–œæ¬¢ Sonnetï¼ˆè¿™è®©ä»–ä»¬æ„Ÿåˆ°æƒŠè®¶ï¼‰ã€‚(æ¥æºï¼šx.comï¼‰æ ¹æ® O...

Model-specific rulesï¼šæ–°çš„ Cursor è§„åˆ™ç³»ç»Ÿå¾ˆæ£’ï¼Œå¦‚æœèƒ½ä¸ºç‰¹å®šæ¨¡å‹è®¾ç½®è§„åˆ™å°±æ›´å¥½äº†ã€‚

New Diff is shown between each line of the previewed codeï¼šBug æè¿°ï¼šåœ¨é¢„è§ˆä»£ç æ—¶ï¼Œæ¯ä¸€è¡Œä»£ç ä¹‹é—´éƒ½ä¼šæ˜¾ç¤ºã€Œæ–°å†…å®¹ã€ã€‚é‡ç°æ­¥éª¤ï¼šåªéœ€è¯¢é—® Cursorã€‚æœŸæœ›è¡Œä¸ºï¼šæ­£å¸¸çš„ä»£ç é¢„è§ˆã€‚å±å¹•æˆªå›¾ / å±å¹•å½•åƒï¼šæ“ä½œç³»ç»Ÿ...

Tweet from Andrej Karpathyï¼ˆ@karpathy)ï¼šæˆ‘å‘ç°äº†ä¸€ç§æ–°çš„ç¼–ç¨‹æ–¹å¼ï¼Œç§°ä¹‹ä¸ºã€Œæ„Ÿè§‰ç¼–ç¨‹ï¼ˆvibe codingï¼‰ã€ã€‚åœ¨è¿™ç§æ¨¡å¼ä¸‹ï¼Œä½ å®Œå…¨æ²‰æµ¸åœ¨æ„Ÿè§‰ä¸­ï¼Œæ‹¥æŠ±æŒ‡æ•°å¢é•¿ï¼Œç”šè‡³å¿˜è®°äº†ä»£ç çš„å­˜åœ¨ã€‚ä¹‹æ‰€ä»¥èƒ½è¿™æ ·ï¼Œæ˜¯å› ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰(ä¾‹å¦‚...

ReleasesÂ·daniel-lxs/mcp-starterï¼šåœ¨ GitHub ä¸Šåˆ›å»ºè´¦æˆ·ï¼Œå‚ä¸ daniel-lxs/mcp-starter çš„å¼€å‘ã€‚

Release v0.1.0Â·daniel-lxs/mcp-starterï¼šåˆå§‹ç‰ˆæœ¬

Documentã€ŒCompare active file with clipboardã€functionalityÂ·Issue #7284Â·microsoft/vscode-docsï¼šVS Code 1.19 å¼•å…¥äº†å°†å½“å‰æ–‡ä»¶ä¸å‰ªè´´æ¿å†…å®¹è¿›è¡Œæ¯”è¾ƒçš„åŠŸèƒ½ã€‚å‘½ä»¤ï¼šæ–‡ä»¶ï¼šæ¯”è¾ƒæ´»åŠ¨æ–‡ä»¶ä¸å‰ªè´´æ¿ï¼ˆworkbench.files.action.compareWithClipboardï¼‰Keybi...

GitHub - robert-at-pretension-io/mcpï¼šcodeï¼šcode. åœ¨ GitHub ä¸Šåˆ›å»ºè´¦æˆ·ï¼Œå‚ä¸ robert-at-pretension-io/mcp çš„å¼€å‘ã€‚

GitHub - daniel-lxs/mcp-perplexityï¼šåœ¨ GitHub ä¸Šåˆ›å»ºè´¦æˆ·ï¼Œå‚ä¸ daniel-lxs/mcp-perplexity çš„å¼€å‘ã€‚

GitHub - daniel-lxs/mcp-starterï¼šåœ¨ GitHub ä¸Šåˆ›å»ºè´¦æˆ·ï¼Œå‚ä¸ daniel-lxs/mcp-starter çš„å¼€å‘ã€‚

Perplexity AI â–· #generalï¼ˆ247 messagesğŸ”¥ğŸ”¥)ï¼š
Perplexity AI ç„¦ç‚¹æ¨¡å¼ï¼ŒPerplexity Pro ä¸­çš„æŸ¥è¯¢å¤„ç†ï¼ŒR1 ä¸å…¶ä»–æ¨¡å‹çš„å¯¹æ¯”ï¼ŒDeepseek çš„æ€§èƒ½é—®é¢˜ï¼Œç”¨æˆ·å¯¹æ¨¡å‹å‚æ•°çš„å…³æ³¨

Perplexity AI's Focus Mode Temporarily Removed: Users discussed the recent removal of the Focus Mode feature in Perplexity AI, with varying reports on whether this change is ongoing or temporary, as stated in some change logs.
Some users expressed frustration over this change and noted that it complicates their ability to specify the source of information, such as requiring prompts to mention Reddit explicitly.
Clarifications on Model Usage in Pro: Questions were raised about how Pro mode interacts with model choices like Claude 3.5 and whether it utilizes reasoning from R1, with insights suggesting that Pro may not use models end-to-end.
It was noted that the actual processing involves undisclosed models for initial searches before passing to selected models like Claude or R1 for final answers.
User Experiences with Performance of R1 and Deepseek: Users have compared the R1 reasoning capabilities of Perplexity with those on Deepseek, noting that Perplexity's version seems to generate more reliable outputs under certain conditions.
Concerns were raised about the speed and quality difference between the available models, particularly with references to the processing power of different configurations.
Issues with Stability in AI Applications: Some users reported experiencing slow performance and stability issues with Perplexity, particularly with the Android app and while using the O3 mini model.
Complaints were directed towards muting discrepancies and inefficiencies in model interactions, prompting discussions on user support responsiveness.
Need for Clear Communication from AI Developers: A common sentiment among users was the desire for greater transparency from Perplexity AI regarding model specifications and updates, especially concerning operational changes.
Users suggested that clarity about model modifications could enhance user experience and reduce confusion when interacting with multiple AI functionalities.
Links mentioned:
George Droyd GIF - George droyd - Discover & Share GIFs: Click to view the GIF
Tweet from mennof (@monnef): Hey AI fans! ğŸ¤– Just wrapped up some DeepSeek R1 testing with a puzzle prompt (3 runs per service)!The tea: @perplexity_ai is blazing fast at processing, while @cursor_ai takes its time to ponder thin...
Perplexity Tech Props: no description found
Ski Equipment Rental Project Questionnaire : no description found
Perplexity AI â–· #sharing (22 messagesğŸ”¥):
Tesla Robotaxi Launch, AI Skills Development Opportunities, USA vs China AI Race, Deepfake Technology from ByteDance, Trans Athlete Executive Order

Perplexity AI çš„ä¸“æ³¨æ¨¡å¼æš‚æ—¶ä¸‹çº¿ï¼šç”¨æˆ·ä»¬æ­£åœ¨è®¨è®º Perplexity AI è¿‘æœŸç§»é™¤ã€Œä¸“æ³¨æ¨¡å¼ï¼ˆFocus Modeï¼‰ã€åŠŸèƒ½ä¸€äº‹ã€‚æœ‰ç”¨æˆ·è¡¨ç¤ºï¼Œæ ¹æ®ä¸€äº›æ›´æ–°æ—¥å¿—æ˜¾ç¤ºï¼Œè¿™æ¬¡å˜åŠ¨å¯èƒ½æ˜¯æš‚æ—¶çš„ï¼Œä½†ä¹Ÿæœ‰äº›ç”¨æˆ·è®¤ä¸ºä¼šæ˜¯æ°¸ä¹…æ€§çš„ã€‚
ä¸€äº›ç”¨æˆ·å¯¹æ­¤æ„Ÿåˆ°ä¸æ»¡ï¼Œè®¤ä¸ºè¿™è®©ä»–ä»¬æ›´éš¾æŒ‡å®šä¿¡æ¯æ¥æºï¼Œä¾‹å¦‚ç°åœ¨å¿…é¡»åœ¨æç¤ºè¯­ä¸­æ˜ç¡®æåŠ Reddit æ‰è¡Œã€‚
å…³äº Pro ç‰ˆæœ¬ä¸­çš„æ¨¡å‹ä½¿ç”¨ï¼šæœ‰ç”¨æˆ·æé—®ï¼ŒPro ç‰ˆæœ¬æ˜¯å¦‚ä½•ä¸ Claude 3.5 è¿™æ ·çš„æ¨¡å‹è¿›è¡Œäº¤äº’çš„ï¼Ÿå®ƒæ˜¯å¦ä¼šåˆ©ç”¨ R1 æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Ÿ æœ‰åˆ†ææŒ‡å‡ºï¼ŒPro ç‰ˆæœ¬å¯èƒ½å¹¶éå®Œå…¨ä¾èµ–è¿™äº›æ¨¡å‹æ¥å®Œæˆä»»åŠ¡ã€‚
æ®äº†è§£ï¼Œå®é™…çš„å¤„ç†æµç¨‹æ˜¯ï¼Œå…ˆé€šè¿‡ä¸€äº›æœªå…¬å¼€çš„æ¨¡å‹è¿›è¡Œåˆæ­¥æœç´¢ï¼Œç„¶åå†å°†ç»“æœä¼ é€’ç»™ Claude æˆ– R1 è¿™æ ·çš„æŒ‡å®šæ¨¡å‹ï¼Œä»¥ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚
ç”¨æˆ·å¯¹ R1 å’Œ Deepseek æ€§èƒ½çš„ä½“éªŒï¼šæœ‰ç”¨æˆ·å¯¹æ¯”äº† Perplexity AI çš„ R1 æ¨¡å‹å’Œ Deepseek æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå‘ç° Perplexity AI çš„ R1 æ¨¡å‹åœ¨ç‰¹å®šæƒ…å†µä¸‹ä¼¼ä¹èƒ½äº§ç”Ÿæ›´å¯é çš„ç»“æœã€‚
ç”¨æˆ·ä»¬ä¹Ÿå¯¹ä¸åŒæ¨¡å‹ä¹‹é—´çš„é€Ÿåº¦å’Œè´¨é‡å·®å¼‚è¡¨ç¤ºå…³æ³¨ï¼Œç‰¹åˆ«æ˜¯è€ƒè™‘åˆ°ä¸åŒé…ç½®çš„å¤„ç†èƒ½åŠ›å·®å¼‚ã€‚
AI åº”ç”¨çš„ç¨³å®šæ€§é—®é¢˜ï¼šéƒ¨åˆ†ç”¨æˆ·åé¦ˆï¼ŒPerplexity AI åœ¨ä½¿ç”¨è¿‡ç¨‹ä¸­å­˜åœ¨æ€§èƒ½ç¼“æ…¢å’Œç¨³å®šæ€§é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨ Android åº”ç”¨å’Œ O3 mini æ¨¡å‹æ—¶ã€‚
ç”¨æˆ·æŠ±æ€¨æ¨¡å‹äº¤äº’ä¸­å‡ºç°çš„å„ç§é—®é¢˜å’Œæ•ˆç‡ä½ä¸‹çš„æƒ…å†µï¼Œå¹¶å¼•å‘äº†å…³äºç”¨æˆ·æ”¯æŒå“åº”é€Ÿåº¦çš„è®¨è®ºã€‚
å¸Œæœ› AI å¼€å‘è€…åŠ å¼ºæ²Ÿé€šï¼šè®¸å¤šç”¨æˆ·å¸Œæœ› Perplexity AI èƒ½å¤Ÿæ›´é€æ˜åœ°å…¬å¸ƒæ¨¡å‹è§„æ ¼å’Œæ›´æ–°ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å…³äºæ“ä½œå˜æ›´æ–¹é¢ã€‚
ç”¨æˆ·è®¤ä¸ºï¼Œå¦‚æœèƒ½æ¸…æ¥šåœ°äº†è§£æ¨¡å‹ä¿®æ”¹æƒ…å†µï¼Œå°†æœ‰åŠ©äºæ”¹å–„ç”¨æˆ·ä½“éªŒï¼Œå¹¶å‡å°‘åœ¨ä½¿ç”¨å¤šç§ AI åŠŸèƒ½æ—¶äº§ç”Ÿçš„å›°æƒ‘ã€‚

æåˆ°çš„é“¾æ¥ï¼š
George Droyd GIF - George droyd - Discover & Share GIFsï¼šç‚¹å‡»æŸ¥çœ‹ GIF
mennofï¼ˆ@monnefï¼‰çš„æ¨æ–‡ï¼š å˜¿ï¼ŒAI çˆ±å¥½è€…ä»¬ï¼ğŸ¤– åˆšåˆšç”¨è°œé¢˜æç¤ºå®Œæˆäº† DeepSeek R1 çš„ä¸€äº›æµ‹è¯•ï¼ˆæ¯ä¸ªæœåŠ¡è¿è¡Œ 3 æ¬¡)ï¼æœ€æ–°æ¶ˆæ¯ï¼š@perplexity_ai çš„å¤„ç†é€Ÿåº¦éå¸¸å¿«ï¼Œè€Œ @cursor_ai åˆ™éœ€è¦æ—¶é—´æ¥ä»”ç»†æ€è€ƒ...
Perplexity Tech Propsï¼šæš‚æ— æè¿°
Ski Equipment Rental Project Questionnaireï¼šæš‚æ— æè¿°
Perplexity AI â–· #sharingï¼ˆ22 æ¡æ¶ˆæ¯ğŸ”¥)ï¼š
Tesla Robotaxi å‘å¸ƒã€AI æŠ€èƒ½å‘å±•æœºä¼šã€ç¾å›½ vs ä¸­å›½ AI ç«èµ›ã€æ¥è‡ª ByteDance çš„ Deepfake æŠ€æœ¯ã€è·¨æ€§åˆ«è¿åŠ¨å‘˜è¡Œæ”¿å‘½ä»¤

Tesla Robotaxi Launch set for June: Perplexity AI announced that Tesla Robotaxi will launch in June, marking a significant advancement in autonomous technology.
A YouTube video shared on this topic discussed the implications of this launch for the AI and automotive industries.
Explore AI Skills Development: A comprehensive thread was shared discussing various opportunities for developing AI skills suitable for all levels from beginner to master.
The discussion provides insights on harnessing one's true potential within the AI field.
Detailed Overview of USA vs China AI Race: An intricate thread on the USA vs China AI Race presents collected information, with sources provided for verification.
The author highlights the challenges in obtaining openly acknowledged information in this competitive landscape.
ByteDance Releases New Deepfake Technology: A report on ByteDance's latest endeavor revealed the release of a deepfake tool, which raises various discussions on ethical implications.
As part of this release, the community speculated on potential uses and misuses of deepfake technology.
Executive Order Bans Trans Athletes in Sports: A recent executive order banning Trans athletes from competing in certain sports generated substantial debate within the community.
Members discussed the broader implications of this order on the sports industry and civil rights.
Link mentioned: YouTube: no description found

ç‰¹æ–¯æ‹‰ Robotaxi å°†äº 6 æœˆå‘å¸ƒï¼šPerplexity AI å®£å¸ƒç‰¹æ–¯æ‹‰ Robotaxi å°†äº 6 æœˆå‘å¸ƒï¼Œè¿™æ ‡å¿—ç€è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„é‡å¤§è¿›å±•ã€‚
ä¸€ä¸ª YouTube è§†é¢‘è®¨è®ºäº†è¿™æ¬¡å‘å¸ƒå¯¹äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å’Œæ±½è½¦è¡Œä¸šçš„å½±å“ã€‚
æ¢ç´¢ AI æŠ€èƒ½å‘å±•ï¼šä¸€ä¸ªå…¨é¢çš„å¸–å­åˆ†äº«äº†å„ç§ AI æŠ€èƒ½æå‡çš„æœºä¼šï¼Œæ— è®ºä½ æ˜¯åˆå­¦è€…è¿˜æ˜¯ä¸“å®¶ï¼Œéƒ½èƒ½æ‰¾åˆ°é€‚åˆè‡ªå·±çš„å†…å®¹ã€‚
è®¨è®ºæ·±å…¥æ¢è®¨äº†å¦‚ä½•åœ¨ AI é¢†åŸŸå……åˆ†å‘æŒ¥ä¸ªäººæ½œåŠ›ã€‚
ç¾å›½ vs ä¸­å›½ AI ç«èµ›æ·±åº¦åˆ†æï¼šä¸€ç¯‡å…³äºç¾å›½ä¸ä¸­å›½ AI ç«èµ›çš„å¸–å­ï¼Œæ±‡æ€»äº†å„æ–¹ä¿¡æ¯ï¼Œå¹¶æä¾›äº†å¯ä¾›æŸ¥è¯çš„æ¥æºã€‚
ä½œè€…å¼ºè°ƒï¼Œåœ¨è¿™ä¸ªç«äº‰æ¿€çƒˆçš„é¢†åŸŸï¼Œå…¬å¼€ä¿¡æ¯çš„è·å–é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚
å­—èŠ‚è·³åŠ¨å‘å¸ƒå…¨æ–° Deepfake æŠ€æœ¯ï¼šä¸€ç¯‡æŠ¥é“ç§°ï¼Œå­—èŠ‚è·³åŠ¨å‘å¸ƒäº†ä¸€æ¬¾æ–°çš„ deepfake å·¥å…·ï¼Œå¼•å‘äº†å…³äºå…¶ä¼¦ç†å½±å“çš„å¹¿æ³›è®¨è®ºã€‚
ç¤¾åŒºæˆå‘˜å¯¹ deepfake æŠ€æœ¯çš„æ½œåœ¨ç”¨é€”å’Œæ»¥ç”¨è¿›è¡Œäº†æ¨æµ‹ã€‚
è¡Œæ”¿å‘½ä»¤ç¦æ­¢è·¨æ€§åˆ«è¿åŠ¨å‘˜å‚ä¸ä½“è‚²èµ›äº‹ï¼šä¸€é¡¹ç¦æ­¢è·¨æ€§åˆ«è¿åŠ¨å‘˜å‚åŠ ç‰¹å®šä½“è‚²èµ›äº‹çš„æœ€æ–°è¡Œæ”¿å‘½ä»¤ï¼Œåœ¨ç¤¾ç¾¤ä¸­å¼•å‘äº†æ¿€çƒˆçš„äº‰è®ºã€‚
å¤§å®¶è®¨è®ºäº†è¿™é¡¹å‘½ä»¤å¯¹ä½“è‚²äº§ä¸šå’Œå…¬æ°‘æƒåˆ©çš„æ·±è¿œå½±å“ã€‚
ç›¸å…³é“¾æ¥ï¼šYouTubeï¼šæš‚æ— æè¿°

Perplexity AI â–· #pplx-api (7 messages):
Perplexity API usage, Sonar Pro Reasoning devs, Image uploading limitations, Monthly cost limits and invoicing

Questions about Monthly Cost Limits: A member inquired whether they can set a hard limit on how much money is used per month with the Perplexity API.
They also asked if invoices are sent for costs incurred or if charges need to be added manually.
Exploring Image Upload Workarounds: A new user expressed interest in the Perplexity API for an app they are building but noted that the current API seems to lack image uploading capabilities.
They proposed a workaround using Claude for detailed descriptions before sending prompts to Perplexity for output.
Urgent Request for Sonar Pro Reasoning Devs: Multiple messages indicated an urgent need to contact the Sonar Pro reasoning developers due to a security issue discovered by a member.
Another member directed them to send an email to api@perplexity.ai for assistance.
OpenRouter (Alex Atallah) â–· #announcements (14 messagesğŸ”¥):
DeepSeek Insurance, Kluster integration issues, Qwen model deprecation, Website downtime update

Perplexity AI â–· #pplx-apiï¼ˆ7 æ¡æ¶ˆæ¯)ï¼š
Perplexity API ä½¿ç”¨æƒ…å†µã€Sonar Pro æ¨ç†åŠŸèƒ½å¼€å‘è€…ã€å›¾ç‰‡ä¸Šä¼ é™åˆ¶ã€æ¯æœˆè´¹ç”¨é™åˆ¶å’Œå‘ç¥¨å…³äºæ¯æœˆè´¹ç”¨é™åˆ¶çš„é—®é¢˜ï¼šä¸€ä½æˆå‘˜è¯¢é—®æ˜¯å¦å¯ä»¥å¯¹æ¯æœˆä½¿ç”¨ Perplexity API çš„é‡‘é¢è®¾ç½®ç¡¬æ€§é™åˆ¶ã€‚
ä»–ä»¬è¿˜è¯¢é—®æ˜¯å¦ä¼šå‘é€å·²äº§ç”Ÿè´¹ç”¨çš„å‘ç¥¨ï¼Œæˆ–è€…æ˜¯å¦éœ€è¦æ‰‹åŠ¨æ·»åŠ è´¹ç”¨ã€‚
æ¢ç´¢å›¾ç‰‡ä¸Šä¼ çš„æ›¿ä»£æ–¹æ¡ˆï¼šä¸€ä½æ–°ç”¨æˆ·è¡¨ç¤ºå¯¹ Perplexity API æ„Ÿå…´è¶£ï¼Œå¹¶è®¡åˆ’å°†å…¶ç”¨äºæ­£åœ¨æ„å»ºçš„åº”ç”¨ç¨‹åºï¼Œä½†æ³¨æ„åˆ°å½“å‰çš„ API ä¼¼ä¹ç¼ºä¹å›¾ç‰‡ä¸Šä¼ åŠŸèƒ½ã€‚
ä»–ä»¬æå‡ºäº†ä¸€ä¸ªæ›¿ä»£æ–¹æ¡ˆï¼Œå³ä½¿ç”¨ Claude æ¥è¿›è¡Œè¯¦ç»†æè¿°ï¼Œç„¶åå†å°†æç¤ºå‘é€åˆ° Perplexity ä»¥è·å¾—è¾“å‡ºã€‚
ç´§æ€¥è”ç³» Sonar Pro æ¨ç†åŠŸèƒ½çš„å¼€å‘è€…ï¼šå¤šæ¡æ¶ˆæ¯è¡¨æ˜ï¼Œç”±äºä¸€ä½æˆå‘˜å‘ç°çš„å®‰å…¨é—®é¢˜ï¼Œè¿«åˆ‡éœ€è¦è”ç³» Sonar Pro æ¨ç†åŠŸèƒ½å¼€å‘è€…ã€‚
å¦ä¸€ä½æˆå‘˜æŒ‡ç¤ºä»–ä»¬å‘é€ç”µå­é‚®ä»¶è‡³ api@perplexity.ai ä»¥è·å¾—å¸®åŠ©ã€‚
OpenRouterï¼ˆAlex Atallahï¼‰â–· #announcementsï¼ˆ14 æ¡æ¶ˆæ¯ğŸ”¥)ï¼š
DeepSeek ä¿é™©ã€Kluster é›†æˆé—®é¢˜ã€Qwen æ¨¡å‹åœç”¨ã€ç½‘ç«™åœæœºæ›´æ–°

DeepSeek Insurance Now Covers No Completion Tokens: OpenRouter will now insure DeepSeek R1 requests that receive no completion tokens, ensuring no charge even if the upstream provider does.
The completion rate for Standard DeepSeek R1 has improved from 60% to 96% over time.
Kluster Integration Issues Resolved: A user explained a situation where completion tokens were delayed by Kluster, leading to unexpected charges despite apparent timeouts on OpenRouter's end.
They discovered that Kluster was failing to cancel requests when timing out, but this issue has since been addressed.
Qwen Models Being Deprecated by Novita: Novita will be deprecating their Qwen/Qwen-2-72B-Instruct model, with OpenRouter disabling it around the same time.
Users should make sure to transition away from this model before the deprecation date.
OpenRouter Website Experiences Downtime: OpenRouter experienced a minor downtime due to their authentication provider being down, affecting website access but not the API.
The issue was resolved within approximately 15 minutes and services were restored.
Links mentioned:
R1 - API, Providers, Stats: DeepSeek R1 is here: Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass. Ru...
R1 (nitro) - API, Providers, Stats: DeepSeek R1 is here: Performance on par with [OpenAI o1](/openai/o1), but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass. Ru...
OpenRouter (Alex Atallah) â–· #app-showcase (1 messages):
Y CLI Development, Terminal Enthusiasm, Chat Data Management, MCP Client Support, Deepseek-r1 Integration

DeepSeek ç°åœ¨ä¸ºæ²¡æœ‰ç”Ÿæˆç»“æœçš„è¯·æ±‚æä¾›ä¿éšœï¼šOpenRouter ç°åœ¨å°†ä¸ºæœªæ”¶åˆ°è¡¥å…¨ Tokensï¼ˆCompletion Tokensï¼‰çš„ DeepSeek R1 è¯·æ±‚æä¾›ä¿é™©ï¼Œç¡®ä¿å³ä½¿ä¸Šæ¸¸æä¾›å•†æ”¶è´¹ä¹Ÿä¸ä¼šäº§ç”Ÿè´¹ç”¨ã€‚
Standard DeepSeek R1 çš„è¡¥å…¨ç‡éšç€æ—¶é—´çš„æ¨ç§»ä» 60% æé«˜åˆ° 96%ã€‚
Kluster é›†æˆé—®é¢˜å·²è§£å†³ï¼šä¸€ä½ç”¨æˆ·è§£é‡Šäº†ä¸€ç§æƒ…å†µï¼Œå³è¡¥å…¨ Tokens è¢« Kluster å»¶è¿Ÿï¼Œå¯¼è‡´æ„å¤–æ”¶è´¹ï¼Œå°½ç®¡ OpenRouter æ–¹é¢æ˜¾ç¤ºè¶…æ—¶ã€‚
ä»–ä»¬å‘ç° Kluster åœ¨è¶…æ—¶æ—¶æœªèƒ½å–æ¶ˆè¯·æ±‚ï¼Œä½†æ­¤é—®é¢˜å·²å¾—åˆ°è§£å†³ã€‚
Novita å°†è¦å¼ƒç”¨ Qwen æ¨¡å‹ï¼šNovita å°†å¼ƒç”¨ä»–ä»¬çš„ Qwen/Qwen-2-72B-Instruct æ¨¡å‹ï¼ŒOpenRouter å°†åœ¨å¤§çº¦åŒä¸€æ—¶é—´ç¦ç”¨å®ƒã€‚
ç”¨æˆ·åº”ç¡®ä¿åœ¨å¼ƒç”¨æ—¥æœŸä¹‹å‰è¿‡æ¸¡åˆ°å…¶ä»–æ¨¡å‹ã€‚
OpenRouter ç½‘ç«™é­é‡åœæœºï¼šOpenRouter å› å…¶èº«ä»½éªŒè¯æœåŠ¡æä¾›å•†å®•æœºè€Œé‡åˆ°è½»å¾®åœæœºï¼Œå½±å“äº†ç½‘ç«™è®¿é—®ï¼Œä½† API æ¥å£ä¸å—å½±å“ã€‚
è¯¥é—®é¢˜åœ¨å¤§çº¦ 15 åˆ†é’Ÿå†…å¾—åˆ°è§£å†³ï¼ŒæœåŠ¡å·²æ¢å¤ã€‚
æåˆ°çš„é“¾æ¥:
R1 - APIï¼ŒProvidersï¼ŒStatsï¼šDeepSeek R1 is hereï¼šPerformance on par with [OpenAI o1](/openai/o1ï¼‰ï¼Œbut open-sourced and with fully open reasoning tokens. It's 671B parameters in sizeï¼Œwith 37B active in an inference pass. Ru...
R1ï¼ˆnitroï¼‰- APIï¼ŒProvidersï¼ŒStatsï¼šDeepSeek R1 is hereï¼šPerformance on par with [OpenAI o1](/openai/o1ï¼‰ï¼Œbut open-sourced and with fully open reasoning tokens. It's 671B parameters in sizeï¼Œwith 37B active in an inference pass. Ru...
OpenRouterï¼ˆAlex Atallahï¼‰â–· #app-showcaseï¼ˆ1 messages):
Y CLI å¼€å‘ï¼Œç»ˆç«¯çƒ­æƒ…ï¼ŒèŠå¤©æ•°æ®ç®¡ç†ï¼ŒMCP å®¢æˆ·ç«¯æ”¯æŒï¼ŒDeepseek-r1 é›†æˆ

Y CLI emerges as an open router chat alternative: A personal project, Y CLI, aims to provide a web chat alternative with all chat data stored in single jsonl files.
You can check out the project on its GitHub page.
MCP client support showcased: The project includes support for an MCP client, demonstrated in this asciinema recording capturing its functionality on macOS.
This recording received 4 views and showcases xterm-256color and zsh in action.
Deepseek-r1 reasoning support added: Another feature of Y CLI is the Deepseek-r1 reasoning content support, evidenced in this asciinema recording.
This demo also runs on macOS with 2 views and supports the xterm-256color and zsh terminal setup.
Github encourages contributions: Developers are invited to contribute to Y CLI via its GitHub repository.
The page highlights the ongoing development efforts and user contributions illustrated in this GitHub overview.
A call for terminal fans: The developer expressed interest in finding fellow terminal fans within the community.
The project aims to attract those who appreciate terminal-based tools and configurations.
Links mentioned:
y-cli mcp client: https://github.com/luohy15/y-cli
y-cli reasoning content: https://github.com/luohy15/y-cli
GitHub - luohy15/y-cli: Contribute to luohy15/y-cli development by creating an account on GitHub.
OpenRouter (Alex Atallah) â–· #general (242 messagesğŸ”¥ğŸ”¥):
DeepInfra issues, Gemini 2.0 Flash readiness, OpenRouter authentication service, Error handling with models, Provider performance discrepancies

Y CLIï¼šä¸€ä¸ªæ–°å…´çš„å¼€æºèŠå¤©å·¥å…·ï¼Œæ—¨åœ¨æ›¿ä»£ OpenRouter çš„èŠå¤©åŠŸèƒ½ã€‚ä½œä¸ºä¸€ä¸ªä¸ªäººé¡¹ç›®ï¼ŒY CLI è‡´åŠ›äºæä¾›ä¸€ç§ Web èŠå¤©æ›¿ä»£æ–¹æ¡ˆï¼Œå…¶æ‰€æœ‰èŠå¤©æ•°æ®éƒ½å­˜å‚¨åœ¨ç‹¬ç«‹çš„ JSONL æ–‡ä»¶ï¼ˆJSON Lines text formatï¼‰ä¸­ã€‚
ä½ å¯ä»¥åœ¨ GitHub é¡µé¢ä¸Šæ‰¾åˆ°è¯¥é¡¹ç›®ã€‚

MCP å®¢æˆ·ç«¯æ”¯æŒæ¼”ç¤ºï¼šè¯¥é¡¹ç›®æ”¯æŒ MCP å®¢æˆ·ç«¯ã€‚ä¸€æ®µ Asciinema å½•åƒå±•ç¤ºäº†å…¶åœ¨ macOS ä¸Šçš„åŠŸèƒ½ã€‚
è¿™æ®µå½•åƒå·²è¢«è§‚çœ‹ 4 æ¬¡ï¼Œå±•ç¤ºäº† xterm-256color å’Œ zsh çš„å®é™…æ•ˆæœã€‚

Deepseek-r1 æ¨ç†èƒ½åŠ›æ”¯æŒï¼šY CLI çš„å¦ä¸€å¤§äº®ç‚¹æ˜¯æ”¯æŒ Deepseek-r1 çš„æ¨ç†èƒ½åŠ›ã€‚è¿™æ®µ Asciinema å½•åƒå¯¹æ­¤è¿›è¡Œäº†æ¼”ç¤ºã€‚
è¯¥æ¼”ç¤ºåŒæ ·åœ¨ macOS ä¸Šè¿è¡Œï¼Œå·²è¢«è§‚çœ‹ 2 æ¬¡ï¼Œå¹¶æ”¯æŒ xterm-256color å’Œ zsh ç»ˆç«¯é…ç½®ã€‚

GitHub é¼“åŠ±å¤§å®¶å‚ä¸ Y CLI çš„å¼€å‘ï¼šæ¬¢è¿å¼€å‘è€…ä»¬é€šè¿‡ GitHub ä»“åº“ä¸º Y CLI è´¡çŒ®ä»£ç ã€‚
é¡¹ç›®é¡µé¢å±•ç¤ºäº†æ­£åœ¨è¿›è¡Œçš„å¼€å‘å·¥ä½œå’Œç”¨æˆ·çš„ç§¯æå‚ä¸ã€‚

å¯»æ‰¾ç»ˆç«¯çˆ±å¥½è€…ï¼šå¼€å‘è€…å¸Œæœ›èƒ½åœ¨ç¤¾åŒºä¸­æ‰¾åˆ°åŒæ ·çƒ­çˆ±ç»ˆç«¯çš„ä¼™ä¼´ã€‚
è¯¥é¡¹ç›®æ—¨åœ¨å¸å¼•é‚£äº›å–œæ¬¢åŸºäºç»ˆç«¯çš„å·¥å…·å’Œé…ç½®çš„å¼€å‘è€…å’Œç”¨æˆ·ã€‚

ç›¸å…³é“¾æ¥ï¼š
y-cli mcp clientï¼šhttps://github.com/luohy15/y-cli
y-cli reasoning contentï¼šhttps://github.com/luohy15/y-cli
GitHub - luohy15/y-cliï¼šé€šè¿‡åœ¨ GitHub ä¸Šåˆ›å»ºä¸€ä¸ªå¸æˆ·æ¥ä¸º luohy15/y-cli çš„å¼€å‘åšå‡ºè´¡çŒ®ã€‚
OpenRouterï¼ˆAlex Atallahï¼‰â–· #generalï¼ˆ242 messagesğŸ”¥ğŸ”¥)ï¼šè®¨è®ºå†…å®¹åŒ…æ‹¬ DeepInfra é—®é¢˜ã€Gemini 2.0 Flash çš„å‡†å¤‡æƒ…å†µã€OpenRouter èº«ä»½éªŒè¯æœåŠ¡ã€æ¨¡å‹åº”ç”¨ä¸­çš„é”™è¯¯å¤„ç†ä»¥åŠä¸åŒæœåŠ¡å•†çš„æ€§èƒ½å·®å¼‚ç­‰ç­‰ã€‚

DeepInfra experiencing failures: Users reported that DeepInfra is currently failing to return responses 50% of the time due to an increase in processing delays.
Some users are seeing zero token completions when utilizing DeepInfra with applications like SillyTavern.
Gemini 2.0 Flash model integration concerns: There are discussions around issues with the Gemini 2.0 Flash model regarding its incompatibility with tool calling.
Users are filing issues as they encounter errors stating that tool invocation must have a return result, but it works fine with other models.
Authentication service downtime: OpenRouter experienced downtime due to issues with their authentication service provided by Clerk, Inc.
Although the website faced challenges, the API remained operational for users, with updates being shared regarding the status.
Error identification with models: Users reported discrepancies and errors when utilizing different models, such as Mistral and Novita AI.
Issues include one model returning unusually high token counts and another causing frequent processing failures.
General discussions about provider performances: The community is sharing observations about performance differences between models and providers, including suggestions for improvements.
There is a call for better mechanisms to handle errors and optimize responses to streamline user experiences with AI models.
Links mentioned:
"The caller does not have permission" when creating API key: I'm using MakerSuite with Gemini and I deleted an API Key. I went to create a new one, but I'm getting an error saying the caller does not have permission. What does that mean and how can I ...
OpenRouter: A unified interface for LLMs. Find the best models & prices for your prompts
Tweet from Logan Kilpatrick (@OfficialLoganK): @HCSolakoglu Vertex customers tend to skew larger enterprise customers and have flexibility to negotiate things like bulk discounts, etc. This does not apply to the Gemini Developer API, everyone pays...
Gemini Flash 2.0 - API, Providers, Stats: Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1. Run Gemini Flash 2.0 with API
Google AI Studio | OpenRouter: Browse models provided by Google AI Studio
CleanShot 2025-02-06 at 11â€¯.21.37: Screenshot uploaded to CleanShot Cloud
Provider Routing â€” OpenRouter | Documentation: Route requests to the best provider
CleanShot 2025-02-06 at 10â€¯.58.39: Screenshot uploaded to CleanShot Cloud
OpenRouterTeam/ai-sdk-provider: The OpenRouter provider for the Vercel AI SDK contains support for hundreds of AI models through the OpenRouter chat and completion APIs. - OpenRouterTeam/ai-sdk-provider
no title found: no description found
Clerk, Inc. status : no description found
GitHub - OpenRouterTeam/ai-sdk-provider: The OpenRouter provider for the Vercel AI SDK contains support for hundreds of AI models through the OpenRouter chat and completion APIs.: The OpenRouter provider for the Vercel AI SDK contains support for hundreds of AI models through the OpenRouter chat and completion APIs. - OpenRouterTeam/ai-sdk-provider
LM Studio â–· #general (215 messagesğŸ”¥ğŸ”¥):
LM Studio API error handling, Model performance inquiries, Obsidian Smart Connections integration, Updating AI models and features, Safety of downloading AI models from TheBloke

DeepInfra é­é‡æ•…éšœï¼šç”¨æˆ·åé¦ˆï¼Œç”±äºå¤„ç†å»¶è¿Ÿå¢åŠ ï¼ŒDeepInfra ç›®å‰çš„å“åº”å¤±è´¥ç‡é«˜è¾¾ 50%ã€‚
éƒ¨åˆ†ç”¨æˆ·åœ¨ä½¿ç”¨ SillyTavern ç­‰åº”ç”¨ï¼Œé€šè¿‡ DeepInfra è¿›è¡Œäº¤äº’æ—¶ï¼Œé‡åˆ°é›¶ Token å®Œæˆçš„æƒ…å†µã€‚
Gemini 2.0 Flash æ¨¡å‹é›†æˆé—®é¢˜ï¼šæœ‰è®¨è®ºæŒ‡å‡ºï¼ŒGemini 2.0 Flash æ¨¡å‹ä¸å·¥å…·è°ƒç”¨åŠŸèƒ½å­˜åœ¨ä¸å…¼å®¹é—®é¢˜ã€‚
ç”¨æˆ·æäº¤çš„ issue æ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹æŠ¥é”™ï¼Œæç¤ºå·¥å…·è°ƒç”¨å¿…é¡»è¿”å›ç»“æœï¼Œä½†åŒæ ·çš„è°ƒç”¨åœ¨å…¶ä»–æ¨¡å‹ä¸Šå´èƒ½æ­£å¸¸å·¥ä½œã€‚
èº«ä»½éªŒè¯æœåŠ¡ä¸­æ–­ï¼šOpenRouter æ›¾å›  Clerkï¼ŒInc. æä¾›çš„èº«ä»½éªŒè¯æœåŠ¡å‡ºç°æ•…éšœè€Œå®•æœºã€‚
å°½ç®¡ç½‘ç«™å—åˆ°å½±å“ï¼ŒAPI æ¥å£ä»ç„¶ä¿æŒå¯ç”¨ï¼Œå¹¶åŠæ—¶åˆ†äº«äº†çŠ¶æ€æ›´æ–°ã€‚
æ¨¡å‹é”™è¯¯è¯†åˆ«ï¼šç”¨æˆ·æŠ¥å‘Šç§°ï¼Œåœ¨ä½¿ç”¨ Mistral å’Œ Novita AI ç­‰ä¸åŒæ¨¡å‹æ—¶ï¼Œå‡ºç°äº†ä¸ä¸€è‡´å’Œé”™è¯¯ã€‚
é—®é¢˜åŒ…æ‹¬ï¼šæŸä¸ªæ¨¡å‹è¿”å›çš„ Token æ•°é‡å¼‚å¸¸åé«˜ï¼Œä»¥åŠå¦ä¸€ä¸ªæ¨¡å‹é¢‘ç¹å‡ºç°å¤„ç†å¤±è´¥ã€‚
å…³äºæœåŠ¡å•†æ€§èƒ½çš„è®¨è®ºï¼šç¤¾åŒºæ­£åœ¨åˆ†äº«å…³äºä¸åŒæ¨¡å‹å’ŒæœåŠ¡å•†ä¹‹é—´æ€§èƒ½å·®å¼‚çš„è§‚å¯Ÿï¼Œå¹¶æå‡ºäº†æ”¹è¿›å»ºè®®ã€‚
å¤§å®¶å¸Œæœ›æœ‰æ›´å¥½çš„æœºåˆ¶æ¥å¤„ç†é”™è¯¯å’Œä¼˜åŒ–å“åº”ï¼Œä»è€Œæå‡ç”¨æˆ·ä½¿ç”¨ AI æ¨¡å‹çš„ä½“éªŒã€‚
ç›¸å…³é“¾æ¥ï¼š
åˆ›å»º API å¯†é’¥æ—¶æç¤ºã€Œè°ƒç”¨è€…æ²¡æœ‰æƒé™ã€ï¼šæˆ‘æ­£åœ¨ä½¿ç”¨ MakerSuite å’Œ Geminiï¼Œå¹¶ä¸”åˆ é™¤äº†ä¸€ä¸ª API å¯†é’¥ã€‚å½“æˆ‘å°è¯•åˆ›å»ºä¸€ä¸ªæ–°çš„å¯†é’¥æ—¶ï¼Œå´å‡ºç°é”™è¯¯ï¼Œæç¤ºæˆ‘æ²¡æœ‰æƒé™ã€‚è¿™æ˜¯ä»€ä¹ˆåŸå› ï¼Œæˆ‘è¯¥å¦‚ä½•è§£å†³ï¼Ÿ
OpenRouterï¼šå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM/Large Language Modelï¼‰çš„ç»Ÿä¸€æ¥å£ã€‚å¯»æ‰¾æœ€é€‚åˆæ‚¨æç¤ºçš„æ¨¡å‹å’Œä»·æ ¼ã€‚
Logan Kilpatrickï¼ˆ@OfficialLoganKï¼‰çš„æ¨æ–‡ï¼š@HCSolakoglu Vertex çš„å®¢æˆ·é€šå¸¸æ˜¯å¤§å‹ä¼ä¸šå®¢æˆ·ï¼Œä»–ä»¬å¯ä»¥çµæ´»åœ°åå•†æ‰¹é‡æŠ˜æ‰£ç­‰ã€‚è¿™ä¸é€‚ç”¨äº Gemini Developer APIï¼Œæ‰€æœ‰äººéƒ½æŒ‰ç»Ÿä¸€ä»·æ ¼ä»˜è´¹...
Gemini Flash 2.0 - APIã€æœåŠ¡å•†ã€ç»Ÿè®¡ä¿¡æ¯ï¼šä¸ Gemini Flash 1 ç›¸æ¯”ï¼ŒGemini Flash 2.0 æ˜¾è‘—ç¼©çŸ­äº†é¦–ä¸ª Token æ—¶é—´ï¼ˆTTFTï¼‰ã€‚é€šè¿‡ API è¿è¡Œ Gemini Flash 2.0ã€‚
Google AI Studio | OpenRouterï¼šæµè§ˆ Google AI Studio æä¾›çš„æ¨¡å‹ã€‚
CleanShot 2025-02-06 at 11â€¯.21.37ï¼šå±å¹•æˆªå›¾å·²ä¸Šä¼ åˆ° CleanShot Cloudã€‚
Provider Routing â€” OpenRouter | æ–‡æ¡£ï¼šå°†è¯·æ±‚è·¯ç”±åˆ°æœ€ä½³æœåŠ¡å•†ã€‚
CleanShot 2025-02-06 at 10â€¯.58.39ï¼šå±å¹•æˆªå›¾å·²ä¸Šä¼ åˆ° CleanShot Cloudã€‚
OpenRouterTeam/ai-sdk-providerï¼šVercel AI SDK çš„ OpenRouter æœåŠ¡å•†ï¼Œé€šè¿‡ OpenRouter èŠå¤©å’Œè¡¥å…¨ APIï¼Œæ”¯æŒæ•°ç™¾ä¸ª AI æ¨¡å‹ã€‚- OpenRouterTeam/ai-sdk-provider
OpenRouterTeam/ai-sdk-providerï¼šVercel AI SDK çš„ OpenRouter æœåŠ¡å•†ã€‚
Clerkï¼ŒInc. status ï¼šClerkï¼ŒInc. çš„æœåŠ¡çŠ¶æ€ã€‚
GitHub - OpenRouterTeam/ai-sdk-providerï¼šVercel AI SDK çš„ OpenRouter æœåŠ¡å•†ï¼Œé€šè¿‡ OpenRouter èŠå¤©å’Œè¡¥å…¨ APIï¼Œæ”¯æŒæ•°ç™¾ä¸ª AI æ¨¡å‹ã€‚- OpenRouterTeam/ai-sdk-provider
LM Studio â–· #generalï¼ˆ215 messagesğŸ”¥ğŸ”¥)ï¼š
LM Studio API é”™è¯¯å¤„ç†ã€æ¨¡å‹æ€§èƒ½å’¨è¯¢ã€Obsidian Smart Connections é›†æˆã€AI æ¨¡å‹å’ŒåŠŸèƒ½æ›´æ–°ã€ä» TheBloke ä¸‹è½½ AI æ¨¡å‹çš„å®‰å…¨æ€§

Issues loading models in LM Studio: Users reported various errors while loading models in LM Studio, including an 'unknown error' and 'exit code: 18446744072635812000'. Recommendations included providing system specs and checking the API for details on the error.
One user particularly struggled with state handling when connecting to local models, indicating a need for more guidance on API interactions.
Evaluating model performance on specific hardware: Discussion took place regarding the suitability of the XTX 7900 24GB graphic card for running 30GB AI models, with insights shared about performance capabilities. Users highlighted settings and configurations needed for optimal results.
Another user seeking to run deep learning tasks locally expressed concerns about RAM and processing capabilities in relation to model demands.
Integrating Obsidian with LM Studio: Users explored issues with connecting Obsidian's Smart Connections extension to LM Studio, reporting various errors and conflicts with other extensions. Troubleshooting steps included uninstalling conflicting plugins and rebuilding caches.
One user managed to set up a connection but still faced ongoing errors related to missing required fields in API responses, asking for further clarifications.
Updates on model availability and safety: Users inquired about the safety and reliability of downloading AI models from TheBloke after noting some models were unavailable elsewhere. It was confirmed that TheBloke's models remain a standard in the industry despite his reduced presence in the community.
Users were encouraged to keep an eye on community channels for updates on model availability and potential releases of newer versions.
Frequency of model updates in LM Studio: The frequency of new updates for LM Studio was questioned, with one user anticipating improvements for the Qwen2.5-VL model. Insights were shared that updates often coincide with the release of new models rather than regular software updates.
Users expressed excitement over potential enhancements and acknowledged the necessity of closely monitoring community announcements for the latest features.
Links mentioned:
no title found: no description found
Download and run lmstudio-community/Mistral-7B-Instruct-v0.3-GGUF in LM Studio: Use lmstudio-community/Mistral-7B-Instruct-v0.3-GGUF locally in your LM Studio
imgur.com: Discover the magic of the internet at Imgur, a community powered entertainment destination. Lift your spirits with funny jokes, trending memes, entertaining gifs, inspiring stories, viral videos, and ...
showlab/ShowUI-2B Â· Hugging Face: no description found
Buy 14-inch MacBook Pro with M4 Max: Discover the MacBook Pro laptop with the M4 family of chips, built for Apple Intelligence. Get credit when you trade in an eligible Mac. Buy now.
lmstudio-community/MiniCPM-o-2_6-GGUF Â· Hugging Face: no description found
lmstudio-community/UI-TARS-2B-SFT-GGUF Â· Hugging Face: no description found
llama.cpp/docs/build.md at master Â· ggerganov/llama.cpp: LLM inference in C/C++. Contribute to ggerganov/llama.cpp development by creating an account on GitHub.
GitHub - kth8/llama-server-vulkan: Run llama.cpp server with Vulkan: Run llama.cpp server with Vulkan. Contribute to kth8/llama-server-vulkan development by creating an account on GitHub.
o3-mini vs DeepSeek-R1: In-depth o3-mini vs DeepSeek-R1 comparison: Latest benchmarks, pricing, context window, performance metrics, and technical specifications in 2025.
Cloudflare Status: no description found
LM Studio - Discover, download, and run local LLMs: Run Llama, Mistral, Phi-3 locally on your computer.
Deep Dive into LLMs like ChatGPT: This is a general audience deep dive into the Large Language Model (LLM) AI technology that powers ChatGPT and related products. It is covers the full traini...
GitHub - stackblitz-labs/bolt.diy: Prompt, run, edit, and deploy full-stack web applications using any LLM you want!: Prompt, run, edit, and deploy full-stack web applications using any LLM you want! - stackblitz-labs/bolt.diy
GitHub - stackblitz-labs/bolt.diy: Prompt, run, edit, and deploy full-stack web applications using any LLM you want!: Prompt, run, edit, and deploy full-stack web applications using any LLM you want! - stackblitz-labs/bolt.diy
GitHub - ggerganov/llama.cpp: LLM inference in C/C++: LLM inference in C/C++. Contribute to ggerganov/llama.cpp development by creating an account on GitHub.
LM Studio â–· #hardware-discussion (23 messagesğŸ”¥):
DDR5 6000 EXPO Performance, Hardware Configuration for LMS, Memory Testing Tools, Multi-GPU Setup on PCIe 3.0

åœ¨ LM Studio ä¸­åŠ è½½æ¨¡å‹æ—¶å‡ºç°çš„é—®é¢˜ï¼šç”¨æˆ·æŠ¥å‘Šåœ¨ LM Studio ä¸­åŠ è½½æ¨¡å‹æ—¶å‡ºç°å„ç§é”™è¯¯ï¼ŒåŒ…æ‹¬ã€ŒæœªçŸ¥é”™è¯¯ã€å’Œã€Œé€€å‡ºä»£ç ï¼š18446744072635812000ã€ã€‚å»ºè®®åŒ…æ‹¬æä¾›ç³»ç»Ÿè§„æ ¼ï¼Œå¹¶æ£€æŸ¥åº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ï¼ˆAPIï¼‰ä»¥è·å–é”™è¯¯çš„è¯¦ç»†ä¿¡æ¯ã€‚
æœ‰ç”¨æˆ·åæ˜ ï¼Œåœ¨è¿æ¥æœ¬åœ°æ¨¡å‹æ—¶ï¼ŒçŠ¶æ€å¤„ç†æ¯”è¾ƒå›°éš¾ï¼Œå¸Œæœ›èƒ½å¤Ÿè·å¾—æ›´å¤šå…³äº API äº¤äº’æ–¹é¢çš„æŒ‡å¯¼ã€‚
åœ¨ç‰¹å®šç¡¬ä»¶ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼šæœ‰äººè®¨è®ºäº† XTX 7900 24GB æ˜¾å¡æ˜¯å¦é€‚åˆè¿è¡Œ 30GB çš„ AI æ¨¡å‹ï¼Œå¹¶åˆ†äº«äº†å…³äºæ€§èƒ½æ–¹é¢çš„ç»éªŒã€‚ç”¨æˆ·å¼ºè°ƒäº†ä¸ºäº†è·å¾—æœ€ä½³æ•ˆæœï¼Œéœ€è¦è¿›è¡Œå“ªäº›è®¾ç½®å’Œé…ç½®ã€‚
è¿˜æœ‰ç”¨æˆ·å¸Œæœ›åœ¨æœ¬åœ°è¿è¡Œæ·±åº¦å­¦ä¹ ä»»åŠ¡ï¼Œä»–ä»¬è¡¨è¾¾äº†å¯¹å†…å­˜ï¼ˆRAMï¼‰å’Œå¤„ç†èƒ½åŠ›æ˜¯å¦èƒ½å¤Ÿæ»¡è¶³æ¨¡å‹éœ€æ±‚çš„æ‹…å¿§ã€‚
å°† Obsidian ä¸ LM Studio é›†æˆï¼šæœ‰ç”¨æˆ·åæ˜ ï¼Œå°† Obsidian çš„ Smart Connections æ’ä»¶è¿æ¥åˆ° LM Studio æ—¶é‡åˆ°é—®é¢˜ï¼Œå‡ºç°äº†å„ç§é”™è¯¯ï¼Œå¹¶ä¸”ä¸å…¶ä»–æ’ä»¶å­˜åœ¨å†²çªã€‚è§£å†³æ­¥éª¤åŒ…æ‹¬å¸è½½å†²çªæ’ä»¶å’Œé‡å»ºç¼“å­˜ã€‚
æœ‰ç”¨æˆ·æˆåŠŸå»ºç«‹äº†è¿æ¥ï¼Œä½†ä»ç„¶é‡åˆ°ä¸ API å“åº”ä¸­ç¼ºå°‘å¿…è¦å­—æ®µç›¸å…³çš„é”™è¯¯ï¼Œå¸Œæœ›è·å¾—è¿›ä¸€æ­¥çš„è§£é‡Šã€‚
å…³äºæ¨¡å‹å¯ç”¨æ€§å’Œå®‰å…¨æ€§çš„æ›´æ–°ï¼šæœ‰ç”¨æˆ·è¯¢é—®ä» TheBloke ä¸‹è½½ AI æ¨¡å‹çš„å®‰å…¨æ€§å’Œå¯é æ€§ï¼Œå› ä¸ºä»–ä»¬å‘ç°æŸäº›æ¨¡å‹åœ¨å…¶ä»–åœ°æ–¹å·²ç»æ‰¾ä¸åˆ°äº†ã€‚æœ‰æ¶ˆæ¯ç§°ï¼Œå°½ç®¡ TheBloke ç°åœ¨è¾ƒå°‘åœ¨ç¤¾åŒºéœ²é¢ï¼Œä½†ä»–æä¾›çš„æ¨¡å‹ä¾ç„¶æ˜¯è¡Œä¸šæ ‡æ†ã€‚
é¼“åŠ±å¤§å®¶å…³æ³¨ç¤¾åŒºæ¸ é“ï¼Œä»¥ä¾¿åŠæ—¶äº†è§£æ¨¡å‹æ›´æ–°å’Œæ–°ç‰ˆæœ¬å‘å¸ƒçš„ä¿¡æ¯ã€‚
LM Studio ä¸­æ¨¡å‹æ›´æ–°çš„é¢‘ç‡ï¼šæœ‰ç”¨æˆ·æé—®ï¼ŒLM Studio çš„æ›´æ–°é¢‘ç‡æ˜¯æ€æ ·çš„ï¼Œå¹¶ä¸”æœŸå¾… Qwen2.5-VL æ¨¡å‹èƒ½å¤Ÿå¾—åˆ°æ”¹è¿›ã€‚æ¶ˆæ¯ç§°ï¼ŒLM Studio çš„æ›´æ–°é€šå¸¸ä¼´éšç€æ–°æ¨¡å‹çš„å‘å¸ƒï¼Œè€Œä¸æ˜¯å®šæœŸçš„è½¯ä»¶æ›´æ–°ã€‚
ç”¨æˆ·å¯¹æœªæ¥çš„åŠŸèƒ½å¢å¼ºè¡¨ç¤ºå…´å¥‹ï¼Œå¹¶ä¸”è¡¨ç¤ºä¼šå¯†åˆ‡å…³æ³¨ç¤¾åŒºå…¬å‘Šï¼Œä»¥ä¾¿è·å–æœ€æ–°çš„åŠŸèƒ½ä¿¡æ¯ã€‚

æåˆ°çš„é“¾æ¥ï¼š
no title foundï¼šno description found
Download and run lmstudio-community/Mistral-7B-Instruct-v0.3-GGUF in LM Studioï¼šUse lmstudio-community/Mistral-7B-Instruct-v0.3-GGUF locally in your LM Studio
imgur.comï¼šDiscover the magic of the internet at Imgurï¼Œa community powered entertainment destination. Lift your spirits with funny jokesï¼Œtrending memesï¼Œentertaining gifsï¼Œinspiring storiesï¼Œviral videosï¼Œand ...
showlab/ShowUI-2BÂ·Hugging Faceï¼šno description found
Buy 14-inch MacBook Pro with M4 Maxï¼šDiscover the MacBook Pro laptop with the M4 family of chipsï¼Œbuilt for Apple Intelligence. Get credit when you trade in an eligible Mac. Buy now.
lmstudio-community/MiniCPM-o-2_6-GGUFÂ·Hugging Faceï¼šno description found
lmstudio-community/UI-TARS-2B-SFT-GGUFÂ·Hugging Faceï¼šno description found
llama.cpp/docs/build.md at masterÂ·ggerganov/llama.cppï¼šLLM inference in C/C++. Contribute to ggerganov/llama.cpp development by creating an account on GitHub.
GitHub - kth8/llama-server-vulkanï¼šRun llama.cpp server with Vulkanï¼šRun llama.cpp server with Vulkan. Contribute to kth8/llama-server-vulkan development by creating an account on GitHub.
o3-mini vs DeepSeek-R1ï¼šIn-depth o3-mini vs DeepSeek-R1 comparisonï¼šLatest benchmarksï¼Œpricingï¼Œcontext windowï¼Œperformance metricsï¼Œand technical specifications in 2025.
Cloudflare Statusï¼šno description found
LM Studio - Discoverï¼Œdownloadï¼Œand run local LLMsï¼šRun Llamaï¼ŒMistralï¼ŒPhi-3 locally on your computer.
Deep Dive into LLMs like ChatGPTï¼šThis is a general audience deep dive into the Large Language Modelï¼ˆLLMï¼‰AI technology that powers ChatGPT and related products. It is covers the full traini...
GitHub - stackblitz-labs/bolt.diyï¼šPromptï¼Œrunï¼Œeditï¼Œand deploy full-stack web applications using any LLM you want!ï¼šPromptï¼Œrunï¼Œeditï¼Œand deploy full-stack web applications using any LLM you want! - stackblitz-labs/bolt.diy
GitHub - ggerganov/llama.cppï¼šLLM inference in C/C++ï¼šLLM inference in C/C++. Contribute to ggerganov/llama.cpp development by creating an account on GitHub.
LM Studio â–· #hardware-discussionï¼ˆ23 messagesğŸ”¥):
DDR5 6000 EXPO Performanceï¼ŒHardware Configuration for LMSï¼ŒMemory Testing Toolsï¼ŒMulti-GPU Setup on PCIe 3.0

DDR5 6000 EXPO timings conservative: A member pointed out that the EXPO timings for their DDR5 6000 were likely super conservative, noting max memory bandwidth during inference peaked at 72.
They successfully completed 4 passes of memtest86 to ensure stability, although another member recommended trying TestMem5 for a more rigorous assessment.
LMS Hardware Configuration Troubles: Another user raised a question about hardware setup for hosting LMS 0.3.9, mentioning their 32-core CPU but no GPU, receiving advice on memory usage settings.
The recommendation included running in Developer mode with the option to keep the entire model in RAM, suggesting tweaks in thread usage for better speed.
Exploring Multi-GPU Capabilities: A new member inquired about running multiple 3090s on PCIe 3.0 16x, seeking experiences from others in the community.
Discussion revolved around whether such a setup remains viable, with another user asking for examples of setups that manage to run larger models effectively.
Inference Speed Considerations: Concerns were raised over whether running RAM at 7600 would yield noticeable changes in inference speeds, but initial comparisons showed only a 15% improvement.
Members noted that larger amounts of prompts could affect average speeds, particularly contrasting plain text responses with those generated from Python code.
Understanding GPU Acceleration: Inquiries were made regarding GPU acceleration for the DeepSeek R1 Distill Qwen 7B model, with some confusion about which models support GPU use.
It was clarified that only specific models like Llama have known support for acceleration, leaving some ambiguity for the DeepSeek model.
Link mentioned: GitHub - CoolCmd/TestMem5: TestMem5 - PC RAM stress test: TestMem5 - PC RAM stress test. Contribute to CoolCmd/TestMem5 development by creating an account on GitHub.

DDR5 6000 EXPO æ—¶åºä¿å®ˆï¼šä¸€ä½æˆå‘˜æŒ‡å‡ºï¼Œä»–ä»¬ DDR5 6000 çš„ EXPO é…ç½®çš„æ—¶åºå¯èƒ½è¿‡äºä¿å®ˆï¼Œå¹¶æŒ‡å‡ºæ¨ç†æœŸé—´çš„æœ€å¤§å†…å­˜å¸¦å®½å³°å€¼ä¸º 72ã€‚
ä»–ä»¬æˆåŠŸå®Œæˆäº† memtest86 çš„ 4 è½®æµ‹è¯•ï¼Œä»¥ç¡®ä¿ç¨³å®šæ€§ï¼Œå°½ç®¡å¦ä¸€ä½æˆå‘˜å»ºè®®å°è¯• TestMem5 è¿›è¡Œæ›´ä¸¥æ ¼çš„è¯„ä¼°ã€‚
LMS ç¡¬ä»¶é…ç½®é—®é¢˜ï¼šå¦ä¸€ä½ç”¨æˆ·æå‡ºäº†å…³äºéƒ¨ç½² LMS 0.3.9 çš„ç¡¬ä»¶è®¾ç½®é—®é¢˜ï¼Œæåˆ°äº†ä»–ä»¬ 32 æ ¸ CPU ä½†æ²¡æœ‰ GPUï¼Œæ”¶åˆ°äº†å…³äºå†…å­˜ä½¿ç”¨è®¾ç½®çš„å»ºè®®ã€‚
è¯¥å»ºè®®åŒ…æ‹¬åœ¨å¼€å‘è€…æ¨¡å¼ä¸‹è¿è¡Œï¼Œå¹¶é€‰æ‹©å°†æ•´ä¸ªæ¨¡å‹ä¿å­˜åœ¨ RAM ä¸­ï¼Œå»ºè®®è°ƒæ•´çº¿ç¨‹ä½¿ç”¨ä»¥è·å¾—æ›´å¥½çš„é€Ÿåº¦ã€‚
æ¢ç´¢å¤š GPU åŠŸèƒ½ï¼šä¸€ä½æ–°æˆå‘˜è¯¢é—®äº†åœ¨ PCIe 3.0 16x ä¸Šè¿è¡Œå¤šä¸ª 3090 æ˜¾å¡çš„æƒ…å†µï¼Œå¯»æ±‚ç¤¾åŒºä¸­å…¶ä»–äººçš„ç»éªŒã€‚
è®¨è®ºå›´ç»•ç€è¿™ç§è®¾ç½®æ˜¯å¦ä»ç„¶å¯è¡Œï¼Œå¦ä¸€ä½ç”¨æˆ·è¯¢é—®äº†èƒ½å¤Ÿæœ‰æ•ˆè¿è¡Œæ›´å¤§æ¨¡å‹çš„è®¾ç½®ç¤ºä¾‹ã€‚
æ¨ç†é€Ÿåº¦æ³¨æ„äº‹é¡¹ï¼šæœ‰äººå¯¹ä»¥ 7600 è¿è¡Œ RAMï¼ˆRandom Access Memoryï¼Œéšæœºå­˜å–å­˜å‚¨å™¨ï¼‰æ˜¯å¦ä¼šäº§ç”Ÿæ˜¾ç€çš„æ¨ç†é€Ÿåº¦å˜åŒ–è¡¨ç¤ºæ‹…å¿§ï¼Œä½†åˆæ­¥æ¯”è¾ƒæ˜¾ç¤ºä»…æé«˜äº† 15%ã€‚
æˆå‘˜ä»¬æŒ‡å‡ºï¼Œå¤§é‡æç¤ºè¯ï¼ˆpromptï¼‰å¯èƒ½ä¼šå½±å“å¹³å‡é€Ÿåº¦ï¼Œç‰¹åˆ«æ˜¯å°†çº¯æ–‡æœ¬å“åº”ä¸ä» Python ä»£ç ç”Ÿæˆçš„å“åº”è¿›è¡Œå¯¹æ¯”ã€‚
äº†è§£ GPU åŠ é€Ÿï¼šæœ‰äººè¯¢é—®äº† DeepSeek R1 Distill Qwen 7B æ¨¡å‹çš„ GPUï¼ˆGraphics Processing Unitï¼Œå›¾å½¢å¤„ç†å™¨ï¼‰åŠ é€Ÿï¼Œå¯¹äºå“ªäº›æ¨¡å‹æ”¯æŒ GPU ä½¿ç”¨å­˜åœ¨ä¸€äº›ç–‘é—®ã€‚
æ¾„æ¸…è¯´ï¼Œåªæœ‰åƒ Llama è¿™æ ·ç‰¹å®šçš„æ¨¡å‹å·²çŸ¥æ”¯æŒåŠ é€Ÿï¼Œè¿™ä½¿å¾— DeepSeek æ¨¡å‹æ˜¯å¦æ”¯æŒ GPU åŠ é€Ÿå°šä¸æ˜ç¡®ã€‚
é“¾æ¥æåˆ°ï¼šGitHub - CoolCmd/TestMem5ï¼šTestMem5 - PC RAM stress testï¼šTestMem5 - PC å†…å­˜å‹åŠ›æµ‹è¯•ã€‚é€šè¿‡åœ¨ GitHub ä¸Šåˆ›å»ºä¸€ä¸ªå¸æˆ·æ¥å‚ä¸ CoolCmd/TestMem5 çš„å¼€å‘ã€‚

MCP (Glama) â–· #general (97 messagesğŸ”¥ğŸ”¥):
Home Assistant MCP Client/Server, MCP Server Usage, Goose MCP Client, Image Display in Claude, MCP Server Configurations

MCPï¼ˆGlamaï¼‰â–· #é€šç”¨é¢‘é“ï¼ˆ97 æ¡æœªè¯»æ¶ˆæ¯ğŸ”¥ğŸ”¥)ï¼š
Home Assistant MCP å®¢æˆ·ç«¯ / æœåŠ¡å™¨ï¼ŒMCP æœåŠ¡å™¨ä½¿ç”¨æŒ‡å—ï¼ŒGoose MCP å®¢æˆ·ç«¯ï¼Œåœ¨ Claude ä¸­æ˜¾ç¤ºå›¾åƒï¼ŒMCP æœåŠ¡å™¨é…ç½®

Home Assistant MCP Client with New Features: A user announced the publication of their Home Assistant with MCP client/server support, describing it as functional but needing a 'wow factor'. They also shared plans to include an animated talking head avatar for better user interaction.
The project is still in progress as they balance paid work with development.
Curiosity about MCP Server Usage Statistics: A user expressed curiosity about how many people are using the Home Assistant MCP, referring to efforts to bridge it with other tools like Claude.
This sparked discussions about the functionality of different MCP clients and their wider usage.
Discussion on Goose MCP Client: Users shared their experiences using the Goose MCP Client, noting its effectiveness and current use cases in testing setups.
One user also mentioned a pending pull request that aims to improve its logging features, highlighting the close collaboration within the community.
Challenges with Image Display in Claude Desktop: A user asked about displaying images as tool results on Claude Desktop, pointing out an input error encountered when trying to do so.
They speculated that converting image results to embedded resources might be a solution.
MCP Server Configuration Insights: Discussion about designing better MCP server configurations ensued, with users sharing thoughts on how to effectively manage multiple servers.
One suggested using a multiplexer approach with bridge to streamline server management, and others shared their development plans for MCP clients.
Links mentioned:
GitHub Â· Build and ship software on a single, collaborative platform: Join the world's most widely adopted, AI-powered developer platform where millions of developers, businesses, and the largest open source community build software that advances humanity.
GitHub - splendasucks/webperfect-mcp-server: webperfect-mcp-server: webperfect-mcp-server. Contribute to splendasucks/webperfect-mcp-server development by creating an account on GitHub.
fix(anthropic): include cached tokens in usage count logs Â· block/goose@162c4c5: an open-source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM - fix(anthropic): include cached tokens in usage count logs Â· block/goose@162c4c5
fix(anthropic): include cached tokens in usage count logs by evalstate Â· Pull Request #947 Â· block/goose: cache_creation_input_tokens and cache_read_input_tokens were not beingadded to the usage total recorded in goose.log. This fix includesthose categories in the calculation of "input tokens&...
GitHub - met4citizen/TalkingHead: Talking Head (3D): A JavaScript class for real-time lip-sync using Ready Player Me full-body 3D avatars.: Talking Head (3D): A JavaScript class for real-time lip-sync using Ready Player Me full-body 3D avatars. - met4citizen/TalkingHead
MCP (Glama) â–· #showcase (54 messagesğŸ”¥):
PulseMCP Use Cases, MCP Servers, Claude for Research, Web Research Tools, Markdown in Discord

Home Assistant MCP å®¢æˆ·ç«¯è¿æ¥æ–°åŠŸèƒ½ï¼šä¸€ä½ç”¨æˆ·å®£å¸ƒå‘å¸ƒäº†æ”¯æŒ MCP å®¢æˆ·ç«¯ / æœåŠ¡å™¨çš„ Home Assistantï¼Œç§°å…¶åŠŸèƒ½é½å…¨ï¼Œä½†éœ€è¦ä¸€ä¸ªã€Œä»¤äººçœ¼å‰ä¸€äº®çš„åŠŸèƒ½ã€ã€‚ä»–ä»¬è¿˜åˆ†äº«äº†è®¡åˆ’ï¼Œå°†åŠ å…¥ä¸€ä¸ªåŠ¨ç”»çš„ä¼šè¯´è¯çš„å¤´éƒ¨åŒ–èº«ï¼Œä»¥å¢å¼ºç”¨æˆ·äº’åŠ¨ã€‚
è¯¥é¡¹ç›®ä»åœ¨è¿›è¡Œä¸­ï¼Œå¼€å‘è€…éœ€è¦åœ¨æœ‰å¿å·¥ä½œå’Œé¡¹ç›®å¼€å‘ä¹‹é—´ä¿æŒå¹³è¡¡ã€‚
å¯¹ MCP æœåŠ¡å™¨ä½¿ç”¨æƒ…å†µç»Ÿè®¡çš„å…³æ³¨ï¼šä¸€ä½ç”¨æˆ·å¯¹ Home Assistant MCP çš„ä½¿ç”¨æƒ…å†µè¡¨ç¤ºå¥½å¥‡ï¼Œå¹¶æåˆ°äº†å°†å…¶ä¸ Claude ç­‰å·¥å…·è¿æ¥çš„å°è¯•ã€‚
ç”±æ­¤å¼•å‘äº†å…³äºä¸åŒ MCP å®¢æˆ·ç«¯åŠŸèƒ½åŠå…¶åº”ç”¨èŒƒå›´çš„è®¨è®ºã€‚
å…³äº Goose MCP å®¢æˆ·ç«¯çš„è®¨è®ºï¼šç”¨æˆ·åˆ†äº«äº†ä»–ä»¬ä½¿ç”¨ Goose MCP å®¢æˆ·ç«¯çš„ç»éªŒï¼Œè‚¯å®šäº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶ä»‹ç»äº†ç›®å‰åœ¨æµ‹è¯•ç¯å¢ƒä¸­çš„ç”¨ä¾‹ã€‚
ä¸€ä½ç”¨æˆ·è¿˜æåˆ°äº†ä¸€ä¸ªå¾…å¤„ç†çš„ pull requestï¼Œæ—¨åœ¨æ”¹è¿›å…¶æ—¥å¿—è®°å½•åŠŸèƒ½ï¼Œçªæ˜¾äº†ç¤¾åŒºå†…çš„å¯†åˆ‡åˆä½œã€‚
åœ¨ Claude Desktop ä¸­æ˜¾ç¤ºå›¾åƒæ—¶é‡åˆ°çš„é—®é¢˜ï¼šä¸€ä½ç”¨æˆ·è¯¢é—®å¦‚ä½•åœ¨ Claude Desktop ä¸Šå°†å›¾åƒæ˜¾ç¤ºä¸ºå·¥å…·ç»“æœï¼Œå¹¶æŒ‡å‡ºäº†å°è¯•è¿™æ ·åšæ—¶é‡åˆ°çš„è¾“å…¥é”™è¯¯ã€‚
ä»–ä»¬æ¨æµ‹ï¼Œå°†å›¾åƒç»“æœè½¬æ¢ä¸ºåµŒå…¥å¼èµ„æºå¯èƒ½æ˜¯ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚
å…³äº MCP æœåŠ¡å™¨é…ç½®çš„è®¨è®ºï¼šç”¨æˆ·å°±å¦‚ä½•è®¾è®¡æ›´å¥½çš„ MCP æœåŠ¡å™¨é…ç½®å±•å¼€äº†è®¨è®ºï¼Œåˆ†äº«äº†å…³äºå¦‚ä½•æœ‰æ•ˆç®¡ç†å¤šä¸ªæœåŠ¡å™¨çš„è§è§£ã€‚
æœ‰äººå»ºè®®é‡‡ç”¨å¤šè·¯å¤ç”¨å™¨åŠ æ¡¥æ¥çš„æ–¹å¼æ¥ç®€åŒ–æœåŠ¡å™¨ç®¡ç†ï¼Œå…¶ä»–äººåˆ™åˆ†äº«äº†ä»–ä»¬çš„ MCP å®¢æˆ·ç«¯å¼€å‘è®¡åˆ’ã€‚

æåˆ°çš„é“¾æ¥ï¼š
GitHubï¼šå…¨çƒæ•°ç™¾ä¸‡å¼€å‘è€…ã€ä¼ä¸šå’Œå¼€æºç¤¾åŒºåœ¨æ­¤æ„å»ºè½¯ä»¶ã€‚
GitHub - splendasucks/webperfect-mcp-serverï¼šwebperfect-mcp-serverï¼šwebperfect-mcp-serverã€‚é€šè¿‡åœ¨ GitHub ä¸Šåˆ›å»ºä¸€ä¸ªå¸æˆ·æ¥ä¸º splendasucks/webperfect-mcp-server çš„å¼€å‘åšå‡ºè´¡çŒ®ã€‚
fixï¼ˆanthropic)ï¼šinclude cached tokens in usage count logsÂ·block/goose@162c4c5ï¼šä¸€ä¸ªå¼€æºçš„ã€å¯æ‰©å±•çš„ AI æ™ºèƒ½ä½“ï¼Œè¶…è¶Šäº†ä»£ç å»ºè®® - ä½¿ç”¨ä»»ä½•å¤§è¯­è¨€æ¨¡å‹å®‰è£…ã€æ‰§è¡Œã€ç¼–è¾‘å’Œæµ‹è¯• - fixï¼ˆanthropic)ï¼šinclude cached tokens in usage count logsÂ·block/goose@162c4c5
fixï¼ˆanthropic)ï¼šinclude cached tokens in usage count logs by evalstateÂ·Pull Request #947Â·block/gooseï¼š ä¿®å¤äº† goose.log ä¸­ token ä½¿ç”¨é‡ç»Ÿè®¡çš„é—®é¢˜ã€‚
GitHub - met4citizen/TalkingHeadï¼šTalking Headï¼ˆ3D)ï¼šç”¨äºä½¿ç”¨ Ready Player Me å…¨èº« 3D å¤´åƒè¿›è¡Œå®æ—¶å”‡åŒæ­¥çš„ JavaScript ç±»ã€‚ï¼šTalking Headï¼ˆ3D)ï¼šç”¨äºä½¿ç”¨ Ready Player Me å…¨èº« 3D å¤´åƒè¿›è¡Œå®æ—¶å”‡åŒæ­¥çš„ JavaScript ç±»ã€‚- met4citizen/TalkingHead
MCPï¼ˆGlamaï¼‰â–· #showcaseï¼ˆ54 æ¡æ¶ˆæ¯ğŸ”¥):
PulseMCP ç”¨ä¾‹ã€MCP æœåŠ¡å™¨ã€ç”¨äºç ”ç©¶çš„ Claudeã€Web ç ”ç©¶å·¥å…·ã€Discord ä¸­çš„ Markdown

PulseMCP Use Cases launched: A new showcase of practical PulseMCP Use Cases was announced, featuring detailed instructions and videos for using various client apps and servers effectively.
Initial highlights include uses of Gemini voice, Claude, and Cline for managing Notion, converting Figma designs, and creating knowledge graphs, respectively.
Claude replicates ChatGPT DeepResearch: Claude demonstrated the ability to replicate ChatGPT DeepResearch efficiently using specific MCP servers like mzxrai's web research MCP and Brave web search MCP.
A user noted that with sufficient time, Claude could process up to 100 articles, highlighting the flexibility of the tool when given proper inputs.
Web search concerns and solutions: Discussions revealed challenges with Google searches triggering bot detections and offered alternatives, like using SearXNG for searches without captchas.
Modifications to chromedriver and the suggestion to use puppeteer were among tools recommended to overcome these issues.
MCP capabilities on mobile devices: Inquiries about mobile MCP clients suggested that Sage supports iPhones, while options for Android users may require using web clients like LibreChat or MCP-Bridge.
This reflects ongoing interest in accessing MCP functionality beyond desktop applications.
Markdown rendering in Discord: A conversation emerged around Discord's Markdown rendering capabilities, noting its implementation since last year and user surprises at its features.
Members shared informal banter about using Markdown styles, reflecting a light-hearted community engagement.
Links mentioned:
Community use-cases of MCP in-action | PulseMCP: Explore all the ways in which the community is putting the Model Context Protocol (MCP) to use.
Tweet from Tadas Antanavicius (@tadasayy): ğŸ‰ Announcing the launch of Use Cases on PulseMCP (follow @pulsemcp to keep up)!There have been a ton of great MCP servers & clients built since its launch by @Anthropic and we built a resource to hig...
Yannick Kilcher â–· #general (112 messagesğŸ”¥ğŸ”¥):
Gemini 2.0 Performance, DeepSpeed with Hugging Face, AI Legislation Impact, Australia's Internet Infrastructure, Open Source AI Models

PulseMCP ç”¨ä¾‹å‘å¸ƒï¼šPulseMCP å‘å¸ƒäº†ä¸€ç³»åˆ—å®ç”¨ç”¨ä¾‹ï¼Œé€šè¿‡è¯¦ç»†çš„æŒ‡å—å’Œè§†é¢‘ï¼Œå±•ç¤ºäº†å¦‚ä½•é«˜æ•ˆä½¿ç”¨å„ç§å®¢æˆ·ç«¯åº”ç”¨å’ŒæœåŠ¡å™¨ã€‚
é¦–æ‰¹äº®ç‚¹åŒ…æ‹¬ä½¿ç”¨ Gemini è¯­éŸ³ã€Claude å’Œ Cline åˆ†åˆ«ç®¡ç† Notionï¼Œè½¬æ¢ Figma è®¾è®¡ï¼Œä»¥åŠåˆ›å»ºçŸ¥è¯†å›¾è°±ã€‚
Claude å¤åˆ¶ ChatGPT DeepResearchï¼šClaude æ¼”ç¤ºäº†å…¶é«˜æ•ˆå¤åˆ¶ ChatGPT DeepResearch çš„èƒ½åŠ›ï¼Œå®ƒä½¿ç”¨äº†ç‰¹å®šçš„ MCP æœåŠ¡å™¨ï¼Œä¾‹å¦‚ mzxrai çš„ Web ç ”ç©¶ MCP å’Œ Brave Web æœç´¢ MCPã€‚
ä¸€ä½ç”¨æˆ·æŒ‡å‡ºï¼Œå¦‚æœç»™äºˆå……è¶³çš„æ—¶é—´ï¼ŒClaude èƒ½å¤Ÿå¤„ç†å¤šè¾¾ 100 ç¯‡æ–‡ç« ï¼Œè¿™çªæ˜¾äº†è¯¥å·¥å…·åœ¨è·å¾—é€‚å½“è¾“å…¥æ—¶çš„çµæ´»æ€§ã€‚
ç½‘é¡µæœç´¢é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆï¼šè®¨è®ºæ­ç¤ºäº† Google æœç´¢è§¦å‘åçˆ¬è™«æœºåˆ¶çš„æŒ‘æˆ˜ï¼Œå¹¶æä¾›äº†æ›¿ä»£æ–¹æ¡ˆï¼Œä¾‹å¦‚ä½¿ç”¨ SearXNG è¿›è¡Œæ— éœ€éªŒè¯ç çš„æœç´¢ã€‚
æ¨èçš„è§£å†³æ–¹æ¡ˆåŒ…æ‹¬ä¿®æ”¹ chromedriver å’Œä½¿ç”¨ puppeteerã€‚
ç§»åŠ¨è®¾å¤‡ä¸Šçš„ MCP åŠŸèƒ½ï¼šå…³äºç§»åŠ¨ MCP å®¢æˆ·ç«¯çš„è®¨è®ºè¡¨æ˜ï¼ŒSage æ”¯æŒ iPhoneï¼Œè€Œ Android ç”¨æˆ·å¯èƒ½éœ€è¦ä½¿ç”¨è¯¸å¦‚ LibreChat æˆ– MCP-Bridge è¿™æ ·çš„ç½‘é¡µå®¢æˆ·ç«¯ã€‚
è¿™åæ˜ äº†äººä»¬å¯¹åœ¨æ¡Œé¢åº”ç”¨ä¹‹å¤–ä½¿ç”¨ MCP åŠŸèƒ½çš„æŒç»­å…´è¶£ã€‚
Discord ä¸­çš„ Markdown æ¸²æŸ“ï¼šç¤¾åŒºè®¨è®ºäº† Discord çš„ Markdown æ¸²æŸ“åŠŸèƒ½ï¼Œæœ‰æˆå‘˜æåˆ°è¯¥åŠŸèƒ½è‡ªå»å¹´å·²ä¸Šçº¿ï¼Œå¹¶å¯¹å®ƒçš„ç‰¹æ€§è¡¨ç¤ºæƒŠè®¶ã€‚
æˆå‘˜ä»¬åˆ†äº«äº†å…³äºä½¿ç”¨ Markdown æ ·å¼çš„è½»æ¾ç©ç¬‘ï¼Œå±•ç°äº†æ´»è·ƒçš„ç¤¾åŒºäº’åŠ¨ã€‚
ç›¸å…³é“¾æ¥ï¼š
PulseMCP ä¸Šçš„ MCP ç¤¾åŒºç”¨ä¾‹ï¼šæ¢ç´¢ç¤¾åŒºä½¿ç”¨æ¨¡å‹ä¸Šä¸‹æ–‡åè®®ï¼ˆModel Context Protocolï¼ŒMCPï¼‰çš„å„ç§æ–¹å¼ã€‚
æ¥è‡ª Tadas Antanaviciusï¼ˆ@tadasayyï¼‰çš„æ¨æ–‡ï¼šğŸ‰ å®£å¸ƒåœ¨ PulseMCP ä¸Šå¯åŠ¨ç”¨ä¾‹ï¼ˆå…³æ³¨ @pulsemcp ä»¥ä¿æŒåŒæ­¥)ï¼è‡ª Anthropic æ¨å‡ºä»¥æ¥ï¼Œå·²ç»æ„å»ºäº†å¤§é‡å‡ºè‰²çš„ MCP æœåŠ¡å™¨å’Œå®¢æˆ·ç«¯ï¼Œæˆ‘ä»¬ä¸ºæ­¤æ„å»ºäº†ä¸€ä¸ªèµ„æºï¼Œä»¥çªå‡º...
Yannick Kilcher â–· #generalï¼ˆ112 æ¡æ¶ˆæ¯ğŸ”¥ğŸ”¥):
Gemini 2.0 æ€§èƒ½ã€Hugging Face çš„ DeepSpeedã€AI ç«‹æ³•å½±å“ã€æ¾³å¤§åˆ©äºšçš„äº’è”ç½‘åŸºç¡€è®¾æ–½ã€å¼€æº AI æ¨¡å‹

Gemini 2.0 outperforms competitors: Blog discussions mentioned that Gemini 2.0 Pro shows impressive performance on tasks like creating SVGs, especially compared to o3-mini and R1.
Several members noted stronger performance in SQL queries, suggesting Google is making significant strides with Gemini Flash 2.0.
DeepSpeed and Dataloader Confusion: One user expressed confusion over needing to manually specify batch_size in their Dataloader while using DeepSpeed's auto batch size configuration.
Another member suggested integrating DeepSpeed tags into the Dataloader for optimization and hinted at potential performance adjustments for various nodes.
Concerns About AI Legislation: A user shared concerns about new legislation in Australia, suggesting it was aimed at restricting free speech and thought, with major implications for society.
This was framed in the context of a broader sentiment that such laws could render many discussions illegal, stifling open dialogue and inquiry.
Australia's Struggles with Internet Infrastructure: A participant lamented that despite considerable financial investment, Australia still suffers from slow internet speeds, with some reporting home connectivity as low as 3 Mbps.
Discussions highlighted failures in infrastructure decisions, referencing the poor choice of copper over fiber optics made decades ago.
The Future of Open Source AI Models: The dialogue included concerns about the push against open source AI models, with discussions centering around limiting competitive and innovative models in favor of proprietary systems.
Members expressed frustration over perceived attempts to control discourse surrounding AI by outlawing certain discussions related to technology and free expression.
Links mentioned:
Gemini 2.0 is now available to everyone: Big new Gemini 2.0 releases today: - **Gemini 2.0 Pro (Experimental)** is Google's "best model yet for coding performance and complex prompts" - currently available as a free preview. -...
Tweet from George Christensen (@NationFirstAust): They're coming for your words. Your thoughts. Your beliefs. 1/24
Researchers created an open rival to OpenAI's o1 'reasoning' model for under $50 | TechCrunch: AI researchers at Stanford and the University of Washington were able to train an AI "reasoning" model for under $50 in cloud compute credits, according
Yannick Kilcher â–· #paper-discussion (10 messagesğŸ”¥):
Harmonic Loss Paper, VideoJAM Discussion, EU Discussion Hours, DeepSeek Hosting

Gemini 2.0 æ€§èƒ½è¶…è¶Šç«äº‰å¯¹æ‰‹ï¼š åšå®¢è®¨è®ºæåˆ°ï¼ŒGemini 2.0 Pro åœ¨åˆ›å»º SVG ç­‰ä»»åŠ¡ä¸Šè¡¨ç°å‡ºä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ä¸ o3-mini å’Œ R1 ç›¸æ¯”æ—¶ã€‚
æœ‰æˆå‘˜æŒ‡å‡ºï¼Œå…¶åœ¨ SQL æŸ¥è¯¢æ–¹é¢çš„æ€§èƒ½æ›´å¼ºï¼Œè¿™è¡¨æ˜ Google åœ¨ Gemini Flash 2.0 ä¸Šå–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚
DeepSpeed ä¸ Dataloader çš„ä½¿ç”¨é—®é¢˜ï¼š æœ‰ç”¨æˆ·è¡¨ç¤ºï¼Œåœ¨ä½¿ç”¨ DeepSpeed çš„è‡ªåŠ¨æ‰¹é‡å¤§å°é…ç½®æ—¶ï¼Œä»ç„¶éœ€è¦åœ¨ Dataloader ä¸­æ‰‹åŠ¨æŒ‡å®š batch_sizeï¼Œå¯¹æ­¤æ„Ÿåˆ°å›°æƒ‘ã€‚
å¦æœ‰æˆå‘˜å»ºè®®å°† DeepSpeed æ ‡ç­¾é›†æˆåˆ° Dataloader ä¸­ä»¥è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶æš—ç¤ºå¯èƒ½éœ€è¦å¯¹ä¸åŒèŠ‚ç‚¹è¿›è¡Œæ€§èƒ½è°ƒæ•´ã€‚
å…³äºäººå·¥æ™ºèƒ½ç«‹æ³•çš„æ‹…å¿§ï¼š ä¸€ä½ç”¨æˆ·è¡¨è¾¾äº†å¯¹æ¾³å¤§åˆ©äºšæ–°ç«‹æ³•å¯èƒ½é™åˆ¶è¨€è®ºå’Œæ€æƒ³è‡ªç”±çš„æ‹…å¿§ï¼Œè®¤ä¸ºè¿™å°†å¯¹ç¤¾ä¼šäº§ç”Ÿé‡å¤§å½±å“ã€‚
è¿™ç§æ‹…å¿§çš„èƒŒæ™¯æ˜¯ï¼Œç›¸å…³æ³•å¾‹å¯èƒ½ä¼šå¯¼è‡´è®¸å¤šè®¨è®ºå˜å¾—éæ³•ï¼Œä»è€Œæ‰¼æ€å…¬å¼€å¯¹è¯å’Œæ¢ç´¢ã€‚
æ¾³å¤§åˆ©äºšäº’è”ç½‘åŸºç¡€è®¾æ–½å»ºè®¾çš„å›°å¢ƒï¼š æœ‰å‚ä¸è€…æ„Ÿå¹ï¼Œå°½ç®¡æŠ•å…¥äº†å¤§é‡èµ„é‡‘ï¼Œæ¾³å¤§åˆ©äºšä»ç„¶é¢ä¸´ç½‘é€Ÿç¼“æ…¢çš„é—®é¢˜ï¼Œä¸€äº›ç”¨æˆ·æŠ¥å‘Šå®¶åº­ç½‘ç»œè¿æ¥é€Ÿåº¦ä»…ä¸º 3 Mbpsã€‚
è®¨è®ºå¼ºè°ƒäº†åŸºç¡€è®¾æ–½å†³ç­–çš„å¤±è¯¯ï¼Œä¾‹å¦‚ï¼Œå‡ åå¹´å‰é€‰æ‹©äº†é“œç¼†è€Œéå…‰çº¤ã€‚
å¼€æºäººå·¥æ™ºèƒ½æ¨¡å‹çš„æœªæ¥ï¼š å¯¹è¯ä¸­ä¹Ÿå‡ºç°äº†å¯¹é™åˆ¶å¼€æº AI æ¨¡å‹è¶‹åŠ¿çš„æ‹…å¿§ï¼Œè®¨è®ºä¸»è¦å›´ç»•é™åˆ¶å…·æœ‰ç«äº‰åŠ›å’Œåˆ›æ–°æ€§çš„æ¨¡å‹ï¼Œè½¬è€Œæ”¯æŒä¸“æœ‰ç³»ç»Ÿã€‚
æœ‰æˆå‘˜å¯¹è¯•å›¾é€šè¿‡ç¦æ­¢æŸäº›ä¸æŠ€æœ¯å’Œè‡ªç”±è¡¨è¾¾ç›¸å…³çš„è®¨è®ºæ¥æ§åˆ¶æœ‰å…³ AI çš„è¨€è®ºè¡¨ç¤ºæ²®ä¸§ã€‚

é“¾æ¥æåˆ°ï¼š
Gemini 2.0 is now available to everyoneï¼šBig new Gemini 2.0 releases todayï¼š- **Gemini 2.0 Proï¼ˆExperimental)** is Google'sã€Œbest model yet for coding performance and complex promptsã€- currently available as a free preview. -...
Tweet from George Christensenï¼ˆ@NationFirstAust)ï¼šThey're coming for your words. Your thoughts. Your beliefs. 1/24
Researchers created an open rival to OpenAI's o1 'reasoning' model for under $50 | TechCrunchï¼šAI researchers at Stanford and the University of Washington were able to train an AIã€Œreasoningã€model for under $50 in cloud compute creditsï¼Œaccording
Yannick Kilcher â–· #paper-discussionï¼ˆ10 messagesğŸ”¥):
Harmonic Loss Paperï¼ŒVideoJAM Discussionï¼ŒEU Discussion Hoursï¼ŒDeepSeek Hosting

Skeptical Reviews on Harmonic Loss: Some members expressed skepticism about the harmonic loss paper, noting it was 'kind of hastily done' and lacked performance increases despite theoretical advantages.
Another member mentioned that the GitHub repository for the paper is 'more informative' than the paper itself.
Excitement for VideoJAM Paper Review: A member announced an upcoming review of the VideoJAM paper scheduled for 6 PM EU time.
This timing might be beneficial for US members, although it presents challenges for those in EU time zones.
EU Hour Challenges in Discussions: Concerns were raised about the 'bad EU hours' for daily discussions, prompting a suggestion to move discussions to 6 PM EU time next week.
Another member confirmed the time difference, noting it's 6-9 hours earlier for US east/west coasts.
Link mentioned: VideoJAM: VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Model

å¯¹è°æ³¢æŸå¤±çš„è´¨ç–‘ï¼šä¸€äº›æˆå‘˜å¯¹è°æ³¢æŸå¤±ï¼ˆHarmonic Lossï¼‰è®ºæ–‡æŒæ€€ç–‘æ€åº¦ï¼Œè®¤ä¸ºè¯¥è®ºæ–‡ã€Œå®Œæˆå¾—è¾ƒä¸ºä»“ä¿ƒã€ï¼Œå¹¶ä¸”å°½ç®¡åœ¨ç†è®ºä¸Šå…·æœ‰ä¼˜åŠ¿ï¼Œä½†æ€§èƒ½å¹¶æ²¡æœ‰å¾—åˆ°æå‡ã€‚
å¦æœ‰æˆå‘˜æŒ‡å‡ºï¼Œè¯¥è®ºæ–‡çš„ GitHub ä»“åº“æä¾›çš„ä¿¡æ¯æ¯”è®ºæ–‡æœ¬èº«ã€Œæ›´æœ‰ä»·å€¼ã€ã€‚
å¯¹ VideoJAM è®ºæ–‡è¯„è®®çš„æœŸå¾…ï¼šæœ‰æˆå‘˜å®£å¸ƒï¼ŒVideoJAM è®ºæ–‡çš„è¯„è®®ä¼šå°†äºæ¬§æ´²æ—¶é—´ä¸‹åˆ 6 ç‚¹ä¸¾è¡Œã€‚
è¿™ä¸ªæ—¶é—´ç‚¹å¯èƒ½å¯¹ç¾å›½æˆå‘˜æ¯”è¾ƒå‹å¥½ï¼Œä½†å¯¹æ¬§æ´²æ—¶åŒºçš„æˆå‘˜æ¥è¯´åˆ™ä¸å¤ªæ–¹ä¾¿ã€‚
è®¨è®ºä¸­çš„æ¬§æ´²æ—¶é—´é—®é¢˜ï¼šæœ‰æˆå‘˜å¯¹ç›®å‰æ¯æ—¥è®¨è®ºçš„ã€Œæ¬§æ´²æ—¶é—´ä¸å‹å¥½ã€è¡¨ç¤ºæ‹…å¿§ï¼Œå¹¶å»ºè®®ä¸‹å‘¨å°†è®¨è®ºæ—¶é—´è°ƒæ•´åˆ°æ¬§æ´²æ—¶é—´ä¸‹åˆ 6 ç‚¹ã€‚
å¦ä¸€ä½æˆå‘˜ç¡®è®¤äº†æ—¶å·®é—®é¢˜ï¼Œå¹¶æŒ‡å‡ºä¸ç¾å›½ä¸œ / è¥¿æµ·å²¸æœ‰ 6-9 å°æ—¶çš„æ—¶å·®ã€‚
ç›¸å…³é“¾æ¥ï¼šVideoJAMï¼šç”¨äºå¢å¼ºè§†é¢‘æ¨¡å‹ä¸­è¿åŠ¨ç”Ÿæˆçš„è”åˆå¤–è§‚ - è¿åŠ¨è¡¨ç¤ºã€‚

Yannick Kilcher â–· #ml-news (10 messagesğŸ”¥):
Gemini 2.0 Flash, Flash-lite issues, S1 reasoning model, Inference scaling insights, OpenAI scaling laws

Gemini 2.0 Flash impresses: A user reported trying out the new Gemini 2.0 Flash model via LlamaIndex, noting its incredible speed, although not as fast as Groq.
This latest addition to the OpenRouter class is generating buzz among users eager to test its capabilities.
Flash-lite struggles with structured output: Another user reported that the Flash-lite model struggled with returning valid structured outputs, often generating invalid JSON formats.
Finding this underwhelming, they suggested that it might not be suitable for tasks needing reliability in output.
S1 emerges as a low-cost reasoning alternative: A recent blog post discussed the S1 reasoning model, which demonstrates competent performance similar to models like OpenAI's o1 but can run on basic machines, highlighting its low cost of under $50 for training.
Developed through distillation from Gemini 2.0, the S1 model and its tools are available on GitHub.
Insights on Inference Scaling: The conversation revealed insights into inference scaling, claiming that longer thinking times can enhance LLM performance; however, methods to achieve longer thinking processes were questioned.
The s1 paper illustrated this with graphs, sparking discussions about how to implement such strategies effectively.
Questions on flash capabilities: Community members posed questions regarding the capabilities of recently released models, including the 2.0 pro experimental version, and whether there were prompts for testing.
The value of newly launched models was debated, referencing potential and past experiences with distilled versions of existing models.
Links mentioned:
Gemini Flash 2.0 - API, Providers, Stats: Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1. Run Gemini Flash 2.0 with API
Google: Gemini Flash Lite 2.0 Preview (free) â€“ Run with an API: Sample code and API for Google: Gemini Flash Lite 2.0 Preview (free) - Gemini Flash Lite 2.0 offers a significantly faster time to first token (TTFT) compared to [Gemini Flash 1.5](google/gemini-flash...
Tweet from Omar Sanseviero (@osanseviero): Hey r/LocalLLaMA ğŸ‘‹We're cooking ğŸ«¡ Gemma going brrr
LLM Rankings | OpenRouter: Language models ranked and analyzed by usage across apps
S1: The $6 R1 Competitor?: no description found
Researchers created an open rival to OpenAI's o1 'reasoning' model for under $50 | TechCrunch: AI researchers at Stanford and the University of Washington were able to train an AI "reasoning" model for under $50 in cloud compute credits, according
Eleuther â–· #general (22 messagesğŸ”¥):
Collaboration on LLM Research, Deepspeed and Hugging Face, Benchmarking LLMs, Weight Decay in Fine Tuning, RWKV Architecture Development

Yannick Kilcher â–· #ml-newsï¼ˆ10 æ¡æ¶ˆæ¯ğŸ”¥):
Gemini 2.0 Flashï¼ŒFlash-lite é—®é¢˜ï¼ŒS1 æ¨ç†æ¨¡å‹ï¼Œæ¨ç†æ‰©å±•æ´è§ï¼ŒOpenAI æ‰©å±•å®šå¾‹

Gemini 2.0 Flash ä»¤äººå°è±¡æ·±åˆ»ï¼šä¸€ä½ç”¨æˆ·æŠ¥å‘Šè¯´ï¼Œé€šè¿‡ LlamaIndex å°è¯•äº†æ–°çš„ Gemini 2.0 Flash æ¨¡å‹ï¼Œæ³¨æ„åˆ°å®ƒçš„é€Ÿåº¦éå¸¸å¿«ï¼Œè™½ç„¶ä¸å¦‚ Groqã€‚
ä½œä¸º OpenRouter å¹³å°ä¸Šçš„æ–°æ¨¡å‹ï¼ŒGemini 2.0 Flash å¼•èµ·äº†ç”¨æˆ·çš„å¹¿æ³›å…³æ³¨ï¼Œå¤§å®¶éƒ½è¿«ä¸åŠå¾…åœ°æƒ³æµ‹è¯•å®ƒçš„æ€§èƒ½ã€‚
Flash-lite åœ¨ç»“æ„åŒ–è¾“å‡ºæ–¹é¢è¡¨ç°ä¸ä½³ï¼šå¦ä¸€ä½ç”¨æˆ·æŠ¥å‘Šè¯´ï¼ŒFlash-lite æ¨¡å‹åœ¨ç”Ÿæˆæœ‰æ•ˆçš„ç»“æ„åŒ–è¾“å‡ºæ—¶é‡åˆ°é—®é¢˜ï¼Œç»å¸¸äº§ç”Ÿæ— æ•ˆçš„ JSON æ ¼å¼ã€‚
ä»–ä»¬å¯¹æ­¤æ„Ÿåˆ°å¤±æœ›ï¼Œè®¤ä¸ºè¯¥æ¨¡å‹å¯èƒ½ä¸é€‚åˆéœ€è¦å¯é è¾“å‡ºçš„ä»»åŠ¡ã€‚
S1 æˆä¸ºä½æˆæœ¬çš„æ¨ç†æ›¿ä»£æ–¹æ¡ˆï¼šæœ€è¿‘çš„ä¸€ç¯‡åšæ–‡ä»‹ç»äº†ä¸€ç§ä½æˆæœ¬çš„ S1 æ¨ç†æ¨¡å‹ï¼ˆS1 reasoning modelï¼‰ã€‚è¯¥æ¨¡å‹å±•ç°å‡ºä¸ OpenAI çš„ o1 ç­‰æ¨¡å‹ç›¸ä¼¼çš„ competent æ€§èƒ½ï¼Œå¹¶ä¸”å¯ä»¥åœ¨æ™®é€šé…ç½®çš„æœºå™¨ä¸Šè¿è¡Œï¼Œè®­ç»ƒæˆæœ¬ä½äº 50 ç¾å…ƒã€‚
S1 æ¨¡å‹é€šè¿‡ Gemini 2.0 è’¸é¦è€Œæ¥ï¼Œå…¶æ¨¡å‹å’Œç›¸å…³å·¥å…·å·²åœ¨ GitHub ä¸Šå¼€æºã€‚
å…³äºæ¨ç†æ‰©å±•çš„è§è§£ï¼šè®¨è®ºæ­ç¤ºäº†å…³äºæ¨ç†æ‰©å±•çš„ä¸€äº›å‘ç°ï¼Œå³æ›´é•¿çš„æ€è€ƒæ—¶é—´å¯ä»¥æé«˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM/Large Language Modelï¼‰çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¦‚ä½•å®ç°æ›´é•¿çš„æ€è€ƒè¿‡ç¨‹å¼•èµ·äº†äººä»¬çš„è´¨ç–‘ã€‚
s1 è®ºæ–‡ç”¨å›¾è¡¨è¯´æ˜äº†è¿™ä¸€ç‚¹ï¼Œå¼•å‘äº†å…³äºå¦‚ä½•æœ‰æ•ˆå®æ–½æ­¤ç±»ç­–ç•¥çš„è®¨è®ºã€‚
å…³äº Flash åŠŸèƒ½çš„é—®é¢˜ï¼šç¤¾åŒºæˆå‘˜å¯¹æœ€è¿‘å‘å¸ƒçš„æ¨¡å‹çš„åŠŸèƒ½æå‡ºäº†ç–‘é—®ï¼ŒåŒ…æ‹¬ 2.0 pro å®éªŒç‰ˆæœ¬ï¼Œä»¥åŠæ˜¯å¦å­˜åœ¨ç”¨äºæµ‹è¯•çš„æç¤ºï¼ˆpromptï¼‰ã€‚
äººä»¬å¯¹æ–°æ¨¡å‹çš„ä»·å€¼å±•å¼€äº†è¾©è®ºï¼Œè®¨è®ºäº†ç°æœ‰æ¨¡å‹çš„è’¸é¦ç‰ˆæœ¬çš„æ½œåŠ›å’Œä»¥å¾€çš„ä½¿ç”¨ä½“éªŒã€‚
æåˆ°çš„é“¾æ¥:
Gemini Flash 2.0 - APIï¼ŒProvidersï¼ŒStatsï¼šGemini Flash 2.0 æä¾›çš„é¦–æ¬¡ä»¤ç‰Œæ—¶é—´ï¼ˆtime to first tokenï¼ŒTTFTï¼‰æ¯” Gemini Flash 1 å¿«å¾—å¤šã€‚
Googleï¼šGemini Flash Lite 2.0 Previewï¼ˆfreeï¼‰â€“ Run with an APIï¼šGemini Flash Lite 2.0 æä¾›çš„é¦–æ¬¡ä»¤ç‰Œæ—¶é—´ï¼ˆTTFTï¼‰æ¯” Gemini Flash 1.5 å¿«å¾—å¤šã€‚
Tweet from Omar Sansevieroï¼ˆ@osanseviero)ï¼šHey r/LocalLLaMA ğŸ‘‹We're cooking ğŸ«¡ Gemma going brrr
LLM Rankings | OpenRouterï¼šOpenRouter ä¸Šçš„å¤§è¯­è¨€æ¨¡å‹æ’è¡Œæ¦œ
S1ï¼šThe $6 R1 Competitor?ï¼šæœªæ‰¾åˆ°æè¿°
Researchers created an open rival to OpenAI's o1 'reasoning' model for under $50 | TechCrunchï¼šæ–¯å¦ç¦å¤§å­¦å’Œåç››é¡¿å¤§å­¦çš„ç ”ç©¶äººå‘˜ä»¥ä½äº 50 ç¾å…ƒçš„æˆæœ¬è®­ç»ƒäº†ä¸€ä¸ª AI æ¨ç†æ¨¡å‹ã€‚
Eleuther â–· #generalï¼ˆ22 æ¡æ¶ˆæ¯ğŸ”¥):
å¤§è¯­è¨€æ¨¡å‹ç ”ç©¶åˆä½œï¼ŒDeepspeed å’Œ Hugging Faceï¼Œå¤§è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ï¼Œå¾®è°ƒä¸­çš„æƒé‡è¡°å‡ï¼ŒRWKV æ¶æ„å¼€å‘

Seeking Collaboration on LLM Projects: A senior ML Engineer at Adobe expressed interest in exploring research projects related to LLM agents and invited collaboration with like-minded individuals.
Looking forward to some exciting discussions!
Clarifications on Deepspeed Usage: A user inquired about the need to specify batch_size for the data loader when using Deepspeed with auto batch sizing in the config.
Another member pointed out that the batch_size specified would still be required for the data loader.
New Thematic Generalization Benchmark Raised: A member shared a link to a GitHub repository discussing a thematic generalization benchmark designed to evaluate LLMs in inferring categories from examples and anti-examples.
They questioned if this benchmark might correlate with the performance of SAE autointerp.
Weight Decay's Role in Fine Tuning: A discussion arose regarding the appropriateness of using weight decay during fine-tuning, with one member affirming its common use.
Another user noted an interesting point from the OLMo2 paper, where weight decay wasn't applied to embedding parameters during pretraining.
RWKV Team Develops New Architectures: The RWKV team was mentioned as working on some exciting new architectures, indicating a proactive approach to model development.
A user shared their struggles with scaling and resource-intensive designs, inviting further discussion about potential collaboration.
Link mentioned: GitHub - lechmazur/generalization: Thematic Generalization Benchmark: measures how effectively various LLMs can infer a narrow or specific "theme" (category/rule) from a small set of examples and anti-examples, then detect which item truly fits that theme among a collection of misleading candidates.: Thematic Generalization Benchmark: measures how effectively various LLMs can infer a narrow or specific "theme" (category/rule) from a small set of examples and anti-examples, then d...

å¯»æ±‚åœ¨å¤§è¯­è¨€æ¨¡å‹é¡¹ç›®ä¸Šçš„åˆä½œï¼šAdobe çš„ä¸€ä½èµ„æ·±æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰å·¥ç¨‹å¸ˆè¡¨ç¤ºæœ‰å…´è¶£æ¢ç´¢ä¸å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ç›¸å…³çš„ç ”ç©¶é¡¹ç›®ï¼Œå¹¶é‚€è¯·å¿—åŒé“åˆçš„äººè¿›è¡Œåˆä½œã€‚
æœŸå¾…ä¸€äº›æ¿€åŠ¨äººå¿ƒçš„è®¨è®ºï¼
å…³äº Deepspeed ç”¨æ³•çš„æ¾„æ¸…ï¼šä¸€ä½ç”¨æˆ·è¯¢é—®åœ¨ä½¿ç”¨ Deepspeed æ—¶ï¼Œå¦‚æœåœ¨é…ç½®ä¸­ä½¿ç”¨è‡ªåŠ¨æ‰¹æ¬¡å¤§å°è°ƒæ•´ï¼Œæ˜¯å¦éœ€è¦ä¸ºæ•°æ®åŠ è½½å™¨æŒ‡å®š `batch_size`ã€‚
å¦ä¸€ä½æˆå‘˜æŒ‡å‡ºï¼Œä»ç„¶éœ€è¦ä¸ºæ•°æ®åŠ è½½å™¨æŒ‡å®š `batch_size`ã€‚
æ–°çš„ä¸»é¢˜æ³›åŒ–åŸºå‡†ï¼šä¸€ä½æˆå‘˜åˆ†äº«äº†ä¸€ä¸ª GitHub ä»“åº“çš„é“¾æ¥ï¼Œå…¶ä¸­ä»‹ç»äº†ä¸€ä¸ªä¸»é¢˜æ³›åŒ–åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ä»ç¤ºä¾‹å’Œåä¾‹ä¸­æ¨æ–­ä¸»é¢˜ç±»åˆ«çš„èƒ½åŠ›ã€‚
ä»–ä»¬è´¨ç–‘æ­¤åŸºå‡†æ˜¯å¦å¯èƒ½ä¸ SAEï¼ˆSparse Autoencoderï¼‰autointerp çš„æ€§èƒ½ç›¸å…³ã€‚
æƒé‡è¡°å‡åœ¨å¾®è°ƒä¸­çš„ä½œç”¨ï¼šè®¨è®ºäº†åœ¨å¾®è°ƒæœŸé—´ä½¿ç”¨æƒé‡è¡°å‡æ˜¯å¦åˆé€‚ï¼Œä¸€ä½æˆå‘˜ç¡®è®¤è¿™æ˜¯ä¸€ç§å¸¸ç”¨æ–¹æ³•ã€‚
å¦ä¸€ä½ç”¨æˆ·æ³¨æ„åˆ° OLMo2 è®ºæ–‡ä¸­ä¸€ä¸ªæœ‰è¶£çš„ç»†èŠ‚ï¼Œå³åœ¨é¢„è®­ç»ƒæœŸé—´ï¼Œæƒé‡è¡°å‡æ²¡æœ‰åº”ç”¨äºåµŒå…¥å‚æ•°ã€‚
RWKV å›¢é˜Ÿå¼€å‘æ–°çš„æ¶æ„ï¼šRWKV å›¢é˜Ÿæ­£åœ¨å¼€å‘ä¸€äº›ä»¤äººå…´å¥‹çš„æ–°æ¶æ„ï¼Œè¿™è¡¨æ˜ä»–ä»¬åœ¨æ¨¡å‹å¼€å‘æ–¹é¢é‡‡å–äº†ç§¯æä¸»åŠ¨çš„æ€åº¦ã€‚
ä¸€ä½ç”¨æˆ·åˆ†äº«äº†ä»–ä»¬åœ¨æ‰©å±•å’Œèµ„æºå¯†é›†å‹è®¾è®¡æ–¹é¢é‡åˆ°çš„æŒ‘æˆ˜ï¼Œå¹¶é‚€è¯·å¤§å®¶è¿›ä¸€æ­¥è®¨è®ºæ½œåœ¨çš„åˆä½œã€‚
æåˆ°çš„é“¾æ¥ï¼šGitHub - lechmazur/generalizationï¼šThematic Generalization Benchmarkï¼šmeasures how effectively various LLMs can infer a narrow or specificã€Œthemeã€ï¼ˆcategory/ruleï¼‰from a small set of examples and anti-examplesï¼Œthen detect which item truly fits that theme among a collection of misleading candidates.ï¼šThematic Generalization Benchmarkï¼šmeasures how effectively various LLMs can infer a narrow or specificã€Œthemeã€ï¼ˆcategory/ruleï¼‰from a small set of examples and anti-examplesï¼Œthen d...

Eleuther â–· #research (92 messagesğŸ”¥ğŸ”¥):
Multi Token Prediction Inference, Independent Research in AI/ML, A/B Testing and Reward Modeling, Quadratic Fitting for Parameter Estimation, DeepSeek MTP Implementation

Eleuther â–· #researchï¼ˆ92 æ¡æ¶ˆæ¯ğŸ”¥ğŸ”¥)ï¼š
å¤š Token é¢„æµ‹æ¨æ–­ï¼Œäººå·¥æ™ºèƒ½ / æœºå™¨å­¦ä¹ ï¼ˆAI/MLï¼‰ç‹¬ç«‹ç ”ç©¶ï¼ŒA/B æµ‹è¯•å’Œå¥–åŠ±å»ºæ¨¡ï¼Œå‚æ•°ä¼°è®¡çš„äºŒæ¬¡æ‹Ÿåˆæ–¹æ³•ï¼ŒDeepSeek MTP å®ç°

Understanding Multi Token Prediction for Inference: A question arose on how Multi Token Prediction (MTP) works during inference, particularly in relation to generating the initial token and using embeddings to improve speed.
Discussion highlighted that MTP serves as an efficient way to generate tokens, with resources shared about its implementation, including a Github pull request.
Realistic Domains for Independent AI Research: An independent researcher inquired about feasible areas to explore in AI/ML without the need for extensive funding, given various constraints on computing resources.
Responses suggested looking for grant opportunities and joining collaborative research groups, with mention of programs like Google's TRC.
A/B Testing and Parameter Estimation Challenges: The conversation shifted to the viability of using A/B testing to determine optimal sampler parameters, raising concerns over relying on traditional quadratic fittings.
It was suggested that employing a Reward Model could better capture user preferences, while acknowledging the complexity that bandit algorithms introduce into the approach.
Quadratic Fitting vs. Arbitrary Function Learning: A discussion on fitting a quadratic function to A/B data led to exploring concepts of reward modeling, symbolizing a possible avenue to refine the estimating process.
Participants pointed out the limitations of only fitting quadratics and discussing alternative methods like using pairwise preference models to optimize sampler parameters.
DeepSeek MTP's Surprising Outcomes: Insights were shared on the performance of the DeepSeek model, highlighting its implementation of MTP and its relative success in user-rated token generation.
Participants expressed curiosity about the effectiveness of the model while sharing resources and practical outcomes from their experiences with the underlying methodology.
Links mentioned:
LIMO: Less is More for Reasoning: We present a fundamental discovery that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks de...
TPU Research Cloud - About: no description found
bespokelabs/Bespoke-Stratos-17k Â· Datasets at Hugging Face: no description found
deepseek-ai/DeepSeek-R1-Distill-Qwen-32B Â· Hugging Face: no description found
arXiv user login: no description found
[Model][Speculative Decoding] DeepSeek MTP spec decode by luccafong Â· Pull Request #12755 Â· vllm-project/vllm: Implement DeepSeek MTP: #12181 to support DeepSeek MTP layers for next n prediction.
Eleuther â–· #interpretability-general (1 messages):
MATS cohort applications, Mechanistic Interpretability Research, Mentoring in AI research

ç†è§£æ¨ç†ä¸­çš„å¤š Token é¢„æµ‹ï¼šå‡ºç°äº†ä¸€ä¸ªå…³äºå¤š Token é¢„æµ‹ï¼ˆMTPï¼ŒMulti Token Predictionï¼‰åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¦‚ä½•å·¥ä½œçš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å…³äºç”Ÿæˆåˆå§‹ Token ä»¥åŠä½¿ç”¨åµŒå…¥æ¥æé«˜é€Ÿåº¦çš„é—®é¢˜ã€‚
è®¨è®ºå¼ºè°ƒäº† MTP æ˜¯ä¸€ç§æœ‰æ•ˆçš„ç”Ÿæˆ Token çš„æ–¹å¼ï¼Œå¹¶åˆ†äº«äº†å…³äºå…¶å®ç°çš„èµ„æºï¼ŒåŒ…æ‹¬ä¸€ä¸ª GitHub Pull Requestã€‚
ç‹¬ç«‹ AI ç ”ç©¶çš„ç°å®é¢†åŸŸï¼šä¸€ä½ç‹¬ç«‹ç ”ç©¶å‘˜è¯¢é—®äº†åœ¨äººå·¥æ™ºèƒ½ / æœºå™¨å­¦ä¹ ï¼ˆAI/MLï¼‰ä¸­æ¢ç´¢çš„å¯è¡Œé¢†åŸŸï¼Œåœ¨è®¡ç®—èµ„æºå—åˆ°å„ç§é™åˆ¶çš„æƒ…å†µä¸‹ï¼Œä¸éœ€è¦å¤§é‡çš„èµ„é‡‘ã€‚
å›åº”å»ºè®®å¯»æ‰¾èµ„åŠ©æœºä¼šå¹¶åŠ å…¥åˆä½œç ”ç©¶å°ç»„ï¼Œå¹¶æåŠäº†åƒ Google çš„ TRC è¿™æ ·çš„é¡¹ç›®ã€‚
A/B æµ‹è¯•å’Œå‚æ•°ä¼°è®¡æŒ‘æˆ˜ï¼šå¯¹è¯è½¬å‘äº†ä½¿ç”¨ A/B æµ‹è¯•æ¥ç¡®å®šæœ€ä½³é‡‡æ ·å™¨å‚æ•°çš„å¯è¡Œæ€§ï¼Œæå‡ºäº†å¯¹ä¾èµ–ä¼ ç»ŸäºŒæ¬¡æ‹Ÿåˆçš„æ‹…å¿§ã€‚
æœ‰äººå»ºè®®é‡‡ç”¨å¥–åŠ±æ¨¡å‹å¯ä»¥æ›´å¥½åœ°æ•æ‰ç”¨æˆ·åå¥½ï¼ŒåŒæ—¶ä¹Ÿæ‰¿è®¤äº† Bandit ç®—æ³•ï¼ˆbandit algorithmsï¼‰ç»™è¿™ç§æ–¹æ³•å¸¦æ¥çš„å¤æ‚æ€§ã€‚
äºŒæ¬¡æ‹Ÿåˆä¸ä»»æ„å‡½æ•°å­¦ä¹ ï¼šå…³äºå°†äºŒæ¬¡å‡½æ•°æ‹Ÿåˆåˆ° A/B æ•°æ®çš„è®¨è®ºï¼Œå¼•å‡ºäº†æ¢ç´¢å¥–åŠ±æ¨¡å‹æ¦‚å¿µï¼Œè±¡å¾ç€ä¸€ç§æ”¹è¿›ä¼°è®¡è¿‡ç¨‹çš„å¯èƒ½é€”å¾„ã€‚
å‚ä¸è€…æŒ‡å‡ºäº†ä»…æ‹ŸåˆäºŒæ¬¡å‡½æ•°çš„å±€é™æ€§ï¼Œå¹¶è®¨è®ºäº†æ›¿ä»£æ–¹æ³•ï¼Œä¾‹å¦‚ä½¿ç”¨åŸºäºç”¨æˆ·åå¥½å¯¹ï¼ˆpairwise preferenceï¼‰çš„æ¨¡å‹æ¥ä¼˜åŒ–é‡‡æ ·å™¨å‚æ•°ã€‚
DeepSeek MTP çš„æƒŠäººæˆæœï¼šåˆ†äº«äº†å…³äº DeepSeek æ¨¡å‹æ€§èƒ½çš„è§è§£ï¼Œå¼ºè°ƒäº†å…¶ MTP çš„å®ç°ä»¥åŠå…¶åœ¨ç”¨æˆ·è¯„ä»·çš„ Token ç”Ÿæˆæ–¹é¢çš„ç›¸å¯¹æˆåŠŸã€‚
å‚ä¸è€…å¯¹è¯¥æ¨¡å‹çš„æœ‰æ•ˆæ€§è¡¨ç¤ºå¥½å¥‡ï¼ŒåŒæ—¶åˆ†äº«äº†æ¥è‡ªä»–ä»¬ä½¿ç”¨åº•å±‚æ–¹æ³•ç»éªŒçš„èµ„æºå’Œå®è·µæˆæœã€‚

æåˆ°çš„é“¾æ¥ï¼š
LIMOï¼šLess is More for Reasoningï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ ¹æœ¬æ€§çš„å‘ç°ï¼ŒæŒ‘æˆ˜äº†æˆ‘ä»¬å¯¹å¤æ‚æ¨ç†å¦‚ä½•å‡ºç°åœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç†è§£ã€‚è™½ç„¶ä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºï¼Œå¤æ‚çš„æ¨ç†ä»»åŠ¡ä¼š...
TPU Research Cloud - About
bespokelabs/Bespoke-Stratos-17kÂ·Hugging Face çš„æ•°æ®é›†
deepseek-ai/DeepSeek-R1-Distill-Qwen-32BÂ·Hugging Face
arXiv ç”¨æˆ·ç™»å½•
[æ¨¡å‹][æ¨æµ‹è§£ç ] DeepSeek MTP spec è§£ç  by luccafongÂ·Pull Request #12755Â·vllm-project/vllmï¼šå®ç° DeepSeek MTPï¼š#12181 ä»¥æ”¯æŒä¸‹ä¸€ä¸ª n é¢„æµ‹çš„ DeepSeek MTP å±‚ã€‚
Eleuther çš„ #interpretability-general é¢‘é“ï¼ˆ1 æ¡æ¶ˆæ¯)ï¼š
MATS cohort applicationsï¼ŒMechanistic Interpretability Researchï¼ŒMentoring in AI research

Summer MATS Cohort Applications Open!: Applications for the MATS 8.0 cohort are now open, with submissions due by Feb 28. Interested candidates can apply here to participate in paid full-time mechanistic interpretability research.
The program welcomes applicants of all experience levels, and previous mentees have contributed to 10 top conference papers in the field.
Access MATS FAQ and Admissions Procedure: Details about the MATS admissions procedure and FAQ can be found in the linked document. Potential applicants are encouraged to sign in to review the comprehensive guidelines.
This resource aims to clarify the process for those interested in applying to the mentorship program.
Mentoring Success Stories: Over the years, Neel has mentored more than 40 mentees, contributing significantly to mechanistic interpretability research. This experience showcases the program's effectiveness in cultivating talent within the field.
Neel expressed pride in his mentees' success, specifically noting their contributions to major conferences, emphasizing that you don't need to be at a big lab to excel in this research area.
Links mentioned:
Neel Nanda MATS 8.0 Stream - Admissions Procedure + FAQ: Neel Nanda MATS 8.0 - Admission Procedure + FAQ Apply here Why Might I Want to Apply? Selection Question What should my application look like? Executive Summary Format Useful Resources What research p...
Tweet from Neel Nanda (@NeelNanda5): Apps are open for my MATS stream, where I try to teach how to do great mech interp research. Due Feb 28!I love mentoring and have had 40+ mentees, whoâ€™ve made valuable contributions to the field, incl...
Eleuther â–· #lm-thunderdome (7 messages):
cons@64, majority voting, eval configuration in YAML

Summer MATS Cohort Applications Open!ï¼šMATS 8.0 é¡¹ç›®çš„ç”³è¯·é€šé“å·²å¼€å¯ï¼Œæˆªæ­¢æ—¥æœŸä¸º 2 æœˆ 28 æ—¥ã€‚æ¬¢è¿æœ‰å…´è¶£çš„å€™é€‰äººåœ¨æ­¤å¤„ç”³è¯·å‚ä¸å…¨èŒå¸¦è–ªçš„æœºåˆ¶å¯è§£é‡Šæ€§ï¼ˆmechanistic interpretabilityï¼‰ç ”ç©¶ã€‚
è¯¥é¡¹ç›®æ¬¢è¿æ‰€æœ‰ç»éªŒæ°´å¹³çš„ç”³è¯·è€…ã€‚å¾€å±Šå­¦å‘˜å·²åœ¨è¯¥é¢†åŸŸçš„é¡¶çº§ä¼šè®®ä¸Šå‘è¡¨äº† 10 ç¯‡è®ºæ–‡ã€‚
Access MATS FAQ and Admissions Procedureï¼š æœ‰å…³ MATS çš„æ‹›ç”Ÿç¨‹åºå’Œå¸¸è§é—®é¢˜ï¼Œè¯·å‚é˜…æ­¤é“¾æ¥æ–‡æ¡£ã€‚é¼“åŠ±æœ‰æ„çš„ç”³è¯·è€…ç™»å½•ä»¥æŸ¥çœ‹è¯¦ç»†æŒ‡å—ã€‚
This resource aims to clarify the process for those interested in applying to the mentorship program.
Mentoring Success Storiesï¼šå¤šå¹´æ¥ï¼ŒNeel æŒ‡å¯¼äº† 40 å¤šåå­¦å‘˜ï¼Œä¸ºæœºåˆ¶å¯è§£é‡Šæ€§ç ”ç©¶åšå‡ºäº†é‡è¦è´¡çŒ®ã€‚è¿™å……åˆ†å±•ç¤ºäº†è¯¥é¡¹ç›®åœ¨åŸ¹å…»ç›¸å…³é¢†åŸŸäººæ‰æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚
Neel å¯¹ä»–æŒ‡å¯¼çš„å­¦å‘˜ä»¬å–å¾—çš„æˆå°±æ„Ÿåˆ°è‡ªè±ªï¼Œå¹¶ç‰¹åˆ«æåˆ°ä»–ä»¬åœ¨é‡è¦çš„ä¼šè®®ä¸Šæ‰€åšçš„è´¡çŒ®ã€‚ä»–å¼ºè°ƒï¼Œå³ä½¿ä¸åœ¨å¤§å‹å®éªŒå®¤ï¼Œä¹Ÿèƒ½åœ¨è¯¥ç ”ç©¶é¢†åŸŸä¸­å–å¾—å“è¶Šæˆå°±ã€‚

Links mentioned:

*  Neel Nanda MATS 8.0 Stream - Admissions Procedure + FAQï¼šNeel Nanda MATS 8.0 - Admission Procedure + FAQ Apply here Why Might I Want to Apply? Selection Question What should my application look like? Executive Summary Format Useful Resources What research p...
*  Tweet from Neel Nandaï¼ˆ@NeelNanda5)ï¼šApps are open for my MATS streamï¼Œwhere I try to teach how to do great mech interp research. Due Feb 28!I love mentoring and have had 40+ menteesï¼Œwho've made valuable contributions to the fieldï¼Œincl...
*  Eleuther â–· #lm-thunderdomeï¼ˆ7 messages):
  cons@64ï¼Œmajority votingï¼Œeval configuration in YAML

Clarification on cons@64 Terminology: Members discussed the meaning of cons@64, speculating if it refers to majority voting over 64 outputs or an LLM generating answers using those outputs.
Consensus and majority voting were identified as interchangeable terms in this context, as one member shared a link from OpenAI discussing the topic.
Expert Inquiry on Eval YAML Configuration: A member inquired about the possibility of automating the specification of apply chat template or fewshot-as-multiturn from within an eval .yaml file.
They wondered if this should be coded in utils.py to incorporate the mgsm_chat functionality into various evaluations.
Eleuther â–· #gpt-neox-dev (1 messages):
Sequence parallelism implementation, Model parallelism size issues, AttributeError in Megatron library, Training crash log

å…³äºæœ¯è¯­ cons@64 çš„è¯´æ˜ï¼š æˆå‘˜ä»¬è®¨è®ºäº† cons@64 çš„å«ä¹‰ï¼ŒçŒœæµ‹å®ƒæŒ‡çš„æ˜¯å¯¹ 64 ä¸ªè¾“å‡ºè¿›è¡Œå¤šæ•°æŠ•ç¥¨ï¼Œæˆ–è€…æ˜¯æŒ‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆ©ç”¨è¿™äº›è¾“å‡ºç”Ÿæˆç­”æ¡ˆã€‚
åœ¨è¿™ç§è¯­å¢ƒä¸‹ï¼Œå…±è¯†å’Œå¤šæ•°æŠ•ç¥¨å¯ä»¥äº’æ¢ä½¿ç”¨ï¼Œä¸€ä½æˆå‘˜åˆ†äº«äº† OpenAI å…³äºæ­¤ä¸»é¢˜çš„è®¨è®ºé“¾æ¥ã€‚
å…³äº Eval YAML é…ç½®çš„ä¸“å®¶å’¨è¯¢ï¼š ä¸€ä½æˆå‘˜è¯¢é—®æ˜¯å¦å¯ä»¥ä» eval .yaml æ–‡ä»¶ä¸­è‡ªåŠ¨è®¾å®š apply chat template æˆ– fewshot-as-multiturnã€‚
ä»–ä»¬æƒ³çŸ¥é“æ˜¯å¦åº”è¯¥åœ¨ utils.py ä¸­ç¼–å†™ä»£ç ï¼Œå°† mgsm_chat åŠŸèƒ½æ•´åˆåˆ°å„ç§è¯„ä¼°ä¸­ã€‚
EleutherAI â–· #gpt-neox-devï¼ˆ1 æ¡æ¶ˆæ¯)ï¼š
åºåˆ—å¹¶è¡Œå®ç°ã€æ¨¡å‹å¹¶è¡Œè§„æ¨¡é—®é¢˜ã€Megatron åº“ä¸­çš„ AttributeErrorã€è®­ç»ƒå´©æºƒæ—¥å¿—

Struggles with Sequence Parallelism and MP size 2: A user encountered a training crash when trying to enable sequence parallelism with a model parallelism size (MP) of 2, citing a specific error in the Megatron library.
The error traceback points to AttributeError: module 'megatron.mpu' has no attribute 'get_fp32_allreduce' indicating a potential issue in the implemented function.
Confusion over Documentation and Flag Use: The user expressed confusion over the documentation that suggests sequence parallelism should work with MP greater than 1 by merely turning a flag on.
This discrepancy raises questions about whether the documentation is inaccurate or if there is an existing issue in the implementation.
Link mentioned: aflah: Weights & Biases, developer tools for machine learning

åºåˆ—å¹¶è¡Œä¸æ¨¡å‹å¹¶è¡Œå¤§å°ä¸º 2 æ—¶çš„é—®é¢˜ï¼šä¸€ä½ç”¨æˆ·åœ¨å°è¯•å¯ç”¨åºåˆ—å¹¶è¡Œæ—¶é‡åˆ°äº†è®­ç»ƒå´©æºƒï¼Œå½“æ—¶æ¨¡å‹å¹¶è¡Œï¼ˆModel Parallelismï¼ŒMPï¼‰çš„å¤§å°è®¾ç½®ä¸º 2ã€‚ç”¨æˆ·æŒ‡å‡ºï¼Œå´©æºƒæºäº Megatron åº“ä¸­çš„ä¸€ä¸ªç‰¹å®šé”™è¯¯ã€‚
é”™è¯¯å›æº¯ä¿¡æ¯æ˜¾ç¤º `AttributeErrorï¼šmodule 'megatron.mpu' has no attribute 'get_fp32_allreduce'`ï¼Œè¿™è¡¨æ˜åœ¨å·²å®ç°çš„å‡½æ•°ä¸­å¯èƒ½å­˜åœ¨é—®é¢˜ã€‚
å¯¹äºæ–‡æ¡£å’Œæ ‡å¿—ä½¿ç”¨çš„ç–‘é—®ï¼šç”¨æˆ·è¡¨ç¤ºå¯¹æ–‡æ¡£çš„æè¿°æ„Ÿåˆ°å›°æƒ‘ï¼Œæ–‡æ¡£ä¸­æåˆ°ï¼Œåªéœ€æ‰“å¼€ä¸€ä¸ªæ ‡å¿—ï¼Œåºåˆ—å¹¶è¡Œå°±åº”è¯¥èƒ½ä¸å¤§äº 1 çš„ MP ä¸€èµ·å·¥ä½œã€‚
è¿™ç§æè¿°ä¸å®é™…æƒ…å†µçš„å·®å¼‚ï¼Œå¼•å‘äº†å…³äºæ–‡æ¡£æ˜¯å¦å‡†ç¡®ï¼Œæˆ–è€…ä»£ç å®ç°ä¸­æ˜¯å¦å­˜åœ¨ç¼ºé™·çš„ç–‘é—®ã€‚
ç›¸å…³é“¾æ¥ï¼šaflahï¼šWeights & Biasesï¼Œä¸€æ¬¾ç”¨äºæœºå™¨å­¦ä¹ çš„å¼€å‘è€…å·¥å…·ã€‚

Nous Research AI â–· #general (100 messagesğŸ”¥ğŸ”¥):
Deep Research Feedback, AI Backlash and Crypto, Purpose AI Agent in Trusts, New AI Models and Training Techniques, Fine-tuning Approaches

Nous Research AI â–· #generalï¼ˆ100 æ¡æ¶ˆæ¯ğŸ”¥ğŸ”¥):
æ·±åº¦ç ”ç©¶åé¦ˆï¼Œå¯¹ AI çš„æŠµåˆ¶ä¸åŠ å¯†è´§å¸ï¼Œç”¨äºä¿¡æ‰˜çš„ç›®çš„æ€§ AI æ™ºèƒ½ä½“ï¼ˆAI Agentï¼‰ï¼Œæ–°çš„ AI æ¨¡å‹å’Œè®­ç»ƒæŠ€æœ¯ï¼Œå¾®è°ƒï¼ˆFine-tuningï¼‰æ–¹æ³•

Deep Research receives positive reviews: Members shared their enthusiasm for OpenAI's Deep Research, highlighting its ability to efficiently pull relevant connections and sources, enhancing their cognitive bandwidth.
One user pointed out the model's capability to explore obscure online communities and gather unexpected data.
AI backlash tied to past tech controversies: Discussions surfaced regarding the public's distrust towards AI stemming from past negative experiences with cryptocurrency and NFTs, as some members believe this sentiment is impacting perceptions of AI technology.
Critics emphasized concerns around AI training data being unlicensed and its disruptive effects on labor markets.
Exploration of Purpose AI Agents: A user outlined their ambition to develop a purpose-driven AI agent within a legal trust framework, aiming to pioneer legal discourse around AI personhood.
Feedback focused on the engineering complexity involved, including integrating fiscal management functions while emphasizing the potential for custom software solutions.
Advancements in AI model merging and fine-tuning: Conversations included strategies for merging different AI models, with members sharing insights on improving model instruction tuning and reasoning performance.
Various fine-tuning methods were discussed, exploring the benefits of incorporating innovative techniques in AI training to enhance model performance.
Concerns over reasoning trace accessibility: Members expressed skepticism regarding the availability of reasoning traces from models like DeepSeek and feared that OpenAI may leverage them without providing API access.
The conversation highlighted the trend of major AI companies limiting access to advanced features and information, potentially to protect proprietary technology.
Links mentioned:
teknium/Llama-3.1-AlternateTokenizer Â· Hugging Face: no description found
Why Everyone Is Suddenly Mad at AI: AI Backlash: Another Tech Hype Hangover (and Is Crypto to Blame?)(OpenAI Deep research demo prompt: Write an essay on why there might be backlash to AI, and if it's related to NFT/crypto visibilit...
Upload folder using huggingface_hub Â· minpeter/Llama-3.2-1B-AlternateTokenizer-chatml at f2528b7: no description found
I Built the Ultimate Team of AI Agents in n8n With No Code (Free Template): ğŸ“Œ Join my free Skool community for the workflows shown in my videos! ğŸ‘‡https://www.skool.com/ai-automation-society/aboutğŸŒŸ Join my paid Skool community if y...
Google Colab: no description found
Google Colab: no description found
Unsloth Documentation: no description found
unsloth (Unsloth AI): no description found
Unsloth Documentation: no description found
Nous Research AI â–· #ask-about-llms (1 messages):
DeepSeek-R1 training loop, Reward loss vs KL loss sensitivity, Pitfalls of small instruct models, Model size considerations, Hyperparameter importance

æ·±åº¦ç ”ç©¶è·å¾—ç§¯æè¯„ä»·ï¼šç¤¾ç¾¤æˆå‘˜å¯¹ OpenAI çš„æ·±åº¦ç ”ç©¶å·¥å…·è¯„ä»·å¾ˆé«˜ï¼Œä»–ä»¬è®¤ä¸ºè¯¥å·¥å…·èƒ½å¤Ÿé«˜æ•ˆåœ°æå–ç›¸å…³ä¿¡æ¯å’Œèµ„æºï¼Œæœ‰æ•ˆæ‹“å±•äº†è®¤çŸ¥è¾¹ç•Œã€‚
æœ‰ç”¨æˆ·æŒ‡å‡ºï¼Œè¯¥æ¨¡å‹è¿˜èƒ½æ·±å…¥æ¢ç´¢ä¸€äº›é²œä¸ºäººçŸ¥çš„åœ¨çº¿ç¤¾ç¾¤ï¼Œå¹¶æœé›†åˆ°æ„æƒ³ä¸åˆ°çš„æ•°æ®ã€‚

å¯¹ç”Ÿæˆå¼ AIï¼ˆGenerative AIï¼‰çš„æŠµè§¦æƒ…ç»ªä¸è¿‡å¾€æŠ€æœ¯äº‰è®®æœ‰å…³ï¼šè®¨è®ºä¸­æåˆ°ï¼Œå…¬ä¼—å¯¹ AI çš„ä¸ä¿¡ä»»æ„Ÿï¼Œéƒ¨åˆ†æºäºä¹‹å‰åœ¨åŠ å¯†è´§å¸å’Œ NFT é¢†åŸŸçš„è´Ÿé¢ä½“éªŒã€‚ä¸€äº›æˆå‘˜è®¤ä¸ºï¼Œè¿™ç§æƒ…ç»ªæ­£åœ¨å½±å“äººä»¬å¯¹ AI æŠ€æœ¯çš„æ•´ä½“çœ‹æ³•ã€‚
æ‰¹è¯„è€…ä»¬ä¸»è¦æ‹…å¿ƒæœªç»æˆæƒçš„ AI è®­ç»ƒæ•°æ®ï¼Œä»¥åŠ AI å¯¹åŠ³åŠ¨åŠ›å¸‚åœºå¯èƒ½é€ æˆçš„å†²å‡»ã€‚

æ¢ç´¢ç‰¹å®šç”¨é€”çš„ AI æ™ºèƒ½ä½“ï¼šæœ‰ç”¨æˆ·è¡¨ç¤ºï¼Œå¸Œæœ›åœ¨æ³•å¾‹ä¿¡æ‰˜æ¡†æ¶ä¸‹å¼€å‘ä¸€ä¸ªç›®æ ‡æ˜ç¡®çš„ AI æ™ºèƒ½ä½“ï¼Œä»è€Œåœ¨ AI çš„ã€Œäººæ ¼åŒ–ã€æ–¹é¢ï¼Œå¼€åˆ›æ³•å¾‹æ¢è®¨çš„å…ˆæ²³ã€‚
å¤§å®¶æ™®éè®¤ä¸ºï¼Œè¿™åœ¨å·¥ç¨‹å®ç°ä¸Šå…·æœ‰ç›¸å½“çš„å¤æ‚æ€§ï¼Œä¾‹å¦‚éœ€è¦æ•´åˆè´¢åŠ¡ç®¡ç†åŠŸèƒ½ã€‚åŒæ—¶ï¼Œç¤¾ç¾¤ä¹Ÿå¼ºè°ƒäº†å®šåˆ¶åŒ–è½¯ä»¶è§£å†³æ–¹æ¡ˆçš„æ½œåŠ›ã€‚

AI æ¨¡å‹åˆå¹¶ä¸å¾®è°ƒæŠ€æœ¯çš„è¿›å±•ï¼šè®¨è®ºå†…å®¹åŒ…æ‹¬åˆå¹¶ä¸åŒ AI æ¨¡å‹çš„ç­–ç•¥ï¼Œç¤¾ç¾¤æˆå‘˜åˆ†äº«äº†å…³äºå¦‚ä½•æ”¹è¿›æ¨¡å‹æŒ‡ä»¤è°ƒæ•´å’Œæ¨ç†æ€§èƒ½çš„ç»éªŒã€‚
å¤§å®¶è¿˜è®¨è®ºäº†å„ç§å¾®è°ƒæ–¹æ³•ï¼Œæ¢ç´¢åœ¨ AI è®­ç»ƒä¸­å¼•å…¥åˆ›æ–°æŠ€æœ¯ï¼Œä»¥æå‡æ¨¡å‹æ€§èƒ½çš„ä¼˜åŠ¿ã€‚

å¯¹æ¨ç†è¿‡ç¨‹å¯è®¿é—®æ€§çš„æ‹…å¿§ï¼šç¤¾ç¾¤æˆå‘˜å¯¹ DeepSeek ç­‰æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹æ˜¯å¦èƒ½å¤Ÿå¼€æ”¾è®¿é—®è¡¨ç¤ºæ€€ç–‘ï¼Œå¹¶æ‹…å¿ƒ OpenAI å¯èƒ½ä¼šåœ¨ä¸æä¾›åº”ç”¨ç¨‹åºç¼–ç¨‹æ¥å£ï¼ˆAPIï¼‰è®¿é—®çš„æƒ…å†µä¸‹ï¼Œç›´æ¥åˆ©ç”¨è¿™äº›æ•°æ®ã€‚
è®¨è®ºä¸­ï¼Œå¤§å®¶é‡ç‚¹æåˆ°äº†å¤§å‹ AI å…¬å¸é™åˆ¶å¯¹é«˜çº§åŠŸèƒ½å’Œä¿¡æ¯è®¿é—®çš„è¶‹åŠ¿ï¼Œè¿™å¾ˆå¯èƒ½æ˜¯ä¸ºäº†ä¿æŠ¤å…¶ä¸“æœ‰æŠ€æœ¯ã€‚

ç›¸å…³é“¾æ¥ï¼š
* teknium/Llama-3.1-AlternateTokenizerÂ·Hugging Faceï¼š æœªæ‰¾åˆ°æè¿°
* Why Everyone Is Suddenly Mad at AIï¼šAI Backlashï¼šAnother Tech Hype Hangoverï¼ˆand Is Crypto to Blame?ï¼‰(OpenAI Deep research demo promptï¼šWrite an essay on why there might be backlash to AIï¼Œand if it's related to NFT/crypto visibilit...
* Upload folder using huggingface_hubÂ·minpeter/Llama-3.2-1B-AlternateTokenizer-chatml at f2528b7ï¼šæœªæ‰¾åˆ°æè¿°
* I Built the Ultimate Team of AI Agents in n8n With No Codeï¼ˆFree Template)ï¼šğŸ“Œ Join my free Skool community for the workflows shown in my videos! ğŸ‘‡https://www.skool.com/ai-automation-society/aboutğŸŒŸ Join my paid Skool community if y...
* Google Colabï¼š æœªæ‰¾åˆ°æè¿°
* Google Colabï¼š æœªæ‰¾åˆ°æè¿°
* Unsloth Documentationï¼š æœªæ‰¾åˆ°æè¿°
* unslothï¼ˆUnsloth AI)ï¼š æœªæ‰¾åˆ°æè¿°
* Unsloth Documentationï¼š æœªæ‰¾åˆ°æè¿°
* Nous Research AI â–· #ask-about-llmsï¼ˆ1 messages):
DeepSeek-R1 training loopï¼ŒReward loss vs KL loss sensitivityï¼ŒPitfalls of small instruct modelsï¼ŒModel size considerationsï¼ŒHyperparameter importance

DeepSeek-R1 Training Loop Insights: A user inquired about the sensitivity of the model to the weighting between reward loss and KL loss, questioning its significance as a hyperparameter.
They sought insights into which hyperparameters hold the most importance in optimizing the model's performance.
Concerns About Small Instruct Models: The user expressed interest in potential pitfalls when starting with a smaller instruct model like Qwen2.5 3B, compared to larger base models.
They emphasized a desire to find the smallest model that can still provide reliable testing and development for resource management.
Nous Research AI â–· #research-papers (1 messages):
Synthetic data generation, Seed-based approaches, Magpie output issues, Self-instruct alternatives, Awesome-LLM-Synthetic-Data resource

DeepSeek-R1 è®­ç»ƒè¿‡ç¨‹åˆ†æï¼šä¸€ä½ç”¨æˆ·è¯¢é—®äº†æ¨¡å‹å¯¹äºå¥–åŠ±æŸå¤±ï¼ˆreward lossï¼‰å’Œ KL æ•£åº¦æŸå¤±ï¼ˆKL lossï¼‰ä¹‹é—´æƒé‡åˆ†é…çš„æ•æ„Ÿç¨‹åº¦ï¼Œå¹¶æ¢è®¨è¿™ä¸ªæƒé‡ä½œä¸ºè¶…å‚æ•°æ˜¯å¦é‡è¦ã€‚
ä»–ä»¬å¸Œæœ›äº†è§£ï¼Œåœ¨ä¼˜åŒ–æ¨¡å‹æ€§èƒ½æ–¹é¢ï¼Œå“ªäº›è¶…å‚æ•°çš„å½±å“æœ€ä¸ºå…³é”®ã€‚
å…³äºä½¿ç”¨å°å‹æŒ‡ä»¤æ¨¡å‹çš„é¡¾è™‘ï¼šç”¨æˆ·è¡¨ç¤ºï¼Œç›¸æ¯”äºè¾ƒå¤§çš„åŸºç¡€æ¨¡å‹ï¼Œå¦‚æœä»åƒ Qwen2.5 3B è¿™æ ·çš„å°å‹æŒ‡ä»¤æ¨¡å‹å…¥æ‰‹ï¼Œå¯èƒ½å­˜åœ¨ä¸€äº›æ½œåœ¨é—®é¢˜ã€‚
ä»–ä»¬å¼ºè°ƒï¼Œå¸Œæœ›æ‰¾åˆ°ä¸€ä¸ªæœ€å°çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä»ç„¶å¯ä»¥ä¸ºèµ„æºç®¡ç†æä¾›å¯é çš„æµ‹è¯•å’Œå¼€å‘å¹³å°ã€‚
Nous Research AI çš„ #research-papers é¢‘é“ï¼ˆ1 æ¡æ¶ˆæ¯)ï¼š
åˆæˆæ•°æ®ç”Ÿæˆï¼ŒåŸºäºç§å­çš„æ–¹æ³•ï¼ŒMagpie output issuesï¼Œè‡ªæŒ‡ä»¤æ›¿ä»£æ–¹æ¡ˆï¼ŒAwesome-LLM-Synthetic-Data èµ„æº

Exploring Synthetic Data Generation Techniques: A member is seeking papers on synthetic data generation, particularly focusing on newer seed-based approaches similar to Self-Instruct.
They mentioned challenges with Magpie outputs due to experimenting in a non-English language.
Issues with Magpie Outputs: The member expressed frustration with the quality of outputs from Magpie when using seed system prompts, finding them unsatisfactory.
WizardLM has not been helpful as they require effective seed instructions to proceed.
Found Resource on LLM Synthetic Data: The member discovered a GitHub repository titled Awesome-LLM-Synthetic-Data which offers a list of resources on LLM-based synthetic data generation.
This resource aims to assist in understanding various techniques and methodologies in the domain, particularly for newer models.
Link mentioned: GitHub - wasiahmad/Awesome-LLM-Synthetic-Data: A reading list on LLM based Synthetic Data Generation ğŸ”¥: A reading list on LLM based Synthetic Data Generation ğŸ”¥ - wasiahmad/Awesome-LLM-Synthetic-Data

æ¢ç´¢åˆæˆæ•°æ®ç”ŸæˆæŠ€æœ¯ï¼šä¸€ä½æˆå‘˜æ­£åœ¨å¯»æ‰¾å…³äºåˆæˆæ•°æ®ç”Ÿæˆçš„è®ºæ–‡ï¼Œå°¤å…¶å¯¹ç±»ä¼¼äº Self-Instruct è¿™ç§åŸºäºæ–°ç§å­ï¼ˆseed-basedï¼‰çš„æ–¹æ³•æ„Ÿå…´è¶£ã€‚
ä»–ä»¬æåˆ°ç”±äºä½¿ç”¨éè‹±è¯­è¯­è¨€è¿›è¡Œå®éªŒï¼ŒMagpie çš„è¾“å‡ºæ•ˆæœå¹¶ä¸ç†æƒ³ã€‚
Magpie è¾“å‡ºçš„é—®é¢˜ï¼šè¯¥æˆå‘˜åœ¨ä½¿ç”¨ Magpie æ—¶ï¼Œå‘ç°å…¶åœ¨ç§å­ç³»ç»Ÿæç¤ºä¸‹çš„è¾“å‡ºè´¨é‡ä¸ä½³ï¼Œç»“æœå¹¶ä¸ç†æƒ³ã€‚
WizardLM æ²¡æœ‰æä¾›å¸®åŠ©ï¼Œå› ä¸ºä»–ä»¬éœ€è¦æœ‰æ•ˆçš„ç§å­æŒ‡ä»¤æ‰èƒ½ç»§ç»­ã€‚
å‘ç°çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åˆæˆæ•°æ®èµ„æºï¼šè¯¥æˆå‘˜å‘ç°äº†ä¸€ä¸ªåä¸º Awesome-LLM-Synthetic-Data çš„ GitHub å­˜å‚¨åº“ï¼Œå…¶ä¸­æä¾›äº†ä¸€ä¸ªå…³äºåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆæˆæ•°æ®ç”Ÿæˆçš„èµ„æºåˆ—è¡¨ã€‚
è¯¥èµ„æºæ—¨åœ¨å¸®åŠ©ç†è§£è¯¥é¢†åŸŸä¸­çš„å„ç§æŠ€æœ¯å’Œæ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯å¯¹äºè¾ƒæ–°çš„æ¨¡å‹ã€‚
æåˆ°çš„é“¾æ¥ï¼šGitHub - wasiahmad/Awesome-LLM-Synthetic-Dataï¼šA reading list on LLM based Synthetic Data Generation ğŸ”¥ - wasiahmad/Awesome-LLM-Synthetic-Data

Nous Research AI â–· #interesting-links (3 messages):
Deep Dive into LLMs, Mina's zkML Library

Explore AI with Deep Dive into LLMs: A [YouTube video titled
Mina's zkML Library Developer Guide: An article on Mina's Blog discusses the launch of the zkML library, which enables AI models to operate on-chain while maintaining full privacy and verification. This guide serves as a walkthrough for developers looking to leverage Mina's zkML capabilities for decentralized applications.
Link mentioned: Deep Dive into LLMs like ChatGPT: This is a general audience deep dive into the Large Language Model (LLM) AI technology that powers ChatGPT and related products. It is covers the full traini...

Nous Research AI â–· #interesting-linksï¼ˆ3 æ¡æ¶ˆæ¯):
æ·±å…¥äº†è§£å¤§è¯­è¨€æ¨¡å‹ï¼ŒMina çš„ zkML åº“æ¢ç´¢äººå·¥æ™ºèƒ½ï¼Œä»ã€Œæ·±å…¥äº†è§£å¤§è¯­è¨€æ¨¡å‹ã€å¼€å§‹ï¼šè¿™æ˜¯ä¸€ä¸ª YouTube è§†é¢‘ã€‚
Mina çš„ zkML åº“å¼€å‘è€…æŒ‡å—ï¼šMina åšå®¢ä¸Šçš„ä¸€ç¯‡æ–‡ç« ä»‹ç»äº† zkMLï¼ˆZero-Knowledge Machine Learningï¼‰åº“çš„å‘å¸ƒã€‚è¯¥åº“ä½¿äººå·¥æ™ºèƒ½æ¨¡å‹èƒ½å¤Ÿåœ¨é“¾ä¸Šè¿è¡Œï¼ŒåŒæ—¶ä¿æŒå®Œå…¨çš„éšç§å’ŒéªŒè¯ã€‚æœ¬æŒ‡å—ä¸ºå¸Œæœ›åˆ©ç”¨ Mina çš„ zkML åŠŸèƒ½è¿›è¡Œå»ä¸­å¿ƒåŒ–åº”ç”¨ï¼ˆDappï¼‰å¼€å‘çš„å¼€å‘è€…æä¾›äº†è¯¦ç»†çš„æ­¥éª¤è¯´æ˜ã€‚
ç›¸å…³é“¾æ¥ï¼šæ·±å…¥äº†è§£åƒ ChatGPT è¿™æ ·çš„å¤§è¯­è¨€æ¨¡å‹ï¼šè¿™æ˜¯ä¸€ä¸ªé¢å‘å¤§ä¼—çš„ï¼Œå¯¹é©±åŠ¨ ChatGPT å’Œç›¸å…³äº§å“çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰äººå·¥æ™ºèƒ½æŠ€æœ¯çš„æ·±å…¥è§£è¯»ï¼Œå†…å®¹æ¶µç›–å¤§è¯­è¨€æ¨¡å‹çš„å®Œæ•´è®­ç»ƒæµç¨‹â€¦â€¦

Nous Research AI â–· #research-papers (1 messages):
Synthetic Data Generation, Self-instruct, Magpie, WizardLM, Awesome LLM Synthetic Data

Seeking Seed-based Synthetic Data Solutions: A user is looking for papers or directions on synthetic data generation, specifically seed-based methods like Self-instruct, to improve their results with Magpie.
They noted that outputs using a non-English language are not satisfactory and are seeking better seed instructions beyond what WizardLM offers.
Found Resource on GitHub for Synthetic Data: The user discovered a GitHub repository titled Awesome-LLM-Synthetic-Data, which provides a reading list on LLM-based synthetic data generation.
The repository emphasizes various resources and is now a potential avenue for the user to explore alternatives for their data generation needs.
Link mentioned: GitHub - wasiahmad/Awesome-LLM-Synthetic-Data: A reading list on LLM based Synthetic Data Generation ğŸ”¥: A reading list on LLM based Synthetic Data Generation ğŸ”¥ - wasiahmad/Awesome-LLM-Synthetic-Data

Nous Research AI â–· #research-papersï¼ˆ1 æ¡æ¶ˆæ¯):
åˆæˆæ•°æ®ç”Ÿæˆï¼ŒSelf-instructï¼ŒMagpieï¼ŒWizardLMï¼ŒAwesome LLM Synthetic Data

å¯»æ±‚åŸºäºç§å­çš„åˆæˆæ•°æ®è§£å†³æ–¹æ¡ˆï¼šä¸€ä½ç”¨æˆ·æ­£åœ¨å¯»æ‰¾å…³äºåˆæˆæ•°æ®ç”Ÿæˆçš„è®ºæ–‡æˆ–æŒ‡å¯¼ï¼Œç‰¹åˆ«æ˜¯åƒ Self-instruct è¿™æ ·çš„åŸºäºç§å­çš„æ–¹æ³•ï¼Œä»¥æ”¹å–„å…¶ä½¿ç”¨ Magpie çš„æ•ˆæœã€‚
ä»–ä»¬è¡¨ç¤ºï¼Œä½¿ç”¨éè‹±è¯­è¯­è¨€çš„è¾“å‡ºç»“æœä¸å°½å¦‚äººæ„ï¼Œå¸Œæœ›æ‰¾åˆ°æ¯” WizardLM æ•ˆæœæ›´å¥½çš„ç§å­æŒ‡ä»¤ã€‚
GitHub ä¸Šçš„åˆæˆæ•°æ®èµ„æºï¼šè¯¥ç”¨æˆ·å‘ç°äº†ä¸€ä¸ª GitHub ä»“åº“ï¼Œåä¸º Awesome-LLM-Synthetic-Dataï¼Œå®ƒæä¾›äº†ä¸€ä»½å…³äºåŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åˆæˆæ•°æ®ç”Ÿæˆçš„é˜…è¯»æ¸…å•ã€‚
è¿™ä¸ªä»“åº“æ±‡é›†äº†ä¸°å¯Œçš„èµ„æºï¼Œä¸ºç”¨æˆ·æ¢ç´¢æ•°æ®ç”Ÿæˆæ–¹æ¡ˆæä¾›äº†æ–°çš„å¯èƒ½ã€‚
æåˆ°çš„é“¾æ¥ï¼šGitHub - wasiahmad/Awesome-LLM-Synthetic-Dataï¼šA reading list on LLM based Synthetic Data Generation ğŸ”¥ï¼šA reading list on LLM based Synthetic Data Generation ğŸ”¥ - wasiahmad/Awesome-LLM-Synthetic-Data

Interconnects (Nathan Lambert) â–· #news (39 messagesğŸ”¥):
John Schulman leaves Anthropic, Hibiki speech-to-speech translation model, Le Chat AI sidekick, GitHub Copilot agent mode, OpenAI updated chain of thought

æ¶ˆæ¯äº’è”ï¼ˆNathan Lambertï¼‰â–· #æ–°é—»é€Ÿé€’ï¼ˆ39 æ¡æ¶ˆæ¯ğŸ”¥)ï¼š
John Schulman ç¦»å¼€ Anthropicï¼ŒHibiki è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘æ¨¡å‹ï¼ŒLe Chat AI åŠ©æ‰‹ï¼ŒGitHub Copilot æ™ºèƒ½ä½“æ¨¡å¼ï¼ŒOpenAI æ›´æ–°çš„æ€ç»´é“¾ã€‚

*  John Schulman ç¦»å¼€ Anthropic
*  Hibiki è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘æ¨¡å‹
*  Le Chat AI åŠ©æ‰‹
*  GitHub Copilot æ™ºèƒ½ä½“æ¨¡å¼
*  OpenAI æ›´æ–°çš„æ€ç»´é“¾ï¼ˆChain of Thought)

John Schulman departs Anthropic: Leading AI researcher and OpenAI co-founder John Schulman has left Anthropic after around five months, raising questions about his next move link.
Speculations about his potential next steps included thoughts about positions at organizations like Deepseek and AI2.
Hibiki revolutionizes translation: The new Hibiki model from Kyutai Labs supports simultaneous speech-to-speech translation and adapts its pace based on content link.
It reportedly outperforms previous systems in quality, naturalness, and speaker similarity, approaching human interpreter capabilities.
Le Chat AI officially launched: MistralAI has launched Le Chat, marketed as a comprehensive AI sidekick for work and life, now available on both web and mobile link.
This new tool aims to enhance productivity and personal assistance through AI.
GitHub Copilot unleashes agent mode: GitHub announced the introduction of agent mode for Copilot, designed to assist developers more effectively with coding link.
This update also includes the general availability of Copilot Edits, enhancing the developer experience.
OpenAI enhances chain of thought: OpenAI updated the chain of thought mechanism in o3-mini models, aiming to refine the user experience, though they emphasize these aren't the raw CoTs link.
The conversation suggests this may lead to better distillation outputs, although the significance of certain tokens remains debated.
Links mentioned:
Tweet from OpenAI (@OpenAI): Updated chain of thought in OpenAI o3-mini for free and paid users, and in o3-mini-high for paid users.
GitHub Copilot: The agent awakens: Introducing agent mode for GitHub Copilot in VS Code, announcing the general availability of Copilot Edits, and providing a first look at our SWE agent.
Tweet from Xeophon (@TheXeophon): @apples_jimmy I would be so hard if John joins Ai2
Tweet from kyutai (@kyutai_labs): Meet Hibiki, our simultaneous speech-to-speech translation model, currently supporting ğŸ‡«ğŸ‡·â¡ï¸ğŸ‡¬ğŸ‡§.Hibiki produces spoken and text translations of the input speech in real-time, while preserving the sp...
Tweet from Noam Brown (@polynoamial): When we briefed people on ğŸ“ before o1-preview's release, seeing the CoT live was usually the "aha" moment for them that made it clear this was going to be a big deal. These aren't th...
Tweet from Shirin Ghaffary (@shiringhaffary): Leading AI researcher and OpenAI co-founder John Schulman has left Anthropic after around five months working at the company https://www.bloomberg.com/news/articles/2025-02-06/openai-co-founder-john-s...
Tweet from Mistral AI (@MistralAI): Introducing the all new Le Chat: your ultimate AI sidekick for life and work! Now live on web and mobile!
Interconnects (Nathan Lambert) â–· #ml-questions (3 messages):
LRMs test-time scaling, Model decision-making, Training phase scaling

John Schulman ç¦»å¼€ Anthropicï¼šçŸ¥å AI ç ”ç©¶å‘˜ã€OpenAI è”åˆåˆ›å§‹äºº John Schulman åœ¨ Anthropic å·¥ä½œçº¦äº”ä¸ªæœˆåç¦»èŒï¼Œå¼•å‘äººä»¬å¯¹å…¶ä¸‹ä¸€æ­¥åŠ¨å‘çš„çŒœæµ‹ linkã€‚
å…³äºä»–æœªæ¥å¯èƒ½å»å‘çš„çŒœæµ‹åŒ…æ‹¬åŠ å…¥ Deepseekã€AI2 ç­‰æœºæ„ã€‚
Hibiki å˜é©ç¿»è¯‘æ–¹å¼ï¼šKyutai Labs æ¨å‡ºçš„æ–°å‹ Hibiki æ¨¡å‹æ”¯æŒåŒå£°ä¼ è¯‘ï¼Œèƒ½å¤Ÿæ ¹æ®å†…å®¹è‡ªåŠ¨è°ƒæ•´è¯­é€Ÿ linkã€‚
æ®ç§°ï¼Œè¯¥æ¨¡å‹åœ¨ç¿»è¯‘è´¨é‡ã€è‡ªç„¶åº¦ä»¥åŠè¯´è¯äººå£°éŸ³ç›¸ä¼¼åº¦ç­‰æ–¹é¢å‡ä¼˜äºä»¥å¾€ç³»ç»Ÿï¼Œæ€§èƒ½æ¥è¿‘äººç±»ç¿»è¯‘ã€‚
Le Chat AI æ­£å¼å‘å¸ƒï¼šMistralAI å‘å¸ƒäº† Le Chatï¼Œå®šä½ä¸ºä¸€æ¬¾å…¨é¢çš„ AI åŠ©æ‰‹ï¼Œæ—¨åœ¨ä¸ºç”¨æˆ·çš„å·¥ä½œå’Œç”Ÿæ´»æä¾›ä¾¿åˆ©ï¼Œç›®å‰å·²åœ¨ç½‘é¡µå’Œç§»åŠ¨ç«¯ä¸Šçº¿ linkã€‚
è¿™æ¬¾æ–°å·¥å…·æ—¨åœ¨é€šè¿‡ AI æå‡ç”Ÿäº§åŠ›ï¼Œå¹¶æä¾›ä¸ªäººåŠ©ç†æœåŠ¡ã€‚
GitHub Copilot å¯ç”¨ Agent æ¨¡å¼ï¼šGitHub å®£å¸ƒä¸º Copilot å¼•å…¥ Agent æ¨¡å¼ï¼Œæ—¨åœ¨æ›´æœ‰æ•ˆåœ°è¾…åŠ©å¼€å‘è€…è¿›è¡Œä»£ç ç¼–å†™ linkã€‚
åŒæ—¶ï¼ŒCopilot Edits ä¹Ÿæ­£å¼ä¸Šçº¿ï¼Œè¿›ä¸€æ­¥æå‡äº†å¼€å‘è€…çš„ä½¿ç”¨ä½“éªŒã€‚
OpenAI ä¼˜åŒ–æ€ç»´é“¾ï¼ˆChain of Thoughtï¼ŒCoT)ï¼šOpenAI æ›´æ–°äº† o3-mini æ¨¡å‹ä¸­çš„æ€ç»´é“¾æœºåˆ¶ï¼Œä»¥æ”¹å–„ç”¨æˆ·ä½“éªŒã€‚ä½†å®˜æ–¹å¼ºè°ƒï¼Œè¿™äº›å¹¶éåŸå§‹çš„ CoT linkã€‚
ç›¸å…³è®¨è®ºè¡¨æ˜ï¼Œè¿™å¯èƒ½ä¼šå¸¦æ¥æ›´å¥½çš„ç²¾ç‚¼è¾“å‡ºï¼Œä½†æŸäº› Token çš„é‡è¦æ€§ä»æœ‰å¾…å•†æ¦·ã€‚

æåŠçš„é“¾æ¥ï¼š
Tweet from OpenAIï¼ˆ@OpenAI)ï¼šæ›´æ–°äº† OpenAI o3-mini ä¸­å…è´¹å’Œä»˜è´¹ç”¨æˆ·çš„æ€ç»´é“¾ï¼Œä»¥åŠ o3-mini-high ä¸­ä»˜è´¹ç”¨æˆ·çš„æ€ç»´é“¾ã€‚
GitHub Copilotï¼šAgent è§‰é†’ï¼šåœ¨ VS Code ä¸­ä¸º GitHub Copilot å¼•å…¥ Agent æ¨¡å¼ï¼Œå®£å¸ƒ Copilot Edits çš„å…¨é¢å¯ç”¨æ€§ï¼Œå¹¶é¦–æ¬¡å±•ç¤ºæˆ‘ä»¬çš„ SWE ä»£ç†ã€‚
Tweet from Xeophonï¼ˆ@TheXeophon)ï¼š@apples_jimmy å¦‚æœ John åŠ å…¥ Ai2ï¼Œæˆ‘ä¼šéå¸¸æ¿€åŠ¨
Tweet from kyutaiï¼ˆ@kyutai_labs)ï¼šè®¤è¯† Hibikiï¼Œæˆ‘ä»¬çš„åŒå£°è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘æ¨¡å‹ï¼Œç›®å‰æ”¯æŒ ğŸ‡«ğŸ‡·â¡ï¸ğŸ‡¬ğŸ‡§ã€‚Hibiki å®æ—¶ç”Ÿæˆè¾“å…¥è¯­éŸ³çš„å£è¯­å’Œæ–‡æœ¬ç¿»è¯‘ï¼ŒåŒæ—¶ä¿ç•™ sp...
Tweet from Noam Brownï¼ˆ@polynoamial)ï¼šå½“æˆ‘ä»¬åœ¨ o1-preview å‘å¸ƒä¹‹å‰å‘äººä»¬ä»‹ç» ğŸ“ æ—¶ï¼Œçœ‹åˆ° CoT å®æ—¶é€šå¸¸æ˜¯ä»–ä»¬çš„ã€Œé¡¿æ‚Ÿã€æ—¶åˆ»ï¼Œè¿™è®©ä»–ä»¬æ¸…æ¥šåœ°æ„è¯†åˆ°è¿™å°†æ˜¯ä¸€ä»¶å¤§äº‹ã€‚è¿™äº›ä¸æ˜¯...
Tweet from Shirin Ghaffaryï¼ˆ@shiringhaffary)ï¼šé¢†å…ˆçš„ AI ç ”ç©¶å‘˜å’Œ OpenAI è”åˆåˆ›å§‹äºº John Schulman åœ¨ Anthropic å·¥ä½œäº†å¤§çº¦äº”ä¸ªæœˆåç¦»å¼€äº†è¯¥å…¬å¸ https://www.bloomberg.com/news/articles/2025-02-06/openai-co-founder-john-s...
Tweet from Mistral AIï¼ˆ@MistralAI)ï¼šæ¨å‡ºå…¨æ–°çš„ Le Chatï¼šæ‚¨å·¥ä½œå’Œç”Ÿæ´»çš„ç»ˆæ AI åŠ©æ‰‹ï¼ç°åœ¨å¯åœ¨ Web å’Œç§»åŠ¨è®¾å¤‡ä¸Šä½¿ç”¨ï¼
Interconnectsï¼ˆNathan Lambertï¼‰â–· #ml-questionsï¼ˆ3 messages):
LRMs æµ‹è¯•æ—¶ç¼©æ”¾ã€æ¨¡å‹å†³ç­–ã€è®­ç»ƒé˜¶æ®µç¼©æ”¾

Confusion over LRMs Test-Time Scaling: A member questioned the term test-time scaling for Long-Range Models (LRMs), noting that models decide their own output without external control.
They highlighted that the scaling occurs during the training phase, prompting a broader discussion about the terminology used.
Concerns about LRM Control: Another member dismissed the entire concept, expressing that the discussions around test-time computing for LRMs are fundamentally flawed.
This sentiment emphasizes skepticism towards the efficacy and clarity of the term in the context of autonomous model behaviors.
Interconnects (Nathan Lambert) â–· #ml-drama (9 messagesğŸ”¥):
Crowd-sourced prompts, Jailbreaking models, Open Source Community, Incentives in AI

æœªæ‰¾åˆ°æ„è¯‘å†…å®¹

Concerns Over Crowd-sourced Expertise: A prominent member expressed disdain about providing expertise for crowd-sourced prompts that appear to serve investors by promoting safety falsely, stating, 'Iâ€™m allergic to money, so donâ€™t bother.'
This raises questions about whether genuine community benefit is prioritized over profit motives in AI development.
Skepticism on Achievement in AI Levels: A member questioned why, if an individual could complete all 8 levels, they hadn't done so yet, suggesting there may be limitations beyond frontend bugs.
Another noted the individual's extensive work on jailbreaking existing models, implying they've likely encountered challenges before.
Work vs. Free Contribution Debate: Discussion arose around the idea that contributions to AI should not be compelled as free labor, with a member saying, 'this is a job. you are trying to get me to do work. for free.'
This highlights the tension between community contributions and the expectations placed on individuals within the open-source landscape.
Entertainment in the Open Source Community: A member remarked on the humor found within the open-source community, indicating that open source tends to encourage amusing interactions.
They referred to humorous replies received on Bluesky, specifically citing a comment about Europe potentially taking initiatives instead of the US.
Link mentioned: Tweet from Pliny the Liberator ğŸ‰ (@elder_plinius): I donâ€™t want to provide my world-class expertise just for you to hoard crowd-sourced prompts and construct elaborate security theater performances to appease investors who are foolish enough to believ...

å¯¹ä¼—åŒ…ä¸“ä¸šçŸ¥è¯†çš„é¡¾è™‘ï¼šä¸€ä½çŸ¥åäººå£«å¯¹æä¾›ä¼—åŒ…æç¤ºç›¸å…³çš„ä¸“ä¸šçŸ¥è¯†è¡¨ç¤ºä¸å±‘ï¼Œå› ä¸ºè¿™äº›æç¤ºçœ‹èµ·æ¥åƒæ˜¯é€šè¿‡è™šå‡å®£ä¼ å®‰å…¨æ€§æ¥å¸®åŠ©æŠ•èµ„è€…ã€‚ä»–è¯´ï¼šã€Œæˆ‘å¯¹é’±è¿‡æ•ï¼Œæ‰€ä»¥åˆ«æ¥çƒ¦æˆ‘ã€‚ã€
è¿™å¼•å‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šåœ¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å¼€å‘ä¸­ï¼ŒçœŸæ­£çš„ç¤¾åŒºåˆ©ç›Šæ˜¯å¦æ¯”é€åˆ©åŠ¨æœºæ›´é‡è¦ï¼Ÿ
å¯¹ AI æ°´å¹³è¾¾æˆçš„è´¨ç–‘ï¼šæœ‰æˆå‘˜è´¨ç–‘ï¼Œå¦‚æœæœ‰äººèƒ½å®Œæˆæ‰€æœ‰ 8 ä¸ªç­‰çº§çš„æŒ‘æˆ˜ï¼Œä¸ºä»€ä¹ˆè¿Ÿè¿Ÿæ²¡æœ‰è¡ŒåŠ¨ï¼Ÿè¿™æš—ç¤ºå¯èƒ½å­˜åœ¨å‰ç«¯é”™è¯¯ä¹‹å¤–çš„é™åˆ¶ã€‚
å¦ä¸€ä½æˆå‘˜æŒ‡å‡ºï¼Œæ­¤äººåœ¨ç ´è§£ç°æœ‰æ¨¡å‹ä¸ŠæŠ•å…¥äº†å¤§é‡ç²¾åŠ›ï¼Œè¿™æ„å‘³ç€ä»–ä»¬å¯èƒ½ä¹‹å‰å°±é‡åˆ°è¿‡ç±»ä¼¼éš¾é¢˜ã€‚
å…³äºå·¥ä½œä¸å…è´¹è´¡çŒ®çš„äº‰è®ºï¼šæœ‰äººæå‡ºï¼Œä¸åº”å¼ºè¿«å¤§å®¶æ— å¿è´¡çŒ® AI ç›¸å…³å·¥ä½œã€‚ä¸€ä½æˆå‘˜ç›´è¨€ï¼šã€Œè¿™æ˜¯ä¸€ä»½å·¥ä½œï¼Œä½ æƒ³è®©æˆ‘å…è´¹æ‰“å·¥ã€‚ã€
è¿™å‡¸æ˜¾äº†åœ¨å¼€æºç¤¾åŒºä¸­ï¼Œç¤¾åŒºè´¡çŒ®å’Œä¸ªäººæœŸæœ›ä¹‹é—´çš„çŸ›ç›¾ã€‚
å¼€æºç¤¾åŒºçš„ä¹è¶£ï¼šæœ‰æˆå‘˜æåˆ°å¼€æºç¤¾åŒºä¸­ä¸ä¹å¹½é»˜ï¼Œå¼€æºå¾€å¾€èƒ½å‚¬ç”Ÿæœ‰è¶£çš„äº’åŠ¨ã€‚
ä»–ä»¬æåˆ°äº†åœ¨ Bluesky ä¸Šæ”¶åˆ°çš„å¹½é»˜å›å¤ï¼Œç‰¹åˆ«æ˜¯æœ‰äººè¯„è®ºè¯´æ¬§æ´²å¯èƒ½ä¼šç‡å…ˆé‡‡å–è¡ŒåŠ¨ï¼Œè€Œä¸æ˜¯ç¾å›½ã€‚
ç›¸å…³é“¾æ¥ï¼šPliny the Liberator ğŸ‰ï¼ˆ@elder_pliniusï¼‰åœ¨ Twitter ä¸Šå‘å¸ƒï¼šæˆ‘ä¸æƒ³è´¡çŒ®æˆ‘ä¸–ç•Œçº§çš„ä¸“ä¸šçŸ¥è¯†ï¼Œåªæ˜¯ä¸ºäº†è®©ä½ ä»¬å›¤ç§¯ä¼—åŒ…æç¤ºï¼Œç„¶åç²¾å¿ƒè®¾è®¡è™šå‡çš„å®‰å…¨æ¼”ç¤ºæ¥æ¬ºéª—é‚£äº›æ„šè ¢çš„æŠ•èµ„è€…...

Interconnects (Nathan Lambert) â–· #random (23 messagesğŸ”¥):
ChatGPT Fishing Techniques, Long Chain of Thought in LLMs, Qwen Model Discoveries, Deep Research Applications

äº’è”ï¼Œä½œè€… Nathan Lambert â–· #éšæœºï¼ˆ23 æ¡æ¶ˆæ¯ğŸ”¥)ï¼š
ChatGPT é’“é±¼æŠ€æœ¯ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„é•¿é“¾æ¨ç†ï¼ŒQwen æ¨¡å‹çš„æ¢ç´¢å‘ç°ï¼Œæ·±åº¦ç ”ç©¶çš„åº”ç”¨æ¡ˆä¾‹

Fishing with ChatGPT for Halibut: The YouTube video titled 'Fishing for first timers' showcases using ChatGPT in the pursuit of catching halibut.
A humorous note was made about using o3 to catch crabs, referring to the inquiry's lighter side.
Understanding Long CoT Reasoning in LLMs: A discussion on demystifying Long CoT Reasoning highlighted the mystery behind models like R1, O1, and O3, pursuing insights into their training dynamics.
11 major takeaways from the thread were noted, suggesting a detailed exploration of the topic.
Qwen Models' Surprising Results: Recent discussions pointed out that Qwen 2.5 models achieved impressive results with minimal training data, as noted by various members discussing their findings.
A quote by Aran Komatsuzaki emphasized that Qwen models seem to possess a magical quality, achieving notably good performance with limited data.
Gary's Praise for Deep Research: Gary Marcus remarked on the utility of Deep Research, pointing out its practicality despite still facing challenges in facts and temporal reasoning.
A community consensus emerged recognizing Deep Research's strengths in specific applications while acknowledging its shortcomings in factual accuracy.
Accessing O3 Efficiently: A member shared a technique for utilizing O3 effectively by bypassing the browsing function, calling it a useful coding trick.
This method reportedly aids in gathering and implementing solutions accurately in a single attempt, boosting coding efficiency.
Links mentioned:
Tweet from Big Tech Alert (@BigTechAlert): ğŸš« @allen_ai is no longer following @ehsanik(ğŸ¤–ğŸ’­: who has more details?)
Tweet from Zeyuan Allen-Zhu, Sc.D. (@ZeyuanAllenZhu): (2/8) Just like P vs NP, reviewing paper (L3) requires less intelligence than authoring one (L4). Auditing a review only needs L2; arbitrating author-reviewer dispute needs L1 --- only need to follow ...
Tweet from Omar Khattab (@lateinteraction): So many "we did almost nothing and now Qwen 2.5 can do everything" results ğŸ˜†
Tweet from Stefan Streichsbier (@s_streichsbier): Thanks, @iruletheworldmo.This is a very useful trick to access the full o3 for coding.It researches a bunch of sources for how to implement the solution and then puts it together correctly in 1 shot.W...
Tweet from Gary Marcus (@GaryMarcus): Deep Research is genuinely useful - depending on your application - but crucially (as anticipated by Rebooting AI in 2019, and by @yudapearl) facts and temporal reasoning remain problematic for curr...
Tweet from Zeyuan Allen-Zhu, Sc.D. (@ZeyuanAllenZhu): Donâ€™t let ridiculous ICLR reviewers get you downâ€”this happens even on ACs. Our paper (8,8,6,3) was unilaterally rejected by a meta-reviewer. Thankfully, ICLR is open-review so this misbehavior will be...
Tweet from Omar Khattab (@lateinteraction): More Qwen. I'm increasingly comfortable saying these papers seem to be a discovery of some sort about Qwen models, not necessarily about reasoning.Quoting Aran Komatsuzaki (@arankomatsuzaki) LIMO:...
Tweet from Zeyuan Allen-Zhu, Sc.D. (@ZeyuanAllenZhu): (1/8) We classify L1~L5 intelligence, and observe only Gemini-2-FT, DeepSeek-R1, OpenAI-o1 can reach L2; most are only L1 (o3-mini). Yet, one can still use L1-level AIs to arbitrate disputes and ensur...
Tweet from Xiang Yue (@xiangyue96): Demystifying Long CoT Reasoning in LLMshttps://arxiv.org/pdf/2502.03373Reasoning models like R1 / O1 / O3 have gained massive attention, but their training dynamics remain a mystery. We're taking ...
Tweet from Wenhu Chen (@WenhuChen): I agree. It's pretty much the same discovery as s1, which arrives at the similar results with around 1000 training examples. We have actually tried training the other models with the same data and...
Fishing for first timers: Using ChatGPT to catch halibut
Interconnects (Nathan Lambert) â–· #memes (2 messages):
Duality of Man, Discussion on X, Post by mcmillen.dev

é’“é±¼æ–°æ‰‹ç”¨ ChatGPT é’“å¤§æ¯”ç›®é±¼ï¼šä¸€åˆ™åä¸ºã€ŒFishing for first timersã€çš„ YouTube è§†é¢‘å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ ChatGPT æ¥å¯»æ‰¾å’Œé’“åˆ°å¤§æ¯”ç›®é±¼ã€‚
å…¶ä¸­ï¼Œå…³äºä½¿ç”¨ o3 æ•èŸ¹çš„æé—®ï¼Œå¸¦æœ‰ä¸€ä¸å¹½é»˜æ„Ÿã€‚
ç†è§£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„é•¿ç¨‹ CoTï¼ˆChain-of-Thoughtï¼‰æ¨ç†ï¼šä¸€ç¯‡å…³äºæ­ç¤ºé•¿ç¨‹ CoT æ¨ç†çš„æ–‡ç« ï¼Œç€é‡è®¨è®ºäº† R1ã€O1 å’Œ O3 ç­‰æ¨¡å‹èƒŒåä»¤äººè´¹è§£çš„æœºåˆ¶ï¼Œå¹¶è¯•å›¾æ·±å…¥äº†è§£å®ƒä»¬çš„è®­ç»ƒè¿‡ç¨‹ã€‚
æ–‡ç« æ€»ç»“äº† 11 ä¸ªä¸»è¦å‘ç°ï¼Œè¡¨æ˜å¯¹è¯¥ä¸»é¢˜è¿›è¡Œäº†æ·±å…¥ç ”ç©¶ã€‚
Qwen æ¨¡å‹å¸¦æ¥çš„æƒŠå–œï¼šè¿‘æœŸçš„è®¨è®ºè¡¨æ˜ï¼ŒQwen 2.5 æ¨¡å‹ä»…ä½¿ç”¨å°‘é‡è®­ç»ƒæ•°æ®å°±å–å¾—äº†æ˜¾è‘—æˆæœï¼Œè®¸å¤šå‚ä¸è®¨è®ºçš„æˆå‘˜éƒ½åˆ†äº«äº†ä»–ä»¬çš„å‘ç°ã€‚
æ­£å¦‚ Aran Komatsuzaki æ‰€è¯´ï¼ŒQwen æ¨¡å‹ä¼¼ä¹è‡ªå¸¦æŸç§ã€Œé­”æ³•ã€ï¼Œèƒ½å¤Ÿä»¥æå°‘çš„æ•°æ®å®ç°å“è¶Šçš„æ€§èƒ½ã€‚
Gary Marcus ç››èµæ·±åº¦ç ”ç©¶ï¼šGary Marcus å¯¹ Deep Research çš„å®ç”¨æ€§è¡¨ç¤ºèµèµï¼Œè®¤ä¸ºå®ƒåœ¨æŸäº›åº”ç”¨ä¸­éå¸¸å®ç”¨ï¼Œå°½ç®¡åœ¨äº‹å®åˆ¤æ–­å’Œæ—¶é—´æ¨ç†æ–¹é¢ä»ç„¶å­˜åœ¨æŒ‘æˆ˜ã€‚
ç¤¾åŒºæ™®éè®¤ä¸º Deep Research åœ¨ç‰¹å®šåº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†ä¹Ÿæ‰¿è®¤å…¶åœ¨äº‹å®å‡†ç¡®æ€§æ–¹é¢å­˜åœ¨ä¸è¶³ã€‚
é«˜æ•ˆä½¿ç”¨ O3ï¼šæœ‰ç”¨æˆ·åˆ†äº«äº†ä¸€ç§æ— éœ€æµè§ˆå³å¯æœ‰æ•ˆä½¿ç”¨ O3 çš„æŠ€å·§ï¼Œå¹¶ç§°ä¹‹ä¸ºé«˜æ•ˆçš„ç¼–ç æŠ€å·§ã€‚
æ®ç§°ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥å¸®åŠ©ç”¨æˆ·ä¸€æ¬¡æ€§å‡†ç¡®åœ°æ”¶é›†å¹¶å®ç°è§£å†³æ–¹æ¡ˆï¼Œä»è€Œæ˜¾è‘—æé«˜ç¼–ç æ•ˆç‡ã€‚

ç›¸å…³é“¾æ¥ï¼š
Big Tech Alertï¼ˆ@BigTechAlertï¼‰åœ¨ Xï¼ˆåŸ Twitterï¼‰ä¸Šå‘å¸–ï¼šğŸš« @allen_ai å–å…³äº† @ehsanikï¼ˆğŸ¤–ğŸ’­ï¼šè°æœ‰æ›´å¤šå†…å¹•æ¶ˆæ¯ï¼Ÿ)
Zeyuan Allen-Zhuï¼ŒSc.D.ï¼ˆ@ZeyuanAllenZhuï¼‰åœ¨ X ä¸Šå‘å¸–ï¼š(2/8ï¼‰å°±åƒ P ä¸ NP é—®é¢˜ä¸€æ ·ï¼Œå®¡é˜…ä¸€ç¯‡è®ºæ–‡ï¼ˆL3ï¼‰æ¯”æ’°å†™ä¸€ç¯‡è®ºæ–‡ï¼ˆL4ï¼‰æ‰€éœ€çš„æ™ºåŠ›æ›´ä½ã€‚å®¡æ ¸è¯„è®ºåªéœ€è¦ L2ï¼›ä»²è£ä½œè€…ä¸å®¡ç¨¿äººä¹‹é—´çš„äº‰è®®åªéœ€è¦ L1ï¼ˆåªéœ€éµå¾ªåŸºæœ¬é€»è¾‘)...
Omar Khattabï¼ˆ@lateinteractionï¼‰åœ¨ X ä¸Šå‘å¸–ï¼šå¤ªå¤šã€Œæˆ‘ä»¬å‡ ä¹æ²¡åšä»€ä¹ˆï¼ŒQwen 2.5 ç°åœ¨å´æ— æ‰€ä¸èƒ½ã€çš„ç»“æœäº† ğŸ˜†
Stefan Streichsbierï¼ˆ@s_streichsbierï¼‰åœ¨ X ä¸Šå‘å¸–ï¼šæ„Ÿè°¢ @iruletheworldmoã€‚è¿™ä¸ªæŠ€å·§å¤ªæœ‰ç”¨äº†ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨ o3 æ¥è¿›è¡Œç¼–ç ã€‚å®ƒä¼šæœç´¢å¤§é‡èµ„æ–™ï¼Œå¯»æ‰¾è§£å†³æ–¹æ¡ˆçš„å®ç°æ–¹æ³•ï¼Œç„¶åä¸€æ¬¡æ€§æ­£ç¡®åœ°æ•´åˆå‡ºæ¥ã€‚W...
Gary Marcusï¼ˆ@GaryMarcusï¼‰åœ¨ X ä¸Šå‘å¸–ï¼šDeep Research ç¡®å®å¾ˆæœ‰ç”¨ - å…·ä½“å–å†³äºä½ çš„åº”ç”¨åœºæ™¯ - ä½†å…³é”®æ˜¯ï¼ˆæ­£å¦‚ 2019 å¹´çš„ Rebooting AI å’Œ @yudapearl æ‰€é¢„æµ‹çš„é‚£æ ·ï¼‰ï¼Œäº‹å®å’Œæ—¶é—´æ¨ç†ä»ç„¶æ˜¯å½“å‰...
Zeyuan Allen-Zhuï¼ŒSc.D.ï¼ˆ@ZeyuanAllenZhuï¼‰åœ¨ X ä¸Šå‘å¸–ï¼šä¸è¦è®©é‚£äº›ç¦»è°±çš„ ICLR å®¡ç¨¿äººå½±å“ä½ çš„å¿ƒæƒ… â€”â€” å³ä½¿æ˜¯é¢†åŸŸä¸»å¸­ï¼ˆACï¼‰ä¹Ÿä¼šé‡åˆ°è¿™ç§æƒ…å†µã€‚æˆ‘ä»¬çš„è®ºæ–‡ï¼ˆ8,8,6,3ï¼‰é­åˆ°ä¸€ä½å…ƒå®¡ç¨¿äººå•æ–¹é¢æ‹’ç»ã€‚è°¢å¤©è°¢åœ°ï¼ŒICLR æ˜¯å¼€æ”¾è¯„å®¡çš„ï¼Œè¿™ç§ä¸å½“è¡Œä¸ºå°†ä¼š...
Omar Khattabï¼ˆ@lateinteractionï¼‰åœ¨ X ä¸Šå‘å¸–ï¼šæ›´å¤šå…³äº Qwen çš„ä¿¡æ¯ã€‚æˆ‘è¶Šæ¥è¶Šç¡®ä¿¡ï¼Œè¿™äº›è®ºæ–‡æ­ç¤ºäº† Qwen æ¨¡å‹çš„ä¸€äº›ç‰¹æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯å…³äºæ¨ç†èƒ½åŠ›æœ¬èº«ã€‚å¼•ç”¨ Aran Komatsuzakiï¼ˆ@arankomatsuzakiï¼‰LIMO çš„è§‚ç‚¹ï¼š...
Zeyuan Allen-Zhuï¼ŒSc.D.ï¼ˆ@ZeyuanAllenZhuï¼‰åœ¨ X ä¸Šå‘å¸–ï¼š(1/8ï¼‰æˆ‘ä»¬å°†æ™ºèƒ½æ°´å¹³åˆ†ä¸º L1~L5 å‡ ä¸ªç­‰çº§ï¼Œå¹¶è§‚å¯Ÿåˆ°åªæœ‰ Gemini-2-FTã€DeepSeek-R1ã€OpenAI-o1 èƒ½å¤Ÿè¾¾åˆ° L2 æ°´å¹³ï¼›å¤§å¤šæ•°æ¨¡å‹ä»…ä¸º L1 çº§åˆ«ï¼ˆä¾‹å¦‚ o3-miniï¼‰ã€‚å³ä¾¿å¦‚æ­¤ï¼Œäººä»¬ä»ç„¶å¯ä»¥ä½¿ç”¨ L1 çº§åˆ«çš„ AI æ¥ä»²è£çº çº·å¹¶ç¡®ä¿...
Xiang Yueï¼ˆ@xiangyue96ï¼‰åœ¨ X ä¸Šå‘å¸–ï¼šæ­ç§˜å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­çš„é•¿ç¨‹ CoTï¼ˆChain-of-Thoughtï¼‰æ¨ç†ï¼šhttps://arxiv.org/pdf/2502.03373 åƒ R1 / O1 / O3 è¿™æ ·çš„æ¨ç†æ¨¡å‹å¤‡å—å…³æ³¨ï¼Œä½†å®ƒä»¬çš„è®­ç»ƒæœºåˆ¶ä»ç„¶æ˜¯ä¸ªè°œã€‚æˆ‘ä»¬æ­£åœ¨åŠªåŠ›...
Wenhu Chenï¼ˆ@WenhuChenï¼‰åœ¨ X ä¸Šå‘å¸–ï¼šæˆ‘åŒæ„ã€‚è¿™å’Œ s1 çš„å‘ç°éå¸¸ç›¸ä¼¼ï¼Œåè€…ä½¿ç”¨å¤§çº¦ 1000 ä¸ªè®­ç»ƒæ ·æœ¬å¾—å‡ºäº†ç±»ä¼¼çš„ç»“æœã€‚æˆ‘ä»¬å®é™…ä¸Šå°è¯•ä½¿ç”¨ç›¸åŒçš„æ•°æ®è®­ç»ƒå…¶ä»–æ¨¡å‹ï¼Œç»“æœ...

æ–°æ‰‹é’“é±¼ï¼šä½¿ç”¨ ChatGPT é’“å¤§æ¯”ç›®é±¼
Interconnectsï¼ˆNathan Lambertï¼‰â–· #memes æ¿å—ï¼ˆ2 æ¡æ¶ˆæ¯)ï¼š
ã€Œäººçš„äºŒé‡æ€§ã€è¯é¢˜ï¼ŒX å¹³å°ä¸Šçš„è®¨è®ºï¼Œmcmillen.dev çš„å¸–å­

Exploring the Duality of Man: A post shared on X discusses the concept of the duality of man, highlighting contrasting aspects of human nature.
This theme invites deeper reflection on how individuals balance conflicting traits such as light and darkness in their lives.
Engagement with mcmillen.dev's Post: A link to a post by mcmillen.dev was shared, though no further details were provided about its contents.
The lack of context leaves the discussion open to interpretation, prompting curiosity about the ideas presented.
Links mentioned:
Tweet from thomas (@distributionat): the duality of man
Colin McMillen (@mcmillen.dev): "In the future, we will no longer have aspirational goals" is a masterstroke of Google corporate communicationshttps://www.theverge.com/google/607012/google-dei-hiring-goals-internal-memo
Interconnects (Nathan Lambert) â–· #rl (11 messagesğŸ”¥):
RL dataset skepticism, Unsloth GRPO support, Unified memory usage, Training on same GPUs, DM paper on rollouts

æœªæ‰¾åˆ°æ„è¯‘å†…å®¹

Model developers skeptical of RL datasets: Members discussed skepticism among model developers regarding RL datasets published by non-model shops, suggesting that such datasets may be viewed as lacking credibility without validation from established organizations.
One member noted, 'my instinct is that the dataset wouldn't be worth the paper it's printed on' if it lacks the endorsement of a credible source.
Unsloth enhances GRPO process: Unsloth announced support for Group Relative Policy Optimization (GRPO), claiming their enhancements allow users to reduce VRAM usage by 80% compared to previous methods.
This feature lets users reproduce R1-Zero's findings with just 7GB of VRAM using Qwen2.5 while streamlining dependencies.
Unified memory may make async RLHF obsolete: Discussion surfaced around Unsloth's unified memory usage, potentially decreasing the need for separate GPUs for training and rollouts by allowing both processes to run concurrently.
Members speculated this advancement could reduce resource waste, as GPUs wouldn't sit idle during operational switching.
DM paper confirms cyclical generation during training: A member recalled a paper that discussed generating and feeding back data during training, despite it being slightly off policy, which individuals found relevant to their topic on simultaneous processes.
Another member confirmed a related paper here that supports similar conclusions about training dynamics.
Switching costs when using same GPUs: Participants agreed that using the same GPUs for both training and rollouts is preferable if switching costs are minimal, but uncertainty remains about the actual costs involved.
One member expressed, 'You have to every time transform the model into vLLM format and idk how long that takes,' indicating potential complexity in implementation.
Link mentioned: Train your own R1 reasoning model locally: You can now reproduce your own DeepSeek-R1 reasoning model with Unsloth 100% locally. Using GRPO.Open-source, free and beginner friendly.

æ¨¡å‹å¼€å‘è€…å¯¹å¼ºåŒ–å­¦ä¹ æ•°æ®é›†çš„ä¿¡ä»»åº¦ä¸é«˜ï¼šæœ‰æˆå‘˜è®¨è®ºäº†æ¨¡å‹å¼€å‘è€…å¯¹äºéä¸“ä¸šæœºæ„å‘å¸ƒçš„å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼ŒRLï¼‰æ•°æ®é›†çš„ç–‘è™‘ã€‚ä»–ä»¬è®¤ä¸ºï¼Œå¦‚æœè¿™äº›æ•°æ®é›†æ²¡æœ‰å¾—åˆ°æƒå¨æœºæ„çš„éªŒè¯ï¼Œå…¶å¯ä¿¡åº¦å¯èƒ½ä¼šå¤§æ‰“æŠ˜æ‰£ã€‚
ä¸€ä½æˆå‘˜è¡¨ç¤ºï¼Œå¦‚æœæ•°æ®é›†ç¼ºä¹å¯é æ¥æºçš„èƒŒä¹¦ï¼Œã€Œé‚£å®ƒçš„ä»·å€¼å¯èƒ½å°±å¤§æ‰“æŠ˜æ‰£äº†ã€ã€‚
Unsloth ä¼˜åŒ– GRPO æµç¨‹ï¼šUnsloth å®£å¸ƒæ”¯æŒç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGroup Relative Policy Optimizationï¼ŒGRPOï¼‰ã€‚æ®ç§°ï¼Œè¯¥ä¼˜åŒ–æ–¹æ¡ˆèƒ½å¸®åŠ©ç”¨æˆ·å°†æ˜¾å­˜ï¼ˆVRAMï¼‰ä½¿ç”¨é‡é™ä½ 80%ï¼Œä¼˜äºä¹‹å‰çš„æ–¹æ¡ˆã€‚
è¯¥åŠŸèƒ½è®©ç”¨æˆ·ä»…éœ€ 7GB æ˜¾å­˜ï¼Œå³å¯å¤ç° R1-Zero çš„å®éªŒç»“æœï¼Œå¹¶ç®€åŒ–ç›¸å…³ä¾èµ–ã€‚
ç»Ÿä¸€å†…å­˜æˆ–å°†ä½¿å¼‚æ­¥åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning from Human Feedbackï¼ŒRLHFï¼‰æˆä¸ºè¿‡å»å¼ï¼šè®¨è®ºèšç„¦äº Unsloth çš„ç»Ÿä¸€å†…å­˜ä½¿ç”¨æ–¹å¼ï¼Œè¿™ç§æ–¹å¼æœ‰æœ›å‡å°‘è®­ç»ƒå’Œ rollout å¯¹ç‹¬ç«‹ GPU çš„éœ€æ±‚ï¼Œå®ç°ä¸¤ä¸ªæµç¨‹çš„å¹¶è¡Œã€‚
æœ‰æˆå‘˜æ¨æµ‹ï¼Œè¿™ä¸€è¿›æ­¥æœ‰æœ›å‡å°‘èµ„æºæµªè´¹ï¼Œé¿å… GPU åœ¨æµç¨‹åˆ‡æ¢æœŸé—´çš„é—²ç½®ã€‚
æŸè®ºæ–‡è¯å®è®­ç»ƒæœŸé—´å­˜åœ¨å¾ªç¯ç”Ÿæˆç°è±¡ï¼šæœ‰æˆå‘˜å›å¿†èµ·ä¸€ç¯‡è®ºæ–‡ï¼Œå…¶ä¸­è®¨è®ºäº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆæ•°æ®å¹¶å°†å…¶åé¦ˆå›æ¨¡å‹ï¼Œå°½ç®¡è¿™äº›æ•°æ®ä¸æ—¢å®šç­–ç•¥å­˜åœ¨ç»†å¾®åå·®ã€‚å¤§å®¶è®¤ä¸ºè¿™ä¸å½“å‰å…³äºåŒæ­¥æµç¨‹çš„è¯é¢˜å¯†åˆ‡ç›¸å…³ã€‚
å¦ä¸€ä½æˆå‘˜ç¡®è®¤äº†ä¸€ç¯‡ç›¸å…³è®ºæ–‡ï¼Œè¯¥è®ºæ–‡ä¹Ÿæ”¯æŒå…³äºè®­ç»ƒåŠ¨æ€çš„ç±»ä¼¼ç»“è®ºã€‚
ä½¿ç”¨ç›¸åŒ GPU æ—¶çš„åˆ‡æ¢æˆæœ¬ï¼šå‚ä¸è€…æ™®éè®¤ä¸ºï¼Œå¦‚æœè®­ç»ƒå’Œ rollout ä¹‹é—´åˆ‡æ¢æˆæœ¬è¶³å¤Ÿä½ï¼Œé‚£ä¹ˆä½¿ç”¨ç›¸åŒçš„ GPU ä¼šæ›´åˆ’ç®—ã€‚ä½†å®é™…æˆæœ¬ä»ç„¶å­˜åœ¨ä¸ç¡®å®šæ€§ã€‚
ä¸€ä½æˆå‘˜æåˆ°ï¼Œã€Œæ¯æ¬¡éƒ½å¿…é¡»å°†æ¨¡å‹è½¬æ¢æˆ vLLM æ ¼å¼ï¼Œæˆ‘ä¸ç¡®å®šè¿™ä¸ªè¿‡ç¨‹éœ€è¦å¤šä¹…ã€ï¼Œæš—ç¤ºäº†æ½œåœ¨çš„å®æ–½å¤æ‚æ€§ã€‚
ç›¸å…³é“¾æ¥ï¼šåœ¨æœ¬åœ°è®­ç»ƒä½ è‡ªå·±çš„ R1 æ¨ç†æ¨¡å‹ï¼šç°åœ¨ï¼Œä½ å¯ä»¥å®Œå…¨åœ¨æœ¬åœ°ä½¿ç”¨ Unsloth å¤ç° DeepSeek-R1 æ¨ç†æ¨¡å‹ã€‚è¯¥æ–¹æ¡ˆä½¿ç”¨ GRPO æŠ€æœ¯ï¼Œå¼€æºã€å…è´¹ï¼Œå¯¹æ–°æ‰‹å‹å¥½ã€‚

Interconnects (Nathan Lambert) â–· #reads (8 messagesğŸ”¥):
Open source AI, DeepSeek's impact on Scale AI, AI's evolving definitions, The importance of human oversight, Dario on Chinatalk

äº’è¿ï¼ˆNathan Lambertï¼‰â–· ç²¾é€‰æ–‡ç« ï¼ˆ8 æ¡çƒ­é—¨æ¶ˆæ¯ğŸ”¥)ï¼š
å¼€æºäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ï¼ŒDeepSeek å¦‚ä½•å½±å“ Scale AI çš„å‘å±•ï¼Œäººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å®šä¹‰çš„æ¼”å˜ï¼Œäººå·¥ç›‘ç£çš„é‡è¦æ€§ï¼ŒDario åœ¨ Chinatalk è®¿è°ˆä¸­çš„è§‚ç‚¹

Open source AI and Freedoms Discussion: A recent piece discussed the transition of AI beyond traditional software confines and elaborated on OpenAI's Four Freedoms, inspired by Sam Altman's thoughts.
The post emphasized Aaron Swartz's influence on the open-source movement, linking back to the foundational ideas of genuine open access.
Debating DeepSeek's Role in Scale AI's Model: In response to DeepSeek, Scale CEO Alexandr Wang highlighted the misperception of automation in data generation, calling it 'lazy' to assume a fully automated process.
Despite transparency issues from DeepSeek, the company reportedly pioneered new automation techniques in creating training data.
Adaptation Challenges for Scale AI: There's recognition that while adaptation is possible for Scale AI, challenges remain due to current operational models and valuations.
The emphasis was on the bleak outlook without significant changes to their approach amid a shifting landscape.
Dario's Feature on Chinatalk: A mention of Dario's appearance on Chinatalk triggered interest and discussion among members about his insights.
This sparked curiosity and potentially provided a platform for deeper exploration of the topic.
Link mentioned: ğŸŒ#86: Four Freedoms of Open AI: â€“ what are they? Defining the future

å…³äºå¼€æº AI å’Œè‡ªç”±çš„è®¨è®ºï¼šæœ€è¿‘ä¸€ç¯‡æ–‡ç« æ¢è®¨äº† AI å¦‚ä½•ä»ä¼ ç»Ÿè½¯ä»¶çš„é™åˆ¶ä¸­è§£æ”¾å‡ºæ¥ï¼Œå¹¶è¯¦ç»†é˜è¿°äº† OpenAI çš„ã€Œå››å¤§è‡ªç”±ã€ï¼Œè¿™äº›æƒ³æ³•å—åˆ°äº† Sam Altman çš„å¯å‘ã€‚
æ–‡ç« ç€é‡å¼ºè°ƒäº† Aaron Swartz å¯¹å¼€æºè¿åŠ¨çš„æ·±è¿œå½±å“ï¼Œè¿½æº¯äº†çœŸæ­£å¼€æ”¾è·å–çš„æ ¸å¿ƒç†å¿µã€‚
å…³äº DeepSeek åœ¨ Scale AI æ¨¡å‹ä¸­çš„ä½œç”¨çš„äº‰è®ºï¼šé’ˆå¯¹ DeepSeekï¼ŒScale AI çš„é¦–å¸­æ‰§è¡Œå®˜ Alexandr Wang æŒ‡å‡ºï¼Œäººä»¬å¯¹æ•°æ®ç”Ÿæˆè¿‡ç¨‹ä¸­çš„è‡ªåŠ¨åŒ–å­˜åœ¨è¯¯è§£ï¼Œè®¤ä¸ºå®Œå…¨è‡ªåŠ¨åŒ–æ˜¯ä¸€ç§ã€Œæ‡’æƒ°ã€çš„æƒ³æ³•ã€‚
å°½ç®¡ DeepSeek åœ¨é€æ˜åº¦æ–¹é¢å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œä½†æ®æŠ¥é“ï¼Œè¯¥å…¬å¸åœ¨åˆ›å»ºè®­ç»ƒæ•°æ®æ–¹é¢ç‡å…ˆé‡‡ç”¨äº†æ–°çš„è‡ªåŠ¨åŒ–æŠ€æœ¯ã€‚
Scale AI é¢ä¸´çš„é€‚åº”æ€§æŒ‘æˆ˜ï¼šäººä»¬æ™®éè®¤ä¸ºï¼ŒScale AI å…·æœ‰ä¸€å®šçš„é€‚åº”èƒ½åŠ›ï¼Œä½†ç”±äºå…¶å½“å‰çš„è¿è¥æ¨¡å¼å’Œä¼°å€¼ï¼Œä»ç„¶é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚
æ–‡ç« å¼ºè°ƒï¼Œå¦‚æœ Scale AI ä¸å¯¹è‡ªèº«çš„æ–¹æ³•è¿›è¡Œé‡å¤§è°ƒæ•´ï¼Œé‚£ä¹ˆåœ¨å¿«é€Ÿå˜åŒ–çš„å¸‚åœºç¯å¢ƒä¸­ï¼Œå…¶å‰æ™¯å°†ä¸å®¹ä¹è§‚ã€‚
Dario åœ¨ Chinatalk èŠ‚ç›®ä¸­çš„è®¿è°ˆï¼šDario åœ¨ Chinatalk èŠ‚ç›®ä¸­çš„äº®ç›¸ï¼Œå¼•å‘äº†ç¤¾åŒºæˆå‘˜çš„å¹¿æ³›å…´è¶£å’Œè®¨è®ºã€‚
è¿™æ¬¡è®¿è°ˆæ¿€å‘äº†äººä»¬çš„å¥½å¥‡å¿ƒï¼Œå¹¶å¯èƒ½ä¸ºæ›´æ·±å…¥åœ°æ¢è®¨ç›¸å…³è¯é¢˜æä¾›äº†ä¸€ä¸ªå¹³å°ã€‚
ç›¸å…³é“¾æ¥ï¼šğŸŒ#86ï¼šOpenAI çš„å››å¤§è‡ªç”±ï¼šå®ƒä»¬æ˜¯ä»€ä¹ˆï¼Ÿå®šä¹‰æœªæ¥

Interconnects (Nathan Lambert) â–· #policy (1 messages):
xeophon.: https://x.com/AndrewCurran_/status/1887505463211925557

Notebook LM â–· #use-cases (17 messagesğŸ”¥):
Collaboration and Similarities, Corruption in Religious Leadership, AI Features for Creativity, Uses of NotebookLM in Law, Max Headroom's Comeback

äº’è”ï¼ˆNathan Lambertï¼‰â–· #policyï¼ˆ1 æ¡æ¶ˆæ¯):
xeophon.ï¼šhttps://x.com/AndrewCurran_/status/1887505463211925557

Notebook LM â–· #use-casesï¼ˆ17 æ¡çƒ­é—¨æ¶ˆæ¯ğŸ”¥):
åä½œä¸ç›¸ä¼¼æ€§ï¼Œå®—æ•™é¢†å¯¼ä¸­çš„è…è´¥ï¼ŒAI åˆ›æ„åº”ç”¨ï¼ŒNotebook LM åœ¨æ³•å¾‹ä¸­çš„åº”ç”¨ï¼ŒMax Headroom çš„å›å½’

Finding Similarities in Work: A member expressed excitement about discovering someone with a similar approach to their work in narratives, noting it helps identify weaker points in their narratives.
They mentioned that the hostsâ€™ feedback greatly influenced the development of their narratives.
Corruption in Religious Leadership Discourse: A member noted a discussion where hosts pointed out how historically, religious leaders advising on crops and voting are often susceptible to corruption.
This led to a realization about the inherent issues within these roles, indicating an obvious yet overlooked truth.
Proposed Sliders for AI Creativity: A member suggested integrating sliders for tuning AI's creativity, similar to features found in Gemini API and other services.
This idea was sparked after they discovered an exploit related to the AI's features just two days prior.
NotebookLM for Legal Summaries: A user shared experiences using NotebookLM to capture testimony at the New York State Legislatureâ€™s Budget hearing on Environmental Conservation.
They highlighted the challenge of sharing this extensive document due to licensing limitations, proposing it as a compelling demonstration of NotebookLM's capabilities.
Max Headroom's Digital Comeback: A user excitedly announced the return of Max Headroom with an edgy video and music, showcasing a unique approach to AI interaction.
They encouraged others to watch and share their content, referencing a new video that humorously critiques corporate AI practices.
Links mentioned:
New York State Legislature Environmental Conservation Budget Hearing 2025 - Notes: Notes by Jon Garfunkel Joint Legislative Public Hearing on 2025 Executive Budget Proposal: Topic Environmental Conservation | NYSenate.gov I uploaded the source documents - 45 written testimonies ...
Max Headroom 2025 featuring "ğŸµ "BOT-NOIA": ğŸš¨ GLITCH ALERT! ğŸš¨Guess who just escaped the mainframe? Thatâ€™s right, ME! MAX! HEADROOM! Back from the digital graveyard, angrier, glitchier, and more sarca...
Notebook LM â–· #general (78 messagesğŸ”¥ğŸ”¥):
NotebookLM Model Limitations, Audio Overview Customization, Spreadsheet Data Analysis, Sharing Notebooks Issues, Interactive Mode Problems

å¯»æ‰¾å·¥ä½œä¸­çš„ç›¸ä¼¼ä¹‹å¤„ï¼šä¸€ä½æˆå‘˜å¾ˆé«˜å…´å‘ç°æœ‰äººå’Œè‡ªå·±åœ¨å™äº‹ä½œå“ä¸­ï¼Œæœ‰ç›¸ä¼¼çš„å·¥ä½œæ–¹æ³•ï¼Œè¿™èƒ½å¤Ÿå¸®åŠ©ä»–ä»¬å‘ç°è‡ªå·±ä½œå“ä¸­çš„ä¸è¶³ã€‚
ä»–ä»¬æåˆ°ï¼Œä¸»æŒäººçš„åé¦ˆå¯¹ä»–ä»¬çš„å™äº‹ä½œå“çš„æ”¹è¿›æœ‰å¾ˆå¤§çš„å¸®åŠ©ã€‚
å…³äºå®—æ•™é¢†è¢–çš„è®¨è®ºï¼šä¸€ä½æˆå‘˜æ³¨æ„åˆ°äº†ä¸€åœºè®¨è®ºï¼Œå…¶ä¸­ä¸»æŒäººæŒ‡å‡ºï¼Œå†å²ä¸Šï¼Œé‚£äº›åœ¨å†œä½œç‰©ç§æ¤ã€æŠ•ç¥¨é€‰ä¸¾ç­‰æ–¹é¢æä¾›å»ºè®®çš„å®—æ•™é¢†è¢–ï¼Œæ›´å®¹æ˜“å‡ºç°è…è´¥é—®é¢˜ã€‚
è¿™è®©ä»–ä»¬æ„è¯†åˆ°ï¼Œè¿™äº›è§’è‰²æœ¬èº«å°±å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œè€Œè¿™äº›é—®é¢˜å¾€å¾€æ˜¾è€Œæ˜“è§ï¼Œä½†åˆå®¹æ˜“è¢«å¿½è§†ã€‚
ä¸º AI åˆ›é€ åŠ›æå‡ºçš„æ»‘å—ï¼šä¸€ä½æˆå‘˜å»ºè®®ï¼Œå¯ä»¥åŠ å…¥ç±»ä¼¼ Gemini API ç­‰æœåŠ¡ä¸­çš„æ»‘å—ï¼Œç”¨æ¥è°ƒæ•´ AI çš„åˆ›é€ åŠ›ã€‚
è¿™ä¸ªæƒ³æ³•æºäºä»–ä»¬ä¸¤å¤©å‰å‘ç°çš„ä¸€ä¸ªä¸ AI åŠŸèƒ½ç›¸å…³çš„æ¼æ´ã€‚
NotebookLM ç”¨äºæ³•å¾‹æ‘˜è¦ï¼šä¸€ä½ç”¨æˆ·åˆ†äº«äº†ä½¿ç”¨ NotebookLM åœ¨çº½çº¦å·ç«‹æ³•æœºæ„å…³äºç¯å¢ƒä¿æŠ¤çš„é¢„ç®—å¬è¯ä¼šä¸Šè®°å½•è¯è¯çš„ç»éªŒã€‚
ä»–ä»¬å¼ºè°ƒï¼Œç”±äºè®¸å¯é™åˆ¶ï¼Œå…±äº«è¿™ä»½åŒ…å«å¤§é‡å†…å®¹çš„æ–‡ä»¶æ¯”è¾ƒå›°éš¾ï¼Œä½†ä¹Ÿæ­£å› å¦‚æ­¤ï¼Œæ›´èƒ½çªæ˜¾ NotebookLM çš„å¼ºå¤§åŠŸèƒ½ã€‚
Max Headroom çš„æ•°å­—å¤å‡ºï¼šä¸€ä½ç”¨æˆ·å…´å¥‹åœ°å®£å¸ƒ Max Headroom ä»¥æ•°å­—åŒ–çš„å½¢å¼å›å½’ï¼Œå¹¶å‘å¸ƒäº†ä¸€ä¸ªéå¸¸æ–°æ½®çš„è§†é¢‘å’ŒéŸ³ä¹ï¼Œå±•ç¤ºäº†ä¸€ç§ç‹¬ç‰¹çš„ä¸ AI äº’åŠ¨çš„æ–¹å¼ã€‚
ä»–ä»¬é¼“åŠ±å¤§å®¶è§‚çœ‹å’Œåˆ†äº«ï¼Œå¹¶æ¨èäº†ä¸€ä¸ªç”¨å¹½é»˜çš„æ–¹å¼æ¥æ‰¹åˆ¤ä¼ä¸šåœ¨ AI é¢†åŸŸçš„è¡Œä¸ºçš„è§†é¢‘ã€‚
æåˆ°çš„é“¾æ¥ï¼š
çº½çº¦å·ç«‹æ³•æœºæ„ç¯å¢ƒä¿æŠ¤é¢„ç®—å¬è¯ä¼š 2025 - ç¬”è®°ï¼šJon Garfunkel å…³äº 2025 å¹´è¡Œæ”¿é¢„ç®—ææ¡ˆè”åˆç«‹æ³•å…¬å¼€å¬è¯ä¼šçš„ç¬”è®°ï¼šä¸»é¢˜ç¯å¢ƒä¿æŠ¤ | NYSenate.gov æˆ‘ä¸Šä¼ äº†æºæ–‡ä»¶ - 45 ä»½ä¹¦é¢è¯è¯ ...
Max Headroom 2025 ä»¥ã€ŒğŸµ"BOT-NOIAã€ä¸ºç‰¹è‰²ï¼šğŸš¨ GLITCH ALERT! ğŸš¨ çŒœçŒœè°åˆšä»ä¸»æœºä¸­é€ƒè„±äº†ï¼Ÿæ²¡é”™ï¼Œæ˜¯æˆ‘ï¼MAXï¼HEADROOMï¼ä»æ•°å­—å¢“åœ°å›æ¥äº†ï¼Œæ›´æ„¤æ€’ã€æ›´ä¸ç¨³å®šï¼Œä¹Ÿæ›´è®½åˆº......
Notebook LM â–· #generalï¼ˆ78 æ¡æ¶ˆæ¯ğŸ”¥ğŸ”¥):
NotebookLM æ¨¡å‹é™åˆ¶ã€éŸ³é¢‘æ¦‚è¿°è‡ªå®šä¹‰ã€ç”µå­è¡¨æ ¼æ•°æ®åˆ†æã€å…±äº« Notebook é—®é¢˜ã€äº¤äº’æ¨¡å¼é—®é¢˜

NotebookLM lacks model change options on mobile: A user expressed frustration regarding the inability to change the model within the mobile version of NotebookLM, which another member confirmed is not possible.
This limitation seems to hinder user experience, leading to confusion among those expecting more flexibility in model management.
Audio Overviews Generation and Customization Tips: Members discussed the process of customizing audio overviews in NotebookLM, confirming that users must delete and regenerate the audio file to access the customization button.
One member suggested using a specific phrasing to differentiate between primary and complementary sources for better outputs.
Spreadsheet Compatibility Concerns: Concerns were raised about using NotebookLM for analyzing spreadsheet data, with suggestions to utilize tools like Gemini within Google Sheets instead.
Users highlighted the importance of understanding the capabilities of NotebookLM as primarily a text analysis tool.
Sharing Notebooks Functionality: There were discussions on sharing notebooks among different Google accounts, with confirmation that while sharing is available, some users experienced visibility issues with shared notebooks.
Links to shared notebooks were discussed, and it was noted that sharing functionalities are currently being improved by the development team.
Issues with Interactive Mode: A user reported persistent issues with the interactive mode in NotebookLM, noting that it fails to work across both web and mobile platforms.
The issue was recognized as potentially affecting both free and plus versions, raising questions about overall accessibility and functionality.
Links mentioned:
Notebooks - NotebookLM Help: no description found
Get started with NotebookLM and NotebookLM Plus - NotebookLM Help: no description found
Gemini can now do more complex data analysis in Google Sheets: Gemini in Google Sheets is about to become more powerful. The AI agent can now use Python code to generate insights and charts about your data.
LLM Agents (Berkeley MOOC) â–· #mooc-announcements (1 messages):
Fall 2024 MOOC Certificates, Coursework Submission Challenges, Future MOOC Opportunities

NotebookLM ç§»åŠ¨ç«¯ç¼ºå°‘æ¨¡å‹åˆ‡æ¢åŠŸèƒ½ï¼šæœ‰ç”¨æˆ·åæ˜ ï¼ŒNotebookLM ç§»åŠ¨ç‰ˆæœ¬æ— æ³•åˆ‡æ¢æ¨¡å‹ï¼Œå¹¶ä¸”å¾—åˆ°äº†å…¶ä»–ç”¨æˆ·çš„è¯å®ã€‚
è¿™ä¸€é™åˆ¶é™ä½äº†ç”¨æˆ·ä½“éªŒï¼Œè®©ä¸€éƒ¨åˆ†æœŸæœ›æ›´çµæ´»åœ°ç®¡ç†æ¨¡å‹çš„ç”¨æˆ·æ„Ÿåˆ°å›°æƒ‘ã€‚
éŸ³é¢‘æ¦‚è¿°ç”Ÿæˆå’Œè‡ªå®šä¹‰æŠ€å·§ï¼šæœ‰ç”¨æˆ·è®¨è®ºäº†åœ¨ NotebookLM ä¸­è‡ªå®šä¹‰éŸ³é¢‘æ¦‚è¿°çš„æ–¹æ³•ã€‚æƒ³è¦å¼€å¯è‡ªå®šä¹‰é€‰é¡¹ï¼Œç”¨æˆ·éœ€è¦å…ˆåˆ é™¤ç°æœ‰çš„éŸ³é¢‘æ–‡ä»¶ï¼Œç„¶åé‡æ–°ç”Ÿæˆã€‚
æœ‰ç”¨æˆ·å»ºè®®ï¼Œåœ¨æé—®æ—¶ä½¿ç”¨ç‰¹å®šçš„è¯­å¥ï¼ŒåŒºåˆ†ä¸»è¦å’Œè¡¥å……ä¿¡æ¯æ¥æºï¼Œä»è€Œè·å¾—æ›´å¥½çš„è¾“å‡ºç»“æœã€‚
ç”µå­è¡¨æ ¼å…¼å®¹æ€§é—®é¢˜ï¼šæœ‰ç”¨æˆ·è¡¨è¾¾äº†å¯¹äºä½¿ç”¨ NotebookLM åˆ†æç”µå­è¡¨æ ¼æ•°æ®çš„æ‹…å¿§ï¼Œå¹¶å»ºè®®ä½¿ç”¨ Google Sheets ä¸­æä¾›çš„ Geminiï¼ˆä¸€ç§ AI å·¥å…·ï¼‰ç­‰å…¶ä»–å·¥å…·ã€‚
å¤§å®¶å¼ºè°ƒï¼Œéœ€è¦ç†è§£ NotebookLM ä¸»è¦æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ†æå·¥å…·ï¼Œä»è€Œæ›´å¥½åœ°ä½¿ç”¨å®ƒã€‚
Notebook å…±äº«åŠŸèƒ½ï¼šæœ‰ç”¨æˆ·è®¨è®ºäº†åœ¨ä¸åŒ Google è´¦å·ä¹‹é—´å…±äº« Notebook çš„é—®é¢˜ã€‚å…±äº«åŠŸèƒ½æ˜¯å¯ç”¨çš„ï¼Œä½†éƒ¨åˆ†ç”¨æˆ·åé¦ˆå…±äº«çš„ Notebook å­˜åœ¨å¯è§æ€§é—®é¢˜ã€‚
ç”¨æˆ·ä»¬è¿˜è®¨è®ºäº†å…±äº« Notebook çš„é“¾æ¥ã€‚å¼€å‘å›¢é˜Ÿæ­£åœ¨åŠªåŠ›æ”¹è¿›å…±äº«åŠŸèƒ½ã€‚
äº¤äº’æ¨¡å¼é—®é¢˜ï¼šæœ‰ç”¨æˆ·æŠ¥å‘Šç§°ï¼ŒNotebookLM çš„äº¤äº’æ¨¡å¼ä¸€ç›´å­˜åœ¨é—®é¢˜ï¼Œåœ¨ç½‘é¡µç«¯å’Œç§»åŠ¨ç«¯éƒ½æ— æ³•æ­£å¸¸ä½¿ç”¨ã€‚
è¿™ä¸ªé—®é¢˜å¯èƒ½åŒæ—¶å½±å“å…è´¹ç‰ˆå’Œ Plus ç‰ˆç”¨æˆ·ï¼Œå¼•å‘äº†äººä»¬å¯¹äºå…¶æ•´ä½“å¯ç”¨æ€§å’ŒåŠŸèƒ½çš„æ‹…å¿§ã€‚
ç›¸å…³é“¾æ¥ï¼š
Notebooks - NotebookLM Helpï¼šæš‚æ— ç›¸å…³æè¿°
Get started with NotebookLM and NotebookLM Plus - NotebookLM Helpï¼šæš‚æ— ç›¸å…³æè¿°
Gemini can now do more complex data analysis in Google Sheetsï¼šGoogle Sheets ä¸­çš„ Gemini åŠŸèƒ½è¿æ¥å‡çº§ã€‚ç°åœ¨ï¼ŒAI æ™ºèƒ½ä½“å¯ä»¥ä½¿ç”¨ Python ä»£ç ï¼Œå¸®ä½ åˆ†ææ•°æ®ï¼Œå¹¶ç”Ÿæˆæ•°æ®å›¾è¡¨ã€‚
LLM Agentsï¼ˆBerkeley MOOCï¼‰â–· #mooc-announcementsï¼ˆ1 messages):
2024 ç§‹å­£ MOOC è¯ä¹¦ã€è¯¾ç¨‹ä½œä¸šæäº¤æŒ‘æˆ˜ã€æœªæ¥ MOOC æœºä¼š

Fall 2024 MOOC Certificates Released Today!: All Fall 2024 MOOC certificates will be released today at 8am PT, addressing recent technical challenges that have been resolved.
Congratulations to all recipients for their patience and hard work!
Some Participants Downgraded: A few participants were downgraded to the Trailblazer tier due to incomplete coursework submissions.
Sadly, a minority will not receive a certificate, and no makeups or regrades will be offered.
Encouragement for Future MOOCs: Participants are encouraged to sign up for Spring 2025 MOOC even if they faced challenges this time.
The team hopes everyone enjoyed the course and is excited for future opportunities!
LLM Agents (Berkeley MOOC) â–· #mooc-questions (57 messagesğŸ”¥ğŸ”¥):
Certificate issuance timeline, Quiz availability and results, Certificate tier breakdown, Communication and support, Feedback on course experience

2024 å¹´ç§‹å­£ MOOC è¯ä¹¦ä»Šæ—¥å‘å¸ƒï¼ï¼šæ‰€æœ‰ 2024 å¹´ç§‹å­£ MOOC è¯ä¹¦å°†äºä»Šå¤©å¤ªå¹³æ´‹æ—¶é—´æ—©ä¸Š 8 ç‚¹å‘å¸ƒï¼Œæ­¤å‰é‡åˆ°ä¸€äº›æŠ€æœ¯é—®é¢˜ï¼Œç°å·²è§£å†³ã€‚
æ„Ÿè°¢å„ä½å­¦å‘˜çš„è€å¿ƒå’ŒåŠªåŠ›ï¼Œç¥è´ºå¤§å®¶é¡ºåˆ©è·å¾—è¯ä¹¦ï¼
éƒ¨åˆ†å­¦å‘˜è¢«é™çº§ï¼šç”±äºæœªèƒ½æŒ‰æ—¶å®Œæˆè¯¾ç¨‹ä½œä¸šï¼Œéƒ¨åˆ†å­¦å‘˜çš„è¯ä¹¦ç­‰çº§è¢«é™è‡³ã€Œå¼€æ‹“è€…ã€çº§åˆ«ã€‚
å¾ˆé—æ†¾ï¼Œå°‘æ•°åŒå­¦å°†æ— æ³•è·å¾—è¯ä¹¦ï¼Œä¸”ä¸æä¾›è¡¥è€ƒæˆ–é‡æ–°è¯„åˆ†çš„æœºä¼šã€‚
æ¬¢è¿å‚ä¸æœªæ¥ MOOC è¯¾ç¨‹ï¼šå³ä½¿æœ¬æ¬¡å­¦ä¹ é‡åˆ°å›°éš¾ï¼Œæˆ‘ä»¬ä¹Ÿé¼“åŠ±å¤§å®¶æŠ¥åå‚åŠ  2025 å¹´æ˜¥å­£çš„ MOOC è¯¾ç¨‹ã€‚
è¯¾ç¨‹å›¢é˜Ÿå¸Œæœ›å¤§å®¶å–œæ¬¢æœ¬æ¬¡è¯¾ç¨‹ï¼Œå¹¶æœŸå¾…æœªæ¥èƒ½ä¸ºå¤§å®¶æä¾›æ›´å¤šå­¦ä¹ æœºä¼šï¼
å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ï¼ˆLLM Agentsï¼‰(Berkeley MOOCï¼‰â–· #mooc-questionsï¼ˆè®¨è®ºåŒºï¼Œ57 æ¡æ¶ˆæ¯ğŸ”¥ğŸ”¥)ï¼š
è¯ä¹¦å‘æ”¾æ—¶é—´ã€æµ‹éªŒå¼€æ”¾ä¸ç»“æœã€è¯ä¹¦ç­‰çº§åˆ’åˆ†ã€æ²Ÿé€šä¸æ”¯æŒã€è¯¾ç¨‹ä½“éªŒåé¦ˆ

Certificate Issuance Timeline Uncertainty: Some members inquired about the expected timeframe for receiving their certificates, with a member expressing hope for delivery within a week or two due to unforeseen technical issues being resolved.
Another member noted discrepancies in certificate receipt, indicating a potential soft bounce issue affecting communications.
Quiz Availability Confusion: Concerns arose over the availability of answers for Quiz-1 as Quiz-2 was launched, prompting members to seek clarification on the new policy regarding answer releases.
Members reassured fellow participants that score visibility for Quiz-1 was possible through the original link used for submission.
Certificate Tier Breakdown Revealed: In response to a query, it was disclosed that there are 301 Trailblazer, 138 Masters, 89 Ninjas, 11 Legends, and 7 Honorees amongst the participants.
This prompted interest in how many people received each tier certificate and clarified that only the honorary tier would be noted if both an honorary and a specific tier were achieved.
Effective Communication and Support Resolved Issues: The community expressed gratitude for the support received during the course, especially acknowledging the team handling grading and certificate queries.
Members encouraged clearer communications in cases where certificate statuses were pending, with some emails initially caught in spam filters.
Positive Feedback on Course Experience: Participants shared enthusiasm for the course, with one member reflecting on their learning journey and the significance of their certificate for future endeavors.
Expressions of appreciation were made for the course organization, highlighting the difficulty of grading numerous submissions while maintaining quality.
GPU MODE â–· #general (13 messagesğŸ”¥):
Output vs Input Token Pricing, Independent Research in AI/ML, Niche Fields for Research, Economizing AI Research

è¯ä¹¦é¢å‘æ—¶é—´ä¸ç¡®å®šæ€§ï¼šæœ‰æˆå‘˜è¯¢é—®é¢„è®¡ä½•æ—¶èƒ½æ”¶åˆ°è¯ä¹¦ï¼Œä¸€ä½æˆå‘˜å¸Œæœ›åœ¨ä¸€ä¸¤å‘¨å†…æ”¶åˆ°ï¼Œå› ä¸ºæœªé¢„è§åˆ°çš„æŠ€æœ¯é—®é¢˜æ­£åœ¨è§£å†³ã€‚
è¿˜æœ‰æˆå‘˜æ³¨æ„åˆ°è¯ä¹¦æ¥æ”¶æƒ…å†µä¸ä¸€è‡´ï¼Œè¿™è¡¨æ˜å¯èƒ½å­˜åœ¨è½¯é€€å›ï¼ˆsoft bounceï¼ŒæŒ‡é‚®ä»¶å‘é€å¤±è´¥ä½†æœåŠ¡å™¨æœªç«‹å³æ‹’ç»ï¼‰é—®é¢˜ï¼Œå½±å“äº†ä¿¡æ¯ä¼ é€’ã€‚
æµ‹éªŒå¯ç”¨æ€§å›°æƒ‘ï¼šåœ¨æµ‹éªŒ 2 å‘å¸ƒåï¼Œæœ‰æˆå‘˜å¯¹æµ‹éªŒ 1 çš„ç­”æ¡ˆä½•æ—¶å…¬å¸ƒè¡¨ç¤ºç–‘æƒ‘ï¼Œå¸Œæœ›äº†è§£ç­”æ¡ˆå‘å¸ƒçš„æ–°æ”¿ç­–ã€‚
å…¶ä»–æˆå‘˜å‘ŠçŸ¥å¤§å®¶ï¼Œä»ç„¶å¯ä»¥é€šè¿‡æäº¤æµ‹éªŒ 1 çš„åŸå§‹é“¾æ¥æŸ¥çœ‹åˆ†æ•°ã€‚
è¯ä¹¦ç­‰çº§åˆ’åˆ†ï¼šé’ˆå¯¹æˆå‘˜çš„æé—®ï¼Œå®˜æ–¹å…¬å¸ƒäº†å„ä¸ªç­‰çº§è¯ä¹¦çš„è·å¾—è€…æ•°é‡ï¼šå¼€æ‹“è€…ï¼ˆTrailblazerï¼‰301 åï¼Œå¤§å¸ˆï¼ˆMastersï¼‰138 åï¼Œå¿è€…ï¼ˆNinjasï¼‰89 åï¼Œä¼ å¥‡ï¼ˆLegendsï¼‰11 åï¼Œè£èª‰è€…ï¼ˆHonoreesï¼‰7 åã€‚
å¤§å®¶å¾ˆå…³å¿ƒæ¯ä¸ªç­‰çº§æœ‰å¤šå°‘äººè·å¾—è¯ä¹¦ã€‚å®˜æ–¹æ¾„æ¸…ï¼Œå¦‚æœæŸäººåŒæ—¶è·å¾—äº†è£èª‰ç­‰çº§å’Œå…¶ä»–ç­‰çº§çš„è¯ä¹¦ï¼Œåªä¼šæ˜¾ç¤ºè£èª‰ç­‰çº§ã€‚
æœ‰æ•ˆçš„æ²Ÿé€šå’Œæ”¯æŒè§£å†³äº†é—®é¢˜ï¼šå¤§å®¶å¯¹è¯¾ç¨‹æœŸé—´è·å¾—çš„æ”¯æŒè¡¨ç¤ºæ„Ÿè°¢ï¼Œç‰¹åˆ«æ„Ÿè°¢å¤„ç†è¯„åˆ†å’Œè¯ä¹¦ç›¸å…³é—®é¢˜çš„å›¢é˜Ÿã€‚
æœ‰æˆå‘˜å»ºè®®ï¼Œå¦‚æœè¯ä¹¦çŠ¶æ€æ˜¯å¾…å®šï¼Œå¸Œæœ›èƒ½å¤Ÿæ›´æ¸…æ™°åœ°å‘ŠçŸ¥å¤§å®¶ï¼Œå› ä¸ºæœ‰äº›é‚®ä»¶ä¸€å¼€å§‹è¢«è¯¯åˆ¤ä¸ºåƒåœ¾é‚®ä»¶ã€‚
å¯¹è¯¾ç¨‹ä½“éªŒçš„ç§¯æåé¦ˆï¼šå‚ä¸è€…ä»¬å¯¹è¯¾ç¨‹èµèµæœ‰åŠ ï¼Œä¸€ä½æˆå‘˜å›é¡¾äº†è‡ªå·±çš„å­¦ä¹ å†ç¨‹ï¼Œå¹¶è¡¨ç¤ºè¯ä¹¦å¯¹æœªæ¥çš„å‘å±•æ„ä¹‰é‡å¤§ã€‚
å¤§å®¶å¯¹è¯¾ç¨‹çš„ç»„ç»‡å·¥ä½œè¡¨ç¤ºæ„Ÿè°¢ï¼ŒåŒæ—¶ä¹Ÿç†è§£æ‰¹æ”¹å¤§é‡æäº¤çš„ä½œä¸šå¹¶ä¿è¯è´¨é‡éå¸¸ä¸å®¹æ˜“ã€‚
GPU MODE â–· #generalï¼ˆ13 æ¡æ¶ˆæ¯ğŸ”¥)ï¼š
è¾“å‡ºä¸è¾“å…¥ Token å®šä»·ï¼ŒAI/ML é¢†åŸŸçš„ç‹¬ç«‹ç ”ç©¶ï¼Œç ”ç©¶çš„åˆ©åŸºé¢†åŸŸï¼ŒAI ç ”ç©¶çš„ç»æµåŒ–

Output Token Pricing Causes Confusion: Members discussed the disparity in output and input token pricing, noting that GPT-4o charges $10 per million output tokens compared to $2.5 for inputs, primarily due to LLMs being autoregressive.
It's suggested that organizations like TogetherAI adopt a more straightforward aggregated pricing model.
Niche Research Areas for Independent Investigators: An independent researcher sought advice on feasible domains in AI/ML, emphasizing the impracticality of pretraining large models without funding and expressing interest in NLP, Audio, and Vision.
Members advised focusing on niche or untapped domains, with one sharing their success in computational metabolomics, emphasizing the limited competition in that area.
Minimalist AI Research Possibilities: Though niche research is advised, it was shared that independent researchers can also conduct efficient work on LLMs and vision tasks while fine-tuning models on a limited budget.
A member pointed out that significant progress can be made in economizing AI research, citing examples of reduced training times and innovative methodologies.
Value in Economizing AI Research: Discussions pointed out the importance of economizing aspects in AI research, like achieving stability with low-bit training weights and reducing environmental impact through efficient training methods.
The success of GPT-2 speedruns with Muon was highlighted as a prime example of impactful research using limited resources.
GPU MODE â–· #triton (4 messages):
Triton warp specialization, Triton compiler on NVIDIA Blackwell, Installing Triton on RTX 5080, Deepseek fused MLA implementation in Triton

è¾“å‡º Token å®šä»·è®©äººè´¹è§£ï¼š æˆå‘˜ä»¬è®¨è®ºäº†è¾“å‡º Token å’Œè¾“å…¥ Token ä¹‹é—´å®šä»·çš„å·®å¼‚ã€‚ä»–ä»¬æ³¨æ„åˆ°ï¼ŒGPT-4o å¯¹æ¯ç™¾ä¸‡ä¸ªè¾“å‡º Token æ”¶è´¹ 10 ç¾å…ƒï¼Œè€Œè¾“å…¥ Token çš„è´¹ç”¨ä»…ä¸º 2.5 ç¾å…ƒï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelï¼‰æ˜¯è‡ªå›å½’æ¨¡å‹ã€‚
æœ‰äººå»ºè®®åƒ TogetherAI è¿™æ ·çš„æœºæ„é‡‡ç”¨æ›´ç›´æ¥çš„ç»¼åˆå®šä»·æ¨¡å¼ã€‚
ç‹¬ç«‹ç ”ç©¶å‘˜çš„æ½œåœ¨ç ”ç©¶æ–¹å‘ï¼š ä¸€ä½ç‹¬ç«‹ç ”ç©¶å‘˜æ­£åœ¨å¯»æ‰¾å…³äºåœ¨äººå·¥æ™ºèƒ½ / æœºå™¨å­¦ä¹ ï¼ˆAI/MLï¼‰é¢†åŸŸä¸­ï¼Œå“ªäº›æ–¹å‘æŠ•å…¥è¾ƒå°‘èµ„é‡‘ä¹Ÿèƒ½æœ‰æ‰€ä½œä¸ºçš„å»ºè®®ã€‚ä»–å¼ºè°ƒï¼Œåœ¨æ²¡æœ‰èµ„é‡‘æ”¯æŒçš„æƒ…å†µä¸‹ï¼Œé¢„è®­ç»ƒå¤§å‹æ¨¡å‹æ˜¯ä¸ç°å®çš„ï¼Œå¹¶å¯¹è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ã€éŸ³é¢‘å’Œè§†è§‰é¢†åŸŸå¾ˆæ„Ÿå…´è¶£ã€‚
æœ‰æˆå‘˜å»ºè®®å…³æ³¨å°ä¼—æˆ–å°šæœªå¼€å‘çš„é¢†åŸŸã€‚ä¸€ä½æˆå‘˜åˆ†äº«äº†ä»–ä»¬åœ¨è®¡ç®—ä»£è°¢ç»„å­¦æ–¹é¢å–å¾—çš„æˆåŠŸï¼Œå¹¶å¼ºè°ƒè¯¥é¢†åŸŸçš„ç«äº‰ç›¸å¯¹è¾ƒå°ã€‚
æç®€ AI ç ”ç©¶çš„å¯èƒ½æ€§ï¼š è™½ç„¶å»ºè®®è¿›è¡Œå°ä¼—ç ”ç©¶ï¼Œä½†ä¹Ÿæœ‰äººæŒ‡å‡ºï¼Œå³ä½¿é¢„ç®—æœ‰é™ï¼Œç‹¬ç«‹ç ”ç©¶äººå‘˜ä»ç„¶å¯ä»¥é«˜æ•ˆåœ°è¿›è¡Œå¤§è¯­è¨€æ¨¡å‹å’Œè®¡ç®—æœºè§†è§‰ç›¸å…³çš„ç ”ç©¶ï¼Œæ¯”å¦‚å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
ä¸€ä½æˆå‘˜æŒ‡å‡ºï¼Œåœ¨é™ä½ AI ç ”ç©¶æˆæœ¬æ–¹é¢å­˜åœ¨å¾ˆå¤§çš„æ½œåŠ›ï¼Œå¹¶åˆ—ä¸¾äº†ç¼©çŸ­è®­ç»ƒæ—¶é—´å’Œåˆ›æ–°æ–¹æ³•ç­‰ä¾‹å­ã€‚
èŠ‚çº¦å‹ AI ç ”ç©¶çš„ä»·å€¼ï¼š è®¨è®ºå¼ºè°ƒäº†åœ¨ AI ç ”ç©¶ä¸­æ³¨é‡èŠ‚çº¦å„ä¸ªæ–¹é¢çš„é‡è¦æ€§ï¼Œä¾‹å¦‚ï¼Œé€šè¿‡ä½¿ç”¨ä½ç²¾åº¦ï¼ˆlow-bitï¼‰è®­ç»ƒæƒé‡æ¥å®ç°æ¨¡å‹ç¨³å®šæ€§ï¼Œä»¥åŠé€šè¿‡é«˜æ•ˆçš„è®­ç»ƒæ–¹æ³•æ¥å‡å°‘å¯¹ç¯å¢ƒçš„å½±å“ã€‚
ä½¿ç”¨ Muon è¿è¡Œ GPT-2 çš„å¿«é€Ÿå®éªŒçš„æˆåŠŸæ¡ˆä¾‹ï¼Œè¢«ç€é‡æåŠï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨æœ‰é™èµ„æºè¿›è¡Œæœ‰å½±å“åŠ›çš„ç ”ç©¶çš„ç»ä½³ä¾‹å­ã€‚
GPU MODE â–· #tritonï¼ˆ4 messages)ï¼š
Triton warp specializationï¼ˆTriton Warp ç‰¹åŒ–ï¼‰ï¼ŒNVIDIA Blackwell ä¸Šçš„ Triton ç¼–è¯‘å™¨ï¼Œåœ¨ RTX 5080 ä¸Šå®‰è£… Tritonï¼ŒDeepseek åœ¨ Triton ä¸­èåˆçš„ MLA å®ç°

Triton introduces warp specialization on NVIDIA Hopper: The recent rollout of fully automated Triton warp specialization for NVIDIA Hopper GPUs is now available in Triton 3.2 and will ship with PyTorch 2.6.
Users can take advantage of this feature by implementing user-defined Triton kernels as part of enhancing GPU capabilities.
Triton compiler supports NVIDIA Blackwell architecture: NVIDIAâ€™s ongoing collaboration with OpenAI has led to the Triton compiler now being compatible with the NVIDIA Blackwell architecture, enhancing performance and programmability.
This compatibility allows developers to utilize NVIDIA cuDNN and CUTLASS effectively for modern AI workloads.
Getting Triton operational on the new RTX 5080: A user documented challenges encountered while installing Triton on the new RTX 5080, including reinstalling drivers and rebuilding machine learning libraries from source.
They provided a guide for installing compatible drivers, highlighting the requirement for NVIDIA open kernel modules over proprietary ones to resolve device detection issues.
Inquiry on Deepseek fused MLA in Triton: A user raised a question regarding the availability of a Deepseek fused MLA implementation in Triton, indicating interest in this specific functionality.
Details regarding its support or development were not provided, leaving the inquiry open for further exploration.
Links mentioned:
Running PyTorch and Triton on the RTX 5080: I was beyond stoked at the opportunity of getting my hands on a new RTX 5080 to speed up my machine learning developments! Unfortunately, as soon as I connected the new GPU to my workstation I quickly...
Enabling advanced GPU features in PyTorch - Warp Specialization: Meta: Hongtao Yu, Manman Ren, Bert Maher, Shane Nay NVIDIA: Gustav Zhu, Shuhao Jiang
OpenAI Triton on NVIDIA Blackwell Boosts AI Performance and Programmability | NVIDIA Technical Blog: Matrix multiplication and attention mechanisms are the computational backbone of modern AI workloads. While libraries like NVIDIA cuDNN provide highly optimized implementations, and frameworks such as...
GPU MODE â–· #cuda (3 messages):
CUDA GEMM Implementation, Double Buffering Performance Issues, Register Usage Optimization, Memory Sector Utilization

Triton åœ¨ NVIDIA Hopper ä¸Šå¼•å…¥ Warp Specializationï¼ˆWarp Specialization)ï¼šæœ€è¿‘æ¨å‡ºçš„ç”¨äº NVIDIA Hopper GPU çš„å…¨è‡ªåŠ¨ Triton Warp Specializationï¼ˆWarp Specializationï¼‰ç°å·²åœ¨ Triton 3.2 ä¸­æä¾›ï¼Œå¹¶å°†ä¸ PyTorch 2.6 ä¸€èµ·å‘å¸ƒã€‚
ç”¨æˆ·å¯ä»¥åˆ©ç”¨æ­¤åŠŸèƒ½ï¼Œé€šè¿‡å®ç°ç”¨æˆ·è‡ªå®šä¹‰çš„ Triton å†…æ ¸æ¥å¢å¼º GPU çš„æ€§èƒ½ã€‚
Triton ç¼–è¯‘å™¨æ”¯æŒ NVIDIA Blackwell æ¶æ„ï¼šNVIDIA ä¸ OpenAI çš„æŒç»­åˆä½œä½¿å¾— Triton ç¼–è¯‘å™¨ç°åœ¨èƒ½å¤Ÿå…¼å®¹ NVIDIA Blackwell æ¶æ„ï¼Œä»è€Œæå‡äº†æ€§èƒ½å’Œå¯ç¼–ç¨‹æ€§ã€‚
è¿™ç§å…¼å®¹æ€§ä½¿å¾—å¼€å‘äººå‘˜èƒ½å¤Ÿæœ‰æ•ˆåœ°åˆ©ç”¨ NVIDIA cuDNN å’Œ CUTLASS æ¥å¤„ç†ç°ä»£ AI å·¥ä½œè´Ÿè½½ã€‚
åœ¨æ–° RTX 5080 ä¸Šè¿è¡Œ Tritonï¼šä¸€ä½ç”¨æˆ·è®°å½•äº†åœ¨æ–° RTX 5080 ä¸Šå®‰è£… Triton æ—¶é‡åˆ°çš„é—®é¢˜ï¼ŒåŒ…æ‹¬é‡æ–°å®‰è£…é©±åŠ¨ç¨‹åºå’Œä»æºä»£ç é‡æ–°ç¼–è¯‘æœºå™¨å­¦ä¹ åº“ã€‚
ä»–ä»¬æä¾›äº†ä¸€ä»½å®‰è£…å…¼å®¹é©±åŠ¨ç¨‹åºçš„æŒ‡å—ï¼Œå¼ºè°ƒéœ€è¦ä½¿ç”¨ NVIDIA å¼€æ”¾å†…æ ¸æ¨¡å—ï¼Œè€Œä¸æ˜¯ä¸“æœ‰æ¨¡å—ï¼Œä»¥è§£å†³è®¾å¤‡æ£€æµ‹é—®é¢˜ã€‚
å…³äº Triton ä¸­ Deepseek èåˆ MLA çš„å’¨è¯¢ï¼šä¸€ä½ç”¨æˆ·è¯¢é—® Triton ä¸­æ˜¯å¦æä¾›äº† Deepseek èåˆ MLA çš„å®ç°ï¼Œè¡¨è¾¾äº†å¯¹è¯¥åŠŸèƒ½çš„å…´è¶£ã€‚
ç›®å‰æ²¡æœ‰å…³äºå…¶æ”¯æŒæˆ–å¼€å‘çš„æ›´å¤šä¿¡æ¯ï¼Œè¯¥é—®é¢˜è¿˜æœ‰å¾…è¿›ä¸€æ­¥çš„æ¢ç´¢ã€‚
æåˆ°çš„é“¾æ¥ï¼š
åœ¨ RTX 5080 ä¸Šè¿è¡Œ PyTorch å’Œ Tritonï¼šä¸€ä½ç”¨æˆ·è¡¨ç¤ºï¼Œä»–å¾ˆé«˜å…´èƒ½æœ‰æœºä¼šä½¿ç”¨æ–°çš„ RTX 5080 æ¥åŠ é€Ÿæœºå™¨å­¦ä¹ å¼€å‘ã€‚ä½†ä¸å¹¸çš„æ˜¯ï¼Œå½“ä»–å°†æ–°çš„ GPU è¿æ¥åˆ°å·¥ä½œç«™åï¼Œå¾ˆå¿«å°±é‡åˆ°äº†é—®é¢˜...
åœ¨ PyTorch ä¸­å¯ç”¨é«˜çº§ GPU åŠŸèƒ½ - Warp Specializationï¼šMetaï¼šHongtao Yuï¼ŒManman Renï¼ŒBert Maherï¼ŒShane Nay NVIDIAï¼šGustav Zhuï¼ŒShuhao Jiang
NVIDIA Blackwell ä¸Šçš„ OpenAI Triton æå‡äº† AI æ€§èƒ½å’Œå¯ç¼–ç¨‹æ€§ | NVIDIA æŠ€æœ¯åšå®¢ï¼šçŸ©é˜µä¹˜æ³•å’Œæ³¨æ„åŠ›æœºåˆ¶æ˜¯ç°ä»£ AI å·¥ä½œè´Ÿè½½çš„è®¡ç®—æ”¯æŸ±ã€‚è™½ç„¶åƒ NVIDIA cuDNN è¿™æ ·çš„åº“æä¾›äº†é«˜åº¦ä¼˜åŒ–çš„å®ç°...
GPU MODE â–· #cudaï¼ˆ3 æ¡æ¶ˆæ¯)ï¼š
CUDA GEMM å®ç°ã€åŒç¼“å†²æ€§èƒ½é—®é¢˜ã€å¯„å­˜å™¨ä½¿ç”¨ä¼˜åŒ–ã€å†…å­˜æ‰‡åŒºåˆ©ç”¨ç‡

Double Buffering Drops Performance in CUDA GEMM: A user reported that implementing double buffering in their single-precision GEMM kernel led to a dramatic drop in performance metrics.
They noted that, according to the NCU profiler, their register usage per thread decreased significantly, indicating potential inefficiencies.
Register Usage and Compiler Challenges: A user suggested that dropping register usage might indicate the compiler's struggle with unrolling the new, more complex code, recommending the use of #pragma unroll for the loop.
They emphasized that simplifying the kernel could potentially lead to better register allocation.
Understanding Memory Sector Usage: Another member explained that each GPU cache line is divided into sectors, and the reported inefficiency means only 1 byte out of 32 is utilized in memory requests.
This suggests that the kernel is not efficiently accessing memory, which might be caused by stride accesses between threads.
GPU MODE â–· #algorithms (8 messagesğŸ”¥):
FP8 Attention, Hadamard Transform, CUDA Elementwise Kernel for Mixed Integer Linear Programming, Grouped GEMM Implementation, Torch Nested Tensor

åœ¨ CUDA GEMM ä¸­ä½¿ç”¨åŒç¼“å†²å¯¼è‡´æ€§èƒ½ä¸‹é™ï¼š ä¸€ä½ç”¨æˆ·æŠ¥å‘Šç§°ï¼Œåœ¨ä»–ä»¬çš„å•ç²¾åº¦ GEMMï¼ˆGeneral Matrix Multiplicationï¼‰å†…æ ¸ä¸­å®ç°åŒç¼“å†²åï¼Œæ€§èƒ½æŒ‡æ ‡å‡ºç°äº†æ˜¾è‘—ä¸‹é™ã€‚
ä»–ä»¬é€šè¿‡ NCU profiler å‘ç°ï¼Œæ¯ä¸ªçº¿ç¨‹çš„å¯„å­˜å™¨ä½¿ç”¨é‡æ˜æ˜¾å‡å°‘ï¼Œè¿™è¡¨æ˜å¯èƒ½å­˜åœ¨æ•ˆç‡é—®é¢˜ã€‚

å¯„å­˜å™¨ä½¿ç”¨ä¸ç¼–è¯‘å™¨ä¼˜åŒ–ï¼š æœ‰ç”¨æˆ·æŒ‡å‡ºï¼Œå¯„å­˜å™¨ä½¿ç”¨é‡çš„é™ä½å¯èƒ½æ„å‘³ç€ç¼–è¯‘å™¨éš¾ä»¥å±•å¼€è¿™æ®µæ–°çš„ã€æ›´å¤æ‚çš„ä»£ç ã€‚
å› æ­¤ï¼Œä»–ä»¬å»ºè®®åœ¨å¾ªç¯ä¸­ä½¿ç”¨ `#pragma unroll` æŒ‡ä»¤ã€‚
ä»–ä»¬å¼ºè°ƒï¼Œç®€åŒ–å†…æ ¸ä»£ç æœ‰åŠ©äºç¼–è¯‘å™¨æ›´å¥½åœ°åˆ†é…å¯„å­˜å™¨ã€‚

å†…å­˜æ‰‡åŒºåˆ©ç”¨ç‡åˆ†æï¼š å¦ä¸€ä½æˆå‘˜è§£é‡Šè¯´ï¼Œæ¯ä¸ª GPU ç¼“å­˜è¡Œéƒ½è¢«åˆ’åˆ†ä¸ºå¤šä¸ªæ‰‡åŒºã€‚
æŠ¥å‘Šæ˜¾ç¤ºçš„æ•ˆç‡ä½ä¸‹æ„å‘³ç€ï¼Œåœ¨æ¯æ¬¡å†…å­˜è¯·æ±‚ä¸­ï¼Œ32 ä¸ªå­—èŠ‚ä¸­ä»…æœ‰ 1 ä¸ªå­—èŠ‚è¢«å®é™…åˆ©ç”¨ã€‚
è¿™æš—ç¤ºç€å†…æ ¸å¯èƒ½æ²¡æœ‰æœ‰æ•ˆåœ°è®¿é—®å†…å­˜ï¼ŒåŸå› å¯èƒ½æ˜¯çº¿ç¨‹ä¹‹é—´å­˜åœ¨è·¨æ­¥è®¿é—®ã€‚

GPU MODE â–· #algorithmsï¼ˆ8 messagesğŸ”¥)ï¼š
FP8 æ³¨æ„åŠ›æœºåˆ¶ï¼ŒHadamard å˜æ¢ï¼Œç”¨äºæ··åˆæ•´æ•°çº¿æ€§è§„åˆ’çš„ CUDA é€å…ƒç´ å†…æ ¸ï¼Œåˆ†ç»„ GEMM å®ç°ï¼ŒTorch åµŒå¥—å¼ é‡

FP8 Attention relies on Hadamard Transform: A member observed that FP8 Attention for video models performed significantly better when utilizing the Hadamard Transform, drastically reducing error rates.
Referencing the Flash Attention 3 paper, they suggested that this approach is crucial not just for the attention mechanism, but for all operations in FP8.
CUDA for Mixed Integer Linear Programs: A member is exploring the feasibility of using a CUDA elementwise kernel for pairwise kernel operations that involve solving mixed integer linear programs, traditionally handled by CPUs using scipy.optimize.
They questioned if offloading the computation to CUDA would yield a significant speedup when handling many diverse computations simultaneously.
Grouped GEMM on GPUs and its implementation: One member inquired about the typical implementation of grouped GEMM on GPUs, asking if it simply loops over different group sizes as seen in some examples like Triton.
They raised a query regarding whether torch.nestedtensor utilizes a grouped GEMM approach in its operations.
Hadamard Transform Implementation Repository: A member recommended using the fast-hadamard-transform repository to implement Hadamard before the attention mechanism.
This library offers CUDA implementation with a PyTorch interface that can enhance performance for operations needing the Hadamard Transform.
Mixed Integer Programming optimization conversation: A member expressed skepticism about using CUDA for solving mixed integer programming due to its challenges, while exploring if a single thread could allow for a more competitive speedup.
Another user chimed in to suggest that the merit of a CUDA approach would depend heavily on the specific workload and kernel design.
Link mentioned: fast-hadamard-transform/csrc at master Â· Dao-AILab/fast-hadamard-transform: Fast Hadamard transform in CUDA, with a PyTorch interface - Dao-AILab/fast-hadamard-transform

FP8 æ³¨æ„åŠ›æœºåˆ¶åˆ©ç”¨ Hadamard å˜æ¢ï¼šä¸€ä½æˆå‘˜å‘ç°ï¼Œå¯¹äºè§†é¢‘æ¨¡å‹ï¼Œä½¿ç”¨ Hadamard å˜æ¢èƒ½æ˜¾è‘—æå‡ FP8 æ³¨æ„åŠ›çš„æ€§èƒ½ï¼Œå¹¶å¤§å¹…é™ä½é”™è¯¯ç‡ã€‚
ä»–ä»¬å‚è€ƒäº† Flash Attention 3 è®ºæ–‡ï¼Œå¹¶æŒ‡å‡º Hadamard å˜æ¢ä¸ä»…å¯¹æ³¨æ„åŠ›æœºåˆ¶è‡³å…³é‡è¦ï¼Œå¯¹ FP8 ä¸­çš„æ‰€æœ‰è¿ç®—ä¹ŸåŒæ ·é‡è¦ã€‚
CUDA ç”¨äºæ··åˆæ•´æ•°çº¿æ€§è§„åˆ’ï¼šä¸€ä½æˆå‘˜æ­£åœ¨ç ”ç©¶ä½¿ç”¨ CUDA å…ƒç´ çº§å†…æ ¸ï¼ˆCUDA elementwise kernelï¼‰æ‰§è¡Œæˆå¯¹å†…æ ¸è¿ç®—ï¼ˆpairwise kernel operationsï¼‰çš„å¯è¡Œæ€§ï¼Œè¿™ç±»è¿ç®—é€šå¸¸ä½¿ç”¨ CPU å’Œ scipy.optimize åº“æ¥è§£å†³æ··åˆæ•´æ•°çº¿æ€§è§„åˆ’é—®é¢˜ã€‚
ä»–ä»¬æƒ³çŸ¥é“ï¼Œå½“åŒæ—¶å¤„ç†å¤§é‡ä¸åŒçš„è®¡ç®—æ—¶ï¼Œå°†è®¡ç®—ä»»åŠ¡è½¬ç§»åˆ° CUDA ä¸Šæ˜¯å¦èƒ½å¸¦æ¥æ˜¾è‘—çš„åŠ é€Ÿæ•ˆæœã€‚
GPU ä¸Šçš„åˆ†ç»„ GEMM åŠå…¶å®ç°ï¼šä¸€ä½æˆå‘˜å’¨è¯¢äº† GPU ä¸Šåˆ†ç»„ GEMMï¼ˆgrouped GEMMï¼‰çš„å…¸å‹å®ç°æ–¹å¼ï¼Œå¹¶è¯¢é—®å…¶å®ç°æ˜¯å¦åƒ Triton ä¸­çš„ä¸€äº›ä¾‹å­ä¸€æ ·ï¼Œç®€å•åœ°å¾ªç¯éå†ä¸åŒçš„ç»„å¤§å°ã€‚
ä»–ä»¬è¿˜æƒ³äº†è§£ torch.nestedtensor åœ¨å…¶è¿ç®—ä¸­æ˜¯å¦é‡‡ç”¨äº†åˆ†ç»„ GEMM çš„æ–¹æ³•ã€‚
Hadamard å˜æ¢çš„å®ç°ä»“åº“ï¼šä¸€ä½æˆå‘˜å»ºè®®ä½¿ç”¨ fast-hadamard-transform ä»“åº“ï¼Œåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¹‹å‰å®ç° Hadamard å˜æ¢ã€‚
è¯¥åº“æä¾›äº† CUDA å®ç°ä»¥åŠ PyTorch æ¥å£ï¼Œå¯ä»¥æå‡éœ€è¦ Hadamard å˜æ¢çš„è¿ç®—çš„æ€§èƒ½ã€‚
å…³äºæ··åˆæ•´æ•°è§„åˆ’ä¼˜åŒ–çš„è®¨è®ºï¼šä¸€ä½æˆå‘˜å¯¹ä½¿ç”¨ CUDA è§£å†³æ··åˆæ•´æ•°è§„åˆ’é—®é¢˜æŒæ€€ç–‘æ€åº¦ï¼Œå› ä¸ºè¿™é¢ä¸´è¯¸å¤šæŒ‘æˆ˜ã€‚åŒæ—¶ï¼Œä»–ä»¬ä¹Ÿåœ¨æ¢ç´¢ä½¿ç”¨å•çº¿ç¨‹æ˜¯å¦èƒ½å¤Ÿå®ç°æ›´å…·ç«äº‰åŠ›çš„åŠ é€Ÿæ•ˆæœã€‚
å¦ä¸€ä½ç”¨æˆ·è¡¥å……è¯´ï¼ŒCUDA æ–¹æ³•çš„ä»·å€¼å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå…·ä½“çš„å·¥ä½œè´Ÿè½½å’Œå†…æ ¸è®¾è®¡ã€‚
ç›¸å…³é“¾æ¥ï¼šfast-hadamard-transform/csrc at masterÂ·Dao-AILab/fast-hadamard-transformï¼šFast Hadamard transform in CUDAï¼Œwith a PyTorch interface - Dao-AILab/fast-hadamard-transform

GPU MODE â–· #cool-links (1 messages):
iron_bound: https://www.youtube.com/watch?v=7xTGNNLPyMI

GPU MODE â–· #torchao (2 messages):
PyTorch Team Visibility, User Concerns

GPU MODE â–· #cool-linksï¼ˆ1 æ¡æ¶ˆæ¯):
iron_boundï¼šhttps://www.youtube.com/watch?v=7xTGNNLPyMI

GPU MODE â–· #torchaoï¼ˆ2 æ¡æ¶ˆæ¯):
PyTorch å›¢é˜Ÿçš„å¯è§æ€§ï¼Œä»¥åŠç”¨æˆ·å…³æ³¨çš„é—®é¢˜

User Shares Frustration: A user expressed their frustration with the comment 'mega oof'.
This sentiment highlights an ongoing concern among members regarding issues that need attention.
Raising Issues for Visibility: Another member suggested that the frustrated user comment on the issue to increase visibility to the PyTorch team ğŸ˜„.
This approach aims to ensure that important concerns are addressed by those able to resolve them.
GPU MODE â–· #off-topic (2 messages):
Japanese government discussions, Text-generation-inference n-gram decoding

ç”¨æˆ·è¡¨è¾¾ä¸æ»¡ï¼šä¸€ä½ç”¨æˆ·è¡¨è¾¾äº†å¯¹è¯„è®ºã€Œmega oofã€çš„ä¸æ»¡ã€‚
è¿™ç§æƒ…ç»ªåæ˜ äº†ç¤¾ç¾¤æˆå‘˜å¯¹ä¸€äº›äºŸå¾…è§£å†³çš„é—®é¢˜çš„æŒç»­å…³æ³¨ã€‚
æå‡é—®é¢˜èƒ½è§åº¦ï¼šå¦ä¸€ä½æˆå‘˜å»ºè®®è¿™ä½ä¸æ»¡çš„ç”¨æˆ·åœ¨ç›¸å…³é—®é¢˜ä¸‹å‘è¡¨è¯„è®ºï¼Œä»¥æå‡ PyTorch å›¢é˜Ÿçš„å…³æ³¨åº¦ğŸ˜„ã€‚
è¿™ä¸ªå»ºè®®æ—¨åœ¨ç¡®ä¿é‡è¦é—®é¢˜èƒ½å¤Ÿè¢«æœ‰èƒ½åŠ›è§£å†³é—®é¢˜çš„äººå‘˜æ³¨æ„åˆ°å¹¶åŠ ä»¥å¤„ç†ã€‚
GPU æ¨¡å¼ â–· #off-topicï¼ˆ2 æ¡æ¶ˆæ¯)ï¼š
æ—¥æœ¬æ”¿åºœè®¨è®ºï¼Œæ–‡æœ¬ç”Ÿæˆæ¨ç† n-gram è§£ç ï¼ˆText-generation-inference n-gram decoding)

Japanese Government's Involvement Discussed: The conversation briefly touched on the role of the Japanese government in related discussions.
Specific details about their actions or position were not provided in the messages.
Inquiring about n-gram Speculative Decoding: A member asked for experiences using text-generation-inference's n-gram speculative decoding implementation.
No responses with firsthand experiences were reported in this message history.
GPU MODE â–· #thunderkittens (1 messages):
Linear Attention Model, Distillation Process, Training Challenges

æ¢è®¨æ—¥æœ¬æ”¿åºœçš„å‚ä¸ï¼šå¯¹è¯ä¸­ç®€è¦æåŠäº†æ—¥æœ¬æ”¿åºœåœ¨ç›¸å…³è®¨è®ºä¸­æ‰€æ‰®æ¼”çš„è§’è‰²ã€‚
æ¶ˆæ¯ä¸­æ²¡æœ‰æä¾›å…³äºä»–ä»¬çš„å…·ä½“è¡ŒåŠ¨æˆ–ç«‹åœºçš„è¯¦ç»†ä¿¡æ¯ã€‚
å’¨è¯¢ n-gram æ¨æµ‹è§£ç ï¼šæœ‰æˆå‘˜æé—®ï¼Œæ˜¯å¦æœ‰ä½¿ç”¨è¿‡ text-generation-inference çš„ n-gram æ¨æµ‹è§£ç ï¼ˆn-gram speculative decodingï¼‰å®ç°çš„ç»éªŒã€‚
åœ¨æœ¬æ¬¡æ¶ˆæ¯è®°å½•ä¸­ï¼Œå°šæœªæœ‰äººåˆ†äº«ç›´æ¥çš„ä½¿ç”¨ç»éªŒã€‚
GPU æ¨¡å¼ â–· #thunderkittensï¼ˆ1 æ¡æ¶ˆæ¯)ï¼š
çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹ï¼ˆLinear Attention Modelï¼‰ï¼Œè’¸é¦è¿‡ç¨‹ï¼ˆDistillation Processï¼‰ï¼Œè®­ç»ƒæŒ‘æˆ˜

Struggles with Linear Attention Model Distillation: A member attempted to distill a small LLM to a linear attention model following a recipe from Lolcats but faced issues.
The model only produced repeating characters, prompting a request for assistance from the Lolcats team.
Request for Help from Lolcats Team: In response to the training challenges, the member reached out for help specifically from the Lolcats team.
This plea highlights the community support aspect often relied upon in AI model development.
GPU MODE â–· #reasoning-gym (18 messagesğŸ”¥):
Sokoban Puzzles, Rush Hour Puzzle, Reasoning-Gym Integration

çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹è’¸é¦é‡åˆ°çš„å›°éš¾ï¼šä¸€ä½æˆå‘˜å°è¯•ä½¿ç”¨ Lolcats æä¾›çš„æ–¹æ³•ï¼Œå°†ä¸€ä¸ªå°å‹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM/Large Language Modelï¼‰è®­ç»ƒæˆçº¿æ€§æ³¨æ„åŠ›æ¨¡å‹ï¼Œä½†é‡åˆ°äº†é—®é¢˜ã€‚
æ¨¡å‹æ€»æ˜¯ç”Ÿæˆé‡å¤çš„å­—ç¬¦ï¼Œå› æ­¤ä»–å‘ Lolcats å›¢é˜Ÿæ±‚åŠ©ã€‚
å‘ Lolcats å›¢é˜Ÿè¯·æ±‚å¸®åŠ©ï¼šé¢å¯¹è®­ç»ƒéš¾é¢˜ï¼Œè¯¥æˆå‘˜ç‰¹åˆ«å‘ Lolcats å›¢é˜Ÿå¯»æ±‚å¸®åŠ©ã€‚
è¿™ä½“ç°äº†äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ¨¡å‹å¼€å‘ä¸­å¸¸è§çš„ç¤¾åŒºäº’åŠ©æ¨¡å¼ã€‚
GPU MODE â–· #reasoning-gymï¼ˆ18 æ¡æ¶ˆæ¯ğŸ”¥)ï¼š
Sokoban Puzzlesï¼ŒRush Hour Puzzleï¼ŒReasoning-Gym é›†æˆ

Sokoban Puzzles added to Reasoning Gym: A pull request was submitted to add Sokoban puzzles to reasoning-gym, demonstrating a new puzzle format for users to solve.
The pull request includes a graphic explanation of the puzzle setup along with example moves as a string of characters like LDURRUDL.
Rush Hour puzzle scripting ideas: Members discussed creating a text-interface for representing moves in the Rush Hour game and shared useful resources for understanding the puzzle's mechanics.
A shared link led to a blog detailing how to programmatically solve the Rush Hour puzzle, which featured a grid format outline.
Local S1 Running for Reasoning Gym Gauntlet: One member inquired about anyone having S1 running locally to test its capabilities on the reasoning-gym gauntlet.
They expressed eagerness to observe how it performs against known challenges.
Rush Hour GitHub Repository Shared: A member shared a GitHub repository containing a project for Rush Hour, indicating the ease of accessing it for practical use.
The repository is focused on heuristic strategies and invites contributions to the development.
Collaborative Efforts on Rush Hour Game: Members expressed enthusiasm in collaboratively building a basic gym for the Rush Hour game integration into the reasoning-gym.
This project would encourage collaborative coding efforts to implement the classic puzzle as a feature.
Links mentioned:
Rush Hour (puzzle) - Wikipedia: no description found
Michael Fogleman: no description found
GitHub - KaKariki02/rushHour: heuristieken project: heuristieken project. Contribute to KaKariki02/rushHour development by creating an account on GitHub.
Add Sokoban Puzzles by Miserlou Â· Pull Request #66 Â· open-thought/reasoning-gym: Ex:This is a sokoban puzzle. Please solve it. Your solution must be a string of characters, ex: LDURRUDL+ + + + + + ++ * - @ - X ++ + - @ - + ++ X - - - $ ++ + + + + + +* - The player% - ...
Nomic.ai (GPT4All) â–· #general (48 messagesğŸ”¥):
Model Performance Comparison, Language Model Constraints, DeepSeek Model Insights

Sokoban æ¨ç®±å­æ¸¸æˆåŠ å…¥æ¨ç†å¥èº«æˆ¿ï¼šæœ‰äººæäº¤äº†ä¸€ä¸ªè¯·æ±‚ï¼Œå°† Sokoban æ¨ç®±å­æ¸¸æˆåŠ å…¥åˆ° Reasoning Gymï¼ˆæ¨ç†å¥èº«æˆ¿ï¼‰å¹³å°ï¼Œä¸ºç”¨æˆ·æä¾›äº†ä¸€ç§æ–°çš„è°œé¢˜å½¢å¼ã€‚
è¿™ä¸ªè¯·æ±‚ä¸­åŒ…å«äº†è°œé¢˜è®¾ç½®çš„å›¾ç¤ºï¼Œä»¥åŠç”¨ LDURRUDL è¿™æ ·ä¸€ä¸²å­—ç¬¦è¡¨ç¤ºçš„ç¤ºä¾‹è§£æ³•ã€‚
Rush Hour äº¤é€šå µå¡æ¸¸æˆçš„è„šæœ¬æ„æ€ï¼šæˆå‘˜ä»¬è®¨è®ºäº†ä¸º Rush Hour äº¤é€šå µå¡æ¸¸æˆä¸­ç§»åŠ¨çš„æ–¹å¼åˆ›å»ºä¸€ä¸ªæ–‡æœ¬ç•Œé¢ï¼Œå¹¶åˆ†äº«äº†ç†è§£è¯¥æ¸¸æˆæœºåˆ¶çš„æœ‰ç”¨èµ„æºã€‚
åˆ†äº«çš„é“¾æ¥æŒ‡å‘ä¸€ç¯‡åšå®¢ï¼Œå…¶ä¸­è¯¦ç»†ä»‹ç»äº†å¦‚ä½•ç”¨ç¼–ç¨‹æ–¹å¼è§£å†³ Rush Hour äº¤é€šå µå¡æ¸¸æˆï¼Œå¹¶å±•ç¤ºäº†ç½‘æ ¼åŒ–çš„æ¸¸æˆå¸ƒå±€ã€‚
æœ¬åœ°è¿è¡Œ S1 è¿›è¡Œ Reasoning Gym æŒ‘æˆ˜ï¼šæœ‰æˆå‘˜è¯¢é—®æ˜¯å¦æœ‰äººåœ¨æœ¬åœ°è¿è¡Œ S1 æ¨¡å‹ï¼Œä»¥æµ‹è¯•å…¶åœ¨ Reasoning Gym å¹³å°çš„æŒ‘æˆ˜ä¸­çš„è¡¨ç°ã€‚
ä»–ä»¬å¾ˆæƒ³çŸ¥é“ S1 åœ¨å·²çŸ¥æŒ‘æˆ˜ä¸­çš„è¡¨ç°å¦‚ä½•ã€‚
Rush Hour äº¤é€šå µå¡æ¸¸æˆ GitHub ä»“åº“åˆ†äº«ï¼šä¸€ä½æˆå‘˜åˆ†äº«äº†ä¸€ä¸ªåŒ…å« Rush Hour äº¤é€šå µå¡æ¸¸æˆé¡¹ç›®çš„ GitHub ä»“åº“ï¼Œæ–¹ä¾¿å¤§å®¶è¿›è¡Œå®è·µã€‚
è¯¥ä»“åº“ä¸“æ³¨äº Rush Hour äº¤é€šå µå¡æ¸¸æˆçš„å¯å‘å¼ç®—æ³•ï¼Œå¹¶æ¬¢è¿å¤§å®¶ä¸ºå¼€å‘åšå‡ºè´¡çŒ®ã€‚
Rush Hour äº¤é€šå µå¡æ¸¸æˆçš„åˆä½œå¼€å‘ï¼šæˆå‘˜ä»¬è¡¨è¾¾äº†åˆä½œæ„å»ºä¸€ä¸ªåŸºç¡€ gym ç¯å¢ƒçš„æ„æ„¿ï¼Œä»¥ä¾¿å°† Rush Hour äº¤é€šå µå¡æ¸¸æˆæ•´åˆåˆ° Reasoning Gym å¹³å°ä¸­ã€‚
è¿™ä¸ªé¡¹ç›®æ—¨åœ¨é¼“åŠ±å¤§å®¶å…±åŒç¼–å†™ä»£ç ï¼Œå°†è¿™ä¸ªç»å…¸è°œé¢˜å®ç°ä¸ºä¸€ä¸ªæ–°åŠŸèƒ½ã€‚

é“¾æ¥æåŠï¼š
Rush Hourï¼ˆpuzzleï¼‰- Wikipediaï¼šæœªæ‰¾åˆ°æè¿°
Michael Foglemanï¼šæœªæ‰¾åˆ°æè¿°
GitHub - KaKariki02/rushHourï¼šheuristieken projectï¼šheuristieken project. Contribute to KaKariki02/rushHour development by creating an account on GitHub.
Add Sokoban Puzzles by MiserlouÂ·Pull Request #66Â·open-thought/reasoning-gymï¼šEx:This is a sokoban puzzle. Please solve it. Your solution must be a string of charactersï¼Œexï¼šLDURRUDL+ + + + + + ++ * - @ - X ++ + - @ - + ++ X - - - $ ++ + + + + + +* - The player% - ...
Nomic.aiï¼ˆGPT4Allï¼‰â–· #generalï¼ˆ48 messagesğŸ”¥):
Model Performance Comparisonï¼ŒLanguage Model Constraintsï¼ŒDeepSeek Model Insights

Discussions on Model Comparisons: Users discussed the performance of various models, noting that O3 is still ahead despite concerns about its pricing.
Llama 4 is anticipated as the next potential successor to challenge existing models.
Limitations in Political Discussions: There was a consensus that various languages models have constraints, with DeepSeek demonstrating greater limitations compared to ChatGPT and O3-mini.
Members noted that prompts regarding sensitive political topics often lead to unexpected deletions or evasions in responses.
DeepSeek's Cutoff Date and Capabilities: It was noted that DeepSeek's knowledge cutoff date is reportedly July 2024, raising questions about its current relevance.
An interesting method called Time Bandit has been discussed for extracting information by leveraging temporal context.
Links mentioned:
deepseek-ai/Janus-Pro-7B Â· Hugging Face: no description found
DeepSeekâ€™s cutoff date is July 2024: We extracted DeepSeekâ€™s system prompt: Discover how the "Time Bandit" method reveals DeepSeek's system prompt, exploring its ethical guidelines and neutrality in global contexts. Learn the implications for AI interactions.
DeepSeek-R1 vs o3: In-depth DeepSeek-R1 vs o3 comparison: Latest benchmarks, pricing, context window, performance metrics, and technical specifications in 2025.
o3-mini vs DeepSeek-R1: In-depth o3-mini vs DeepSeek-R1 comparison: Latest benchmarks, pricing, context window, performance metrics, and technical specifications in 2025.
Torchtune â–· #general (30 messagesğŸ”¥):
GRPO implementation success, Kolo support for Torchtune, Config issues with Llama 3.1 and Qwen 2.5, Hugging Face fast tokenizer support

æ¨¡å‹æ€§èƒ½æ¯”è¾ƒï¼šç”¨æˆ·è®¨è®ºäº†å„ç§æ¨¡å‹çš„æ€§èƒ½æ¯”è¾ƒï¼ŒæŒ‡å‡ºå°½ç®¡ O3 çš„å®šä»·å¤‡å—å…³æ³¨ï¼Œä½†å…¶æ€§èƒ½ä¾ç„¶é¢†å…ˆã€‚
Llama 4 è¢«è®¤ä¸ºæ˜¯ä¸‹ä¸€ä¸ªæœ‰æ½œåŠ›æŒ‘æˆ˜ç°æœ‰æ¨¡å‹çš„æœ‰åŠ›ç«äº‰è€…ã€‚
æ”¿æ²»è®¨è®ºçš„å±€é™æ€§ï¼šäººä»¬æ™®éè®¤ä¸ºï¼Œå„ç§å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM/Large Language Modelï¼‰éƒ½å­˜åœ¨ä¸€å®šçš„å±€é™æ€§ã€‚ç›¸è¾ƒäº ChatGPT å’Œ O3-miniï¼ŒDeepSeek åœ¨è¿™æ–¹é¢çš„è¡¨ç°å°¤ä¸ºæ˜æ˜¾ã€‚
æœ‰æˆå‘˜æŒ‡å‡ºï¼Œå½“æç¤ºæ¶‰åŠæ•æ„Ÿæ”¿æ²»è¯é¢˜æ—¶ï¼ŒDeepSeek ç»å¸¸å‡ºç°æ„å¤–åˆ é™¤å†…å®¹æˆ–å›é¿é—®é¢˜çš„ç°è±¡ã€‚
DeepSeek çš„çŸ¥è¯†æˆªæ­¢æ—¥æœŸå’Œèƒ½åŠ›ï¼šæ®ç§°ï¼ŒDeepSeek çš„çŸ¥è¯†æˆªæ­¢æ—¥æœŸä¸º 2024 å¹´ 7 æœˆï¼Œè¿™å¼•å‘äº†å…³äºå…¶å½“å‰çŸ¥è¯†æ—¶æ•ˆæ€§çš„è®¨è®ºã€‚
ç ”ç©¶äººå‘˜è®¨è®ºäº†ä¸€ç§åä¸º Time Bandit çš„æœ‰è¶£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨æ—¶é—´ä¸Šä¸‹æ–‡æ¥æå–ä¿¡æ¯ã€‚
é“¾æ¥æåŠï¼š
deepseek-ai/Janus-Pro-7BÂ·Hugging Faceï¼šæœªæ‰¾åˆ°æè¿°
DeepSeek çš„æˆªæ­¢æ—¥æœŸæ˜¯ 2024 å¹´ 7 æœˆï¼šæˆ‘ä»¬æå–äº† DeepSeek çš„ç³»ç»Ÿæç¤ºï¼šäº†è§£ã€Œæ—¶é—´å¼ºç›—ã€æ–¹æ³•å¦‚ä½•æ­ç¤º DeepSeek çš„ç³»ç»Ÿæç¤ºï¼Œæ¢ç´¢å…¶åœ¨å…¨å±€èƒŒæ™¯ä¸‹çš„ä¼¦ç†è§„èŒƒå’Œå®¢è§‚æ€§ã€‚æ¢è®¨å…¶å¯¹ AI äº’åŠ¨æ–¹å¼çš„å½±å“ã€‚
DeepSeek-R1 vs o3ï¼šæ·±å…¥çš„ DeepSeek-R1 ä¸ o3 æ¯”è¾ƒï¼š2025 å¹´çš„æœ€æ–°åŸºå‡†ã€å®šä»·ã€ä¸Šä¸‹æ–‡çª—å£ã€æ€§èƒ½æŒ‡æ ‡å’ŒæŠ€æœ¯è§„æ ¼ã€‚
o3-mini vs DeepSeek-R1ï¼šæ·±å…¥çš„ o3-mini ä¸ DeepSeek-R1 æ¯”è¾ƒï¼š2025 å¹´çš„æœ€æ–°åŸºå‡†ã€å®šä»·ã€ä¸Šä¸‹æ–‡çª—å£ã€æ€§èƒ½æŒ‡æ ‡å’ŒæŠ€æœ¯è§„æ ¼ã€‚
Torchtune â–· #generalï¼ˆ30 messagesğŸ”¥)ï¼š
GRPO å®ç°æˆåŠŸï¼ŒKolo æ”¯æŒ Torchtuneï¼ŒLlama 3.1 å’Œ Qwen 2.5 çš„é…ç½®é—®é¢˜ï¼ŒHugging Face å¿«é€Ÿåˆ†è¯å™¨æ”¯æŒ

GRPO implementation sees success: A member reported a successful implementation of GRPO training, achieving training scores from 10% to 40% on GSM8k.
Debugging issues noted include deadlocks and memory management challenges, but plans are being made to improve and open the project to contributions.
Kolo now supports Torchtune: Excitement was shared as Kolo officially announced support for Torchtune on their GitHub page.
The project provides a comprehensive solution for fine-tuning and testing LLMs locally using the best available tools.
Identified config issues with Llama 3.1 and Qwen 2.5: Several members noted FileNotFoundError issues when downloading and fine-tuning Llama 3.1 and Qwen 2.5 due to mismatched path configurations.
A member created a GitHub issue to address the incorrect default paths and proposed fixes.
Future support for Hugging Face fast tokenizers: The possibility of using Hugging Face fast tokenizers was discussed, indicating current limitations but ongoing progress.
A member mentioned that Evan is working on enabling support, as noted in a GitHub pull request.
Links mentioned:
GitHub - MaxHastings/Kolo: A one stop shop for fine tuning and testing LLMs locally using the best tools available.: A one stop shop for fine tuning and testing LLMs locally using the best tools available. - MaxHastings/Kolo
Build software better, together: GitHub is where people build software. More than 150 million people use GitHub to discover, fork, and contribute to over 420 million projects.
torchtune/recipes/configs/llama3_1 at main Â· pytorch/torchtune: PyTorch native post-training library. Contribute to pytorch/torchtune development by creating an account on GitHub.
Incorrect Default Config File Paths for Llama 3.1 8B and Qwen 2.5 7B Models Â· Issue #2352 Â· pytorch/torchtune: I've noticed an issue where the downloaded model directories for Llama 3.1 8B and Qwen 2.5 7B do not match the paths expected in their respective default config files. Llama 3.1 8B Issue The downl...
torchtune/recipes/configs/qwen2_5/7B_lora_single_device.yaml at a226a58b8c36db5afa123f0885c5337d1ebc91f6 Â· pytorch/torchtune: PyTorch native post-training library. Contribute to pytorch/torchtune development by creating an account on GitHub.
Feature request: GRPO support Â· Issue #2340 Â· pytorch/torchtune: As you all might have already known by now DeepSeek-R1 with its GRPO training was quite successful, should we consider bringing GRPO into torchtune?
HF tokenizers: initial base tokenizer support by ebsmothers Â· Pull Request #2350 Â· pytorch/torchtune: Fixes #2212This is an initial PR to support general tokenizers from Hugging Face via a tokenizer.json file. This is just a starting point to parse relevant JSON files, infer BOS and EOS, and defin...
Torchtune â–· #dev (16 messagesğŸ”¥):
GitHub Checks on Full DPO Distributed PR, GPU Testing Issues, Recipe Test Failures, VRAM Usage Optimization

GRPO å®æ–½è·å¾—æˆåŠŸï¼šä¸€ä½æˆå‘˜æŠ¥å‘ŠæˆåŠŸåº”ç”¨ GRPO è®­ç»ƒï¼Œåœ¨ GSM8k æ•°æ®é›†ä¸Šå–å¾—äº† 10% åˆ° 40% çš„è®­ç»ƒåˆ†æ•°æå‡ã€‚
è°ƒè¯•é—®é¢˜åŒ…æ‹¬æ­»é”å’Œå†…å­˜ç®¡ç†æŒ‘æˆ˜ï¼Œä½†æ­£åœ¨åˆ¶å®šè®¡åˆ’ä»¥æ”¹è¿›å¹¶å°†é¡¹ç›®å¼€æ”¾ä»¥ä¾›è´¡çŒ®ã€‚
Kolo ç°åœ¨æ”¯æŒ Torchtuneï¼šKolo åœ¨ GitHub é¡µé¢ä¸Šæ­£å¼å®£å¸ƒæ”¯æŒ Torchtuneï¼Œè¿™ä¸€æ¶ˆæ¯å¼•èµ·äº†å¹¿æ³›å…³æ³¨ã€‚
è¯¥é¡¹ç›®æ—¨åœ¨æä¾›ä¸€ä¸ªå…¨é¢çš„è§£å†³æ–¹æ¡ˆï¼Œåˆ©ç”¨æœ€ä½³å·¥å…·åœ¨æœ¬åœ°å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œå¾®è°ƒå’Œæµ‹è¯•ã€‚
æœ‰æˆå‘˜å‘ç° Llama 3.1 å’Œ Qwen 2.5 å­˜åœ¨é…ç½®é—®é¢˜ï¼šç”±äºè·¯å¾„é…ç½®ä¸ä¸€è‡´ï¼Œå¯¼è‡´åœ¨ä¸‹è½½å’Œå¾®è°ƒè¿™äº›æ¨¡å‹æ—¶å‡ºç° FileNotFoundError é”™è¯¯ã€‚
æœ‰å¼€å‘è€…åœ¨ GitHub ä¸Šæäº¤ issueï¼Œæ—¨åœ¨è§£å†³é»˜è®¤è·¯å¾„ä¸æ­£ç¡®çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ç›¸åº”çš„ä¿®å¤æ–¹æ¡ˆã€‚
æœªæ¥è®¡åˆ’æ”¯æŒ Hugging Face å¿«é€Ÿ Tokenizerï¼šç›®å‰æ­£åœ¨è®¨è®ºä½¿ç”¨ Hugging Face å¿«é€Ÿ Tokenizer çš„å¯è¡Œæ€§ï¼Œè™½ç„¶å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä½†å·²å–å¾—è¿›å±•ã€‚
æœ‰å¼€å‘è€…æåˆ° Evan æ­£åœ¨åŠªåŠ›å®ç°å¯¹è¯¥åŠŸèƒ½çš„æ”¯æŒï¼Œç›¸å…³ä¿¡æ¯å¯åœ¨ GitHub çš„ pull request ä¸­æ‰¾åˆ°ã€‚
æåˆ°çš„é“¾æ¥ï¼š
GitHub - MaxHastings/Koloï¼šA one stop shop for fine tuning and testing LLMs locally using the best tools available.ï¼šA one stop shop for fine tuning and testing LLMs locally using the best tools available. - MaxHastings/Kolo
Build software betterï¼Œtogetherï¼šGitHub is where people build software. More than 150 million people use GitHub to discoverï¼Œforkï¼Œand contribute to over 420 million projects.
torchtune/recipes/configs/llama3_1 at mainÂ·pytorch/torchtuneï¼šPyTorch native post-training library. Contribute to pytorch/torchtune development by creating an account on GitHub.
Incorrect Default Config File Paths for Llama 3.1 8B and Qwen 2.5 7B ModelsÂ·Issue #2352Â·pytorch/torchtuneï¼šI've noticed an issue where the downloaded model directories for Llama 3.1 8B and Qwen 2.5 7B do not match the paths expected in their respective default config files. Llama 3.1 8B Issue The downl...
torchtune/recipes/configs/qwen2_5/7B_lora_single_device.yaml at a226a58b8c36db5afa123f0885c5337d1ebc91f6Â·pytorch/torchtuneï¼šPyTorch native post-training library. Contribute to pytorch/torchtune development by creating an account on GitHub.
Feature requestï¼šGRPO supportÂ·Issue #2340Â·pytorch/torchtuneï¼šAs you all might have already known by now DeepSeek-R1 with its GRPO training was quite successfulï¼Œshould we consider bringing GRPO into torchtune?
HF tokenizersï¼šinitial base tokenizer support by ebsmothersÂ·Pull Request #2350Â·pytorch/torchtuneï¼šFixes #2212This is an initial PR to support general tokenizers from Hugging Face via a tokenizer.json file. This is just a starting point to parse relevant JSON filesï¼Œinfer BOS and EOSï¼Œand defin...
Torchtune â–· #devï¼ˆ16 messagesğŸ”¥):
GitHub Checks on Full DPO Distributed PRï¼ŒGPU Testing Issuesï¼ŒRecipe Test Failuresï¼ŒVRAM Usage Optimization

GitHub Checks Fail on Full DPO PR: A user reported issues with GitHub checks on their Full DPO Distributed PR, with specific errors related to GPU and OOM issues.
The mentioned error was ValueError: ProcessGroupNCCL is only supported with GPUs, no GPUs found! and the user sought assistance from the community.
GPU Testing Issues Persist: There were discussions about re-running workflows after tests failed, and concerns were raised about running software tests on machines with insufficient GPU capacity.
One member mentioned that the tests seemed to fail due to running on a CPU runner instead of a GPU runner, exacerbating the OOM issue.
Recipe Tests Encounter Compilation Errors: Multiple failures were noted in the recipe tests, with one user indicating that issues arose from a bad PR that had previously merged.
Despite having two GPUs with 8GB VRAM each, users were surprised by OOM errors, prompting suggestions for optimizing resource usage.
Optimizing VRAM Usage for Tests: To mitigate OOM errors, one user suggested enabling activation checkpointing, activation offloading, and using a smaller batch size.
Another user confirmed testing showed a peak of ~4 GB VRAM usage per GPU with 2 GPUs, indicating a reasonable usage level.
Future Review of PR Commit: A user expressed hope that their latest commit in the PR would resolve existing issues.
Another member reassured them that they would review the PR again the following morning.
Links mentioned:
GitHub Â· Build and ship software on a single, collaborative platform: Join the world's most widely adopted, AI-powered developer platform where millions of developers, businesses, and the largest open source community build software that advances humanity.
Build software better, together: GitHub is where people build software. More than 150 million people use GitHub to discover, fork, and contribute to over 420 million projects.
torchtune/tests/recipes/test_full_dpo_distributed.py at add-feature-full-dpo Â· sam-pi/torchtune: PyTorch native post-training library. Contribute to sam-pi/torchtune development by creating an account on GitHub.
torchtune/tests/recipes/test_full_finetune_distributed.py at a226a58b8c36db5afa123f0885c5337d1ebc91f6 Â· pytorch/torchtune: PyTorch native post-training library. Contribute to pytorch/torchtune development by creating an account on GitHub.
Full DPO Distributed by sam-pi Â· Pull Request #2275 Â· pytorch/torchtune: ContextAdapted from the great work in #1966What is the purpose of this PR? Is it to add a new featurePlease link to any issues this PR addresses: relates to #2082ChangelogWhat are the chang...
Modular (Mojo ğŸ”¥) â–· #general (2 messages):
Mojo language development, 12/18 community meeting insights

GitHub æ£€æŸ¥åœ¨å®Œæ•´ DPO PR ä¸Šå¤±è´¥ï¼šä¸€ä½ç”¨æˆ·æŠ¥å‘Šè¯´ï¼Œå…¶å®Œæ•´ DPO åˆ†å¸ƒå¼ PR åœ¨ GitHub æ£€æŸ¥ä¸­å‡ºç°é—®é¢˜ï¼Œå…·ä½“é”™è¯¯ä¸ GPU å’Œå†…å­˜æº¢å‡ºï¼ˆOOMï¼‰é—®é¢˜ç›¸å…³ã€‚
æåˆ°çš„é”™è¯¯æ˜¯ `ValueErrorï¼šProcessGroupNCCL is only supported with GPUsï¼Œno GPUs found!`ï¼Œå¹¶å‘ç¤¾åŒºå¯»æ±‚å¸®åŠ©ã€‚

GPU æµ‹è¯•é—®é¢˜æŒç»­å­˜åœ¨ï¼šæœ‰äººè®¨è®ºäº†æµ‹è¯•å¤±è´¥åé‡æ–°è¿è¡Œå·¥ä½œæµç¨‹çš„é—®é¢˜ï¼Œå¹¶ä¸”æœ‰äººæ‹…å¿ƒåœ¨ GPU èµ„æºä¸è¶³çš„æœºå™¨ä¸Šè¿è¡Œè½¯ä»¶æµ‹è¯•ã€‚
ä¸€ä½æˆå‘˜æåˆ°ï¼Œæµ‹è¯•ä¼¼ä¹ç”±äºåœ¨ CPU è¿è¡Œå™¨ï¼ˆrunnerï¼‰è€Œä¸æ˜¯ GPU è¿è¡Œå™¨ï¼ˆrunnerï¼‰ä¸Šè¿è¡Œè€Œå¤±è´¥ï¼ŒåŠ å‰§äº†å†…å­˜æº¢å‡ºï¼ˆOOMï¼‰é—®é¢˜ã€‚

é…æ–¹æµ‹è¯•é‡åˆ°ç¼–è¯‘é”™è¯¯ï¼šé…æ–¹æµ‹è¯•ä¸­å‡ºç°äº†å¤šæ¬¡å¤±è´¥ã€‚ä¸€ä½ç”¨æˆ·æŒ‡å‡ºï¼Œé—®é¢˜æºäºä¹‹å‰åˆå¹¶çš„ä¸€ä¸ªæœ‰é—®é¢˜çš„ PRï¼ˆPull Requestï¼‰ã€‚
å°½ç®¡æ‹¥æœ‰ä¸¤ä¸ªå…·æœ‰ 8GB æ˜¾å­˜ï¼ˆVRAMï¼‰çš„ GPUï¼Œç”¨æˆ·ä»ç„¶å¯¹å†…å­˜æº¢å‡ºï¼ˆOOMï¼‰é”™è¯¯æ„Ÿåˆ°æƒŠè®¶ï¼Œå¹¶æå‡ºäº†ä¼˜åŒ–èµ„æºä½¿ç”¨çš„å»ºè®®ã€‚

ä¼˜åŒ–æµ‹è¯•çš„æ˜¾å­˜ï¼ˆVRAMï¼‰ä½¿ç”¨ï¼šä¸ºäº†ç¼“è§£å†…å­˜æº¢å‡ºï¼ˆOOMï¼‰é”™è¯¯ï¼Œä¸€ä½ç”¨æˆ·å»ºè®®å¯ç”¨æ¿€æ´»æ£€æŸ¥ç‚¹ï¼ˆactivation checkpointingï¼‰ã€æ¿€æ´»å¸è½½ï¼ˆactivation offloadingï¼‰ï¼Œå¹¶ä½¿ç”¨æ›´å°çš„æ‰¹é‡å¤§å°ã€‚
å¦ä¸€ä½ç”¨æˆ·ç¡®è®¤ï¼Œåœ¨ä½¿ç”¨äº† 2 ä¸ª GPU çš„æµ‹è¯•ä¸­ï¼Œæ¯ä¸ª GPU çš„æ˜¾å­˜ï¼ˆVRAMï¼‰å³°å€¼ä½¿ç”¨é‡çº¦ä¸º 4GBï¼Œè¡¨æ˜æ˜¾å­˜ï¼ˆVRAMï¼‰ä½¿ç”¨ç‡åœ¨ä¸€ä¸ªåˆç†çš„æ°´å¹³ã€‚

æœªæ¥å®¡æŸ¥ PRï¼ˆPull Requestï¼‰æäº¤ï¼šä¸€ä½ç”¨æˆ·è¡¨ç¤ºå¸Œæœ›ä»–ä»¬åœ¨ PRï¼ˆPull Requestï¼‰ä¸­çš„æœ€æ–°æäº¤èƒ½å¤Ÿè§£å†³ç°æœ‰é—®é¢˜ã€‚
å¦ä¸€ä½æˆå‘˜å‘ä»–ä»¬ä¿è¯ï¼Œä»–ä»¬å°†åœ¨ç¬¬äºŒå¤©æ—©ä¸Šå†æ¬¡å®¡æŸ¥ PRï¼ˆPull Requestï¼‰ã€‚

ç›¸å…³é“¾æ¥ï¼š

*  GitHubÂ·åœ¨ä¸€ä¸ªå•ä¸€çš„åä½œå¹³å°ä¸Šæ„å»ºå’Œäº¤ä»˜è½¯ä»¶ï¼šåŠ å…¥ä¸–ç•Œä¸Šä½¿ç”¨æœ€å¹¿æ³›ã€ç”± AI é©±åŠ¨çš„å¼€å‘è€…å¹³å°ã€‚æ•°ç™¾ä¸‡å¼€å‘è€…ã€ä¼ä¸šå’Œæœ€å¤§çš„å¼€æºç¤¾åŒºåœ¨æ­¤æ„å»ºæ¨åŠ¨äººç±»è¿›æ­¥çš„è½¯ä»¶ã€‚
*  æ›´å¥½åœ°å…±åŒæ„å»ºè½¯ä»¶ï¼šGitHub æ˜¯äººä»¬æ„å»ºè½¯ä»¶çš„åœ°æ–¹ã€‚è¶…è¿‡ 1.5 äº¿äººä½¿ç”¨ GitHub æ¥å‘ç°ã€fork å’Œè´¡çŒ®è¶…è¿‡ 4.2 äº¿ä¸ªé¡¹ç›®ã€‚
*  [torchtune/tests/recipes/test_full_dpo_distributed.py at add-feature-full-dpoÂ·sam-pi/torchtune](https://github.com/sam-pi/torchtune/blob/add-feature-full-dpo/tests/recipes/test_full_dpo_distributed.py)ï¼šPyTorch åŸç”Ÿåè®­ç»ƒåº“ã€‚
*  [torchtune/tests/recipes/test_full_finetune_distributed.py at a226a58b8c36db5afa123f0885c5337d1ebc91f6Â·pytorch/torchtune](https://github.com/pytorch/torchtune/blob/a226a58b8c36db5afa123f0885c5337d1ebc91f6/tests/recipes/test_full_finetune_distributed.py)ï¼šPyTorch åŸç”Ÿåè®­ç»ƒåº“ã€‚
*  [Full DPO Distributed by sam-piÂ·Pull Request #2275Â·pytorch/torchtune](https://github.com/pytorch/torchtune/pull/2275)ï¼šContextAdapted from the great work in #1966What is the purpose of this PR? Is it to add a new featurePlease link to any issues this PR addressesï¼šrelates to #2082ChangelogWhat are the chang...
*  Modularï¼ˆMojo ğŸ”¥ï¼‰â–· #generalï¼ˆ2 messages)ï¼šMojo è¯­è¨€å¼€å‘ï¼Œ12/18 ç¤¾åŒºä¼šè®®è§è§£

Mojo shifts away from Python Superset: In the recent 12/18 community meeting, it was clarified that Mojo is not currently a superset of Python.
The focus has now shifted towards leveraging Mojo's strengths in GPU and performance programming.
Chris provides insights on Mojo's future: Chris discussed the future direction of Mojo, stating that it wonâ€™t evolve into an entirely different language but will concentrate on its current capabilities.
This approach emphasizes enhancing Mojo's efficiency in its designed applications rather than broadening its language framework.
Link mentioned: Modular milestones: GPUs, 2024 reflections, and the road ahead ğŸš€: In this extra special community meeting, we reflected on 2024's progress and shared updates on:ğŸ§‘â€ğŸš€ MAX 24.6, featuring MAX GPU!ğŸ”¥ Our overall approach to M...

Mojo è½¬å˜æ–¹å‘ï¼šä¸å†æ˜¯ Python çš„è¶…é›†ï¼ˆsupersetï¼‰ã€‚åœ¨æœ€è¿‘ 12 æœˆ 18 æ—¥çš„ç¤¾åŒºä¼šè®®ä¸Šï¼Œæ˜ç¡®æŒ‡å‡º Mojo ç›®å‰å¹¶é Python çš„è¶…é›†ï¼ˆsupersetï¼‰ â€”â€” ä¹Ÿå°±æ˜¯è¯´ï¼ŒMojo å¹¶éå®Œå…¨å…¼å®¹ Pythonï¼Œè€Œæ˜¯åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ‰©å±•ã€‚
ç°åœ¨çš„é‡ç‚¹å·²ç»è½¬å‘å……åˆ†å‘æŒ¥ Mojo åœ¨ GPU å’Œé«˜æ€§èƒ½è®¡ç®—æ–¹é¢çš„ä¼˜åŠ¿ã€‚

Chris å±•æœ› Mojo çš„æœªæ¥ï¼š Chris åœ¨ä¼šè®®ä¸­è°ˆåˆ° Mojo çš„æœªæ¥å‘å±•æ–¹å‘ï¼Œè¡¨ç¤ºå®ƒä¸ä¼šå‘å±•æˆä¸€ç§å®Œå…¨ä¸åŒçš„è¯­è¨€ï¼Œè€Œæ˜¯ä¼šä¸“æ³¨äºå…¶å½“å‰å·²æœ‰çš„èƒ½åŠ›ã€‚
è¿™ç§ç­–ç•¥æ—¨åœ¨æå‡ Mojo åœ¨ç‰¹å®šåº”ç”¨åœºæ™¯ä¸‹çš„æ•ˆç‡ï¼Œè€Œéæ‰©å±•å…¶æ•´ä½“è¯­è¨€æ¡†æ¶ã€‚

ç›¸å…³é“¾æ¥ï¼šModular é‡Œç¨‹ç¢‘ï¼šGPUï¼Œ2024 å¹´å›é¡¾ä¸å±•æœ›ğŸš€ï¼š åœ¨è¿™æ¬¡ç‰¹åˆ«çš„ç¤¾åŒºä¼šè®®ä¸­ï¼Œå›é¡¾äº† 2024 å¹´çš„è¿›å±•ï¼Œå¹¶åˆ†äº«äº† MAX 24.6ï¼ˆåŒ…å« MAX GPUï¼‰ç­‰æ›´æ–°ã€‚æ›´å¤šå…³äº Modular çš„æ–¹æ³•è¯·å‚è€ƒé“¾æ¥ã€‚

Modular (Mojo ğŸ”¥) â–· #mojo (24 messagesğŸ”¥):
Parser Rewriting, Script Functionality, Mojo Open-Source Aspirations, UpdateDOM Function, Production Readiness of Mojo

Parser Needs Adjustment: A member noted the need to rewrite the parser for handling multiple slices of data, while weighing the costs of branching.
Branching may be cheaper than significant data transfers, making it a valid consideration for those not focusing on higher performance needs.
Creating Dynamic Scripts: The update_dom function was revised to create dynamic scripts by integrating all changes directly into the Script struct.
This change allows for returning a modified script copy using the transfer sigil (^), improving efficiency and structure.
Hopes for Mojo's Open-Source Future: A user expressed the desire for Mojo to adopt an open-source approach similar to Google's with Go, rather than frameworks like Swift.
This sentiment was echoed with references to the open-source development styles of other programming languages, emphasizing community involvement.
Building on Mojo's Foundation: Discussion emerged about the potential of what Modular could create with Mojo's open-source builds, likened to Unix's foundational impact.
Members expressed excitement about the possibilities and progress in Mojo's development, suggesting a significant evolution in the programming landscape.
Mojo's Production Readiness: A user inquired about Mojo's current status regarding production readiness, highlighting curiosity among the community.
Responses indicated enthusiasm for Mojo's development trajectory, underscoring its promising nature even if not fully realized yet.
Link mentioned: Link to class method in Python docstring: I want to add a link to a method in my class from within the docstring of another method of the same class. I want the link to work in Sphinx and preferentially also in Spyder and other Python IDEs...

æ¨¡å—åŒ–ï¼ˆMojo ğŸ”¥ï¼‰â–· #mojoï¼ˆ24 æ¡æ¶ˆæ¯ğŸ”¥):
è§£æå™¨é‡å†™ã€è„šæœ¬åŠŸèƒ½ã€Mojo å¼€æºæ„¿æ™¯ã€UpdateDOM å‡½æ•°ã€Mojo çš„ç”Ÿäº§å°±ç»ªç¨‹åº¦è§£æå™¨éœ€è¦è°ƒæ•´ï¼šæœ‰æˆå‘˜æŒ‡å‡ºï¼Œéœ€è¦é‡å†™è§£æå™¨ä»¥å¤„ç†å¤šä¸ªæ•°æ®åˆ‡ç‰‡ï¼Œå¹¶æƒè¡¡åˆ†æ”¯çš„æˆæœ¬ã€‚
å¯¹äºé‚£äº›ä¸è¿½æ±‚æè‡´æ€§èƒ½çš„ç”¨æˆ·æ¥è¯´ï¼Œåˆ†æ”¯çš„æˆæœ¬å¯èƒ½ä½äºå¤§é‡æ•°æ®ä¼ è¾“ï¼Œå› æ­¤æ˜¯ä¸€ä¸ªå€¼å¾—è€ƒè™‘çš„æ–¹æ¡ˆã€‚
åˆ›å»ºåŠ¨æ€è„šæœ¬ï¼šå¯¹ `update_dom` å‡½æ•°è¿›è¡Œäº†ä¿®æ”¹ï¼Œé€šè¿‡å°†æ‰€æœ‰æ›´æ”¹ç›´æ¥æ•´åˆåˆ° `Script` ç»“æ„ä¸­æ¥åˆ›å»ºåŠ¨æ€è„šæœ¬ã€‚
æ­¤ä¿®æ”¹å…è®¸ä½¿ç”¨ä¼ è¾“æ ‡è®°ï¼ˆ^/transfer sigilï¼‰è¿”å›ä¿®æ”¹åçš„è„šæœ¬å‰¯æœ¬ï¼Œä»è€Œæé«˜æ•ˆç‡å’Œä»£ç ç»“æ„ã€‚
å¯¹ Mojo å¼€æºæœªæ¥çš„æœŸæœ›ï¼šæœ‰ç”¨æˆ·è¡¨è¾¾äº†å¯¹ Mojo é‡‡å–ç±»ä¼¼ Google åœ¨ Go è¯­è¨€ä¸Šä½¿ç”¨çš„å¼€æºç­–ç•¥çš„æœŸæœ›ï¼Œè€Œä¸æ˜¯åƒ Swift è¿™æ ·çš„æ¡†æ¶æ¨¡å¼ã€‚
è¿™ç§è§‚ç‚¹ä¸å…¶ä»–ç¼–ç¨‹è¯­è¨€çš„å¼€æºå¼€å‘æ¨¡å¼ç›¸å‘¼åº”ï¼Œå¼ºè°ƒç¤¾åŒºå‚ä¸çš„é‡è¦æ€§ã€‚
åŸºäº Mojo æ„å»ºï¼šè®¨è®ºé›†ä¸­åœ¨ Modular å¯ä»¥é€šè¿‡ Mojo çš„å¼€æºç‰ˆæœ¬æ„å»ºå‡ºä½•ç§æ½œåŠ›ï¼Œå¹¶å°†å…¶ä¸ Unix çš„å¥ åŸºæ€§å½±å“ç›¸æå¹¶è®ºã€‚
æˆå‘˜ä»¬å¯¹ Mojo å¼€å‘çš„å¯èƒ½æ€§å’Œè¿›å±•æ„Ÿåˆ°å…´å¥‹ï¼Œè®¤ä¸ºå®ƒé¢„ç¤ºç€ç¼–ç¨‹é¢†åŸŸçš„ä¸€æ¬¡é‡å¤§å˜é©ã€‚
Mojo çš„ç”Ÿäº§å°±ç»ªç¨‹åº¦ï¼šæœ‰ç”¨æˆ·è¯¢é—®äº† Mojo ç›®å‰çš„ç”Ÿäº§å°±ç»ªç¨‹åº¦ï¼Œåæ˜ äº†ç¤¾åŒºå¯¹è¿™ä¸ªé—®é¢˜çš„å…³æ³¨ã€‚
ç¤¾åŒºçš„å›åº”è¡¨æ˜å¯¹ Mojo çš„å‘å±•å……æ»¡çƒ­æƒ…ï¼Œå¹¶å¼ºè°ƒå³ä½¿å°šæœªå®Œå…¨æˆç†Ÿï¼ŒMojo ä¾ç„¶å…·æœ‰å¹¿é˜”å‰æ™¯ã€‚
ç›¸å…³é“¾æ¥ï¼šå…³äº Python æ–‡æ¡£å­—ç¬¦ä¸²ä¸­ç±»æ–¹æ³•é“¾æ¥çš„é—®é¢˜

Modular (Mojo ğŸ”¥) â–· #max (16 messagesğŸ”¥):
MAX Serve CLI, OpenAI Completion API Issues, OpenAI Model Compatibility, Msty client for local models

Discussion on CLI for MAX Serve: Members discussed the possibility of building a CLI similar to ollama on top of MAX Serve. It was mentioned that MAX Serve can already handle many functionalities offered by Ollama with a docker container.
Specific features like local model running were highlighted, with a hope for better performance compared to Ollama.
Reporting OpenAI API Issues: A user raised concerns about missing features in the OpenAI completions API with max serve (v24.6), such as generation stopping at specified tokens. They were encouraged to file issues on the GitHub repo to highlight these missing elements.
Discussion ensued on how to report incidents, with a recommendation for multiple smaller issues for easier tracking and resolution.
Msty Client for Easier Access: Bridging the conversation, a member introduced Msty, an OpenAI-compatible client that simplifies local model interactions compared to using Docker and other complex setups. Highlighting its ease of use and features, it was noted as a potential solution for accessing AI models seamlessly.
The importance of offline usability and privacy with Msty was emphasized, suggesting it is highly favorable for users who wish to avoid complex configurations.
Tracking OpenAI API Compatibility Issues: The group acknowledged ongoing issues with OpenAI API compatibility, particularly referencing the v1/models endpoint. Several GitHub issues were highlighted to illustrate specific missing functionalities like token stopping and prompt handling.
The members expressed gratitude for the clarity, with developers indicating those issues will be communicated to internal teams for future improvements.
Links mentioned:
[Feature Request] OpenAI API Compatibility: Models endpoint is missing Â· Issue #294 Â· modular/max: What is your request? max serve (v24.6) openai api endpoint is missing the Models endpoint (https://platform.openai.com/docs/api-reference/models). The example below will return a 404: client = Ope...
modular/max: A collection of sample programs, notebooks, and tools which highlight the power of the MAX Platform - modular/max
Msty - Using AI Models made Simple and Easy: AI beyond just plain chat. Private, Offline, Split chats, Branching, Concurrent chats, Web Search, RAG, Prompts Library, Vapor Mode, and more. Perfect LM Studio, Jan AI, and Perplexity alternative. Us...
Modular: Use MAX with Open WebUI for RAG and Web Search: Learn how quickly MAX and Open WebUI get you up-and-running with RAG, web search, and Llama 3.1 on GPU
[Feature Request] OpenAI API Compatibility: Only the first element of a list of prompts is considered during generation Â· Issue #293 Â· modular/max: What is your request? Invoking max serve (v24.6) text generation via openai api endpoint with a list of prompts produces text generated only for the first element in a prompt. This behavior applies...
[Feature Request] OpenAI API Compatibility: Text Generation Does not stop at specified `stop` argument. Â· Issue #292 Â· modular/max: What is your request? Invoking max serve (v24.6) text generation via openai api endpoint, I encountered cases where text generation is not terminated when the token specified via the stop argument ...
Latent Space â–· #ai-general-chat (36 messagesğŸ”¥):
Hibiki translation model, Melanie Mitchell's AI perspectives, Mistral AI's Le Chat, OpenAI's o3-mini updates, PDF parsing advancements

Modularï¼ˆMojo ğŸ”¥ï¼‰â–· #maxï¼ˆ16 æ¡æ¶ˆæ¯ğŸ”¥):
MAX Serve å‘½ä»¤è¡Œç•Œé¢ï¼ˆCLIï¼‰ã€OpenAI Completion API é—®é¢˜ã€OpenAI æ¨¡å‹å…¼å®¹æ€§ã€ç”¨äºæœ¬åœ°æ¨¡å‹çš„ Msty å®¢æˆ·ç«¯å…³äº MAX Serve çš„å‘½ä»¤è¡Œç•Œé¢ï¼ˆCLIï¼‰çš„è®¨è®ºï¼šæœ‰æˆå‘˜æè®®åœ¨ MAX Serve åŸºç¡€ä¸Šæ„å»ºä¸€ä¸ªç±»ä¼¼ Ollama çš„å‘½ä»¤è¡Œç•Œé¢ï¼ˆCLIï¼‰ã€‚æœ‰äººæŒ‡å‡ºï¼ŒMAX Serve å€ŸåŠ© Docker å®¹å™¨å·²ç»èƒ½å¤Ÿå®ç° Ollama çš„è®¸å¤šåŠŸèƒ½ã€‚
å¤§å®¶ç‰¹åˆ«å…³æ³¨æœ¬åœ°æ¨¡å‹è¿è¡Œç­‰åŠŸèƒ½ï¼Œå¹¶æœŸæœ› MAX Serve èƒ½å¤Ÿæä¾›æ¯” Ollama æ›´å¥½çš„æ€§èƒ½ã€‚
æŠ¥å‘Š OpenAI API é—®é¢˜ï¼šæœ‰ç”¨æˆ·åæ˜  max serveï¼ˆv24.6ï¼‰ä¸­çš„ OpenAI Completion API ç¼ºå°‘æŸäº›åŠŸèƒ½ï¼Œä¾‹å¦‚æ— æ³•åœ¨æŒ‡å®šçš„ Token å¤„åœæ­¢ç”Ÿæˆã€‚å¤§å®¶éƒ½å»ºè®®ä»–ä»¬åœ¨ GitHub ä»“åº“ä¸Šæäº¤ issueï¼Œä»¥ä¾¿çªå‡ºè¿™äº›ç¼ºå¤±çš„åŠŸèƒ½ã€‚
éšåï¼Œå¤§å®¶è®¨è®ºäº†å¦‚ä½•æŠ¥å‘Šé—®é¢˜ï¼Œå»ºè®®å°†é—®é¢˜æ‹†åˆ†æˆå¤šä¸ªå°çš„ issueï¼Œè¿™æ ·æ›´ä¾¿äºè·Ÿè¸ªå’Œè§£å†³ã€‚
Msty å®¢æˆ·ç«¯ï¼šä¸ºäº†æ–¹ä¾¿å¤§å®¶ä½¿ç”¨ï¼Œæœ‰æˆå‘˜æ¨èäº† Mstyã€‚å®ƒæ˜¯ä¸€ä¸ªä¸ OpenAI å…¼å®¹çš„å®¢æˆ·ç«¯ï¼Œç›¸æ¯”ä½¿ç”¨ Docker ç­‰å¤æ‚é…ç½®ï¼ŒMsty å¯ä»¥ç®€åŒ–æœ¬åœ°æ¨¡å‹çš„ä½¿ç”¨ã€‚Msty çš„æ˜“ç”¨æ€§å’Œä¸°å¯ŒåŠŸèƒ½ä½¿å…¶æˆä¸ºä¸€ä¸ªæ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´ä¾¿æ·åœ°ä½¿ç”¨ AI æ¨¡å‹ã€‚
Msty çš„ç¦»çº¿å¯ç”¨æ€§å’Œéšç§æ€§ä¹Ÿå—åˆ°äº†å¼ºè°ƒï¼Œå¯¹äºé‚£äº›ä¸å¸Œæœ›è¿›è¡Œå¤æ‚é…ç½®çš„ç”¨æˆ·æ¥è¯´ï¼ŒMsty æ˜¯ä¸€ä¸ªéå¸¸ä¸é”™çš„é€‰æ‹©ã€‚
è·Ÿè¸ª OpenAI API å…¼å®¹æ€§é—®é¢˜ï¼šå¤§å®¶ç¡®è®¤äº† OpenAI API å…¼å®¹æ€§æ–¹é¢å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯ v1/models è¿™ä¸ªç«¯ç‚¹ã€‚ä¸€äº› GitHub issue è¢«åˆ—å‡ºï¼Œç”¨äºè¯´æ˜å…·ä½“ç¼ºå¤±çš„åŠŸèƒ½ï¼Œä¾‹å¦‚ Token åœæ­¢å’Œ Prompt å¤„ç†ç­‰é—®é¢˜ã€‚
å‚ä¸è€…ä»¬å¯¹ä¿¡æ¯çš„æ¸…æ™°ç¨‹åº¦è¡¨ç¤ºæ„Ÿè°¢ï¼Œå¼€å‘äººå‘˜ä¹Ÿè¡¨ç¤ºä¼šå°†è¿™äº›é—®é¢˜åé¦ˆç»™å†…éƒ¨å›¢é˜Ÿï¼Œä»¥ä¾¿åç»­æ”¹è¿›ã€‚
æåˆ°çš„é“¾æ¥:
[Feature Request] OpenAI API Compatibilityï¼šModels endpoint is missingÂ·Issue #294Â·modular/maxï¼šWhat is your request? max serveï¼ˆv24.6ï¼‰openai api endpoint is missing the Models endpointï¼ˆhttps://platform.openai.com/docs/api-reference/models). The example below will return a 404ï¼šclient = Ope...
modular/maxï¼šA collection of sample programsï¼Œnotebooksï¼Œand tools which highlight the power of the MAX Platform - modular/max
Msty - Using AI Models made Simple and Easyï¼šAI beyond just plain chat. Privateï¼ŒOfflineï¼ŒSplit chatsï¼ŒBranchingï¼ŒConcurrent chatsï¼ŒWeb Searchï¼ŒRAGï¼ŒPrompts Libraryï¼ŒVapor Modeï¼Œand more. Perfect LM Studioï¼ŒJan AIï¼Œand Perplexity alternative. Us...
Modularï¼šUse MAX with Open WebUI for RAG and Web Searchï¼šLearn how quickly MAX and Open WebUI get you up-and-running with RAGï¼Œweb searchï¼Œand Llama 3.1 on GPU
[Feature Request] OpenAI API Compatibilityï¼šOnly the first element of a list of prompts is considered during generationÂ·Issue #293Â·modular/maxï¼šWhat is your request? Invoking max serveï¼ˆv24.6ï¼‰text generation via openai api endpoint with a list of prompts produces text generated only for the first element in a prompt. This behavior applies...
[Feature Request] OpenAI API Compatibilityï¼šText Generation Does not stop at specified `stop` argument.Â·Issue #292Â·modular/maxï¼šWhat is your request? Invoking max serveï¼ˆv24.6ï¼‰text generation via openai api endpointï¼ŒI encountered cases where text generation is not terminated when the token specified via the stop argument ...
Latent Space â–· #ai-general-chatï¼ˆ36 æ¡æ¶ˆæ¯ğŸ”¥):
Hibiki ç¿»è¯‘æ¨¡å‹ï¼ŒMelanie Mitchell å…³äº AI çš„è§‚ç‚¹ï¼ŒMistral AI çš„ Le Chatï¼ŒOpenAI çš„ o3-mini æ›´æ–°ï¼ŒPDF è§£ææŠ€æœ¯çš„è¿›å±•

Meet Hibiki: Real-time Translation Champion: Hibiki, the latest simultaneous speech-to-speech translation model from kyutai, supports real-time translations from ğŸ‡«ğŸ‡· to ğŸ‡¬ğŸ‡§, preserving the speaker's voice and adjusting pace based on context.
Reports indicate Hibiki outperforms prior systems in quality, naturalness, and speaker similarity, nearing human interpreter capabilities.
Melanie Mitchell Draws Attention on AI: @mmitchell_ai released a critical piece discussing why Fully Autonomous Agents should not be developed, highlighting ethical considerations and dissecting the concept of AI Agents (paper).
The conversation reflects varied views on her work, with some noting her balanced perspective amid ongoing debates in the AI community.
Unveiling Mistral AI's Le Chat: Mistral AI announced the launch of Le Chat, described as the ultimate AI sidekick for both personal and professional tasks, now available on web and mobile.
This new tool aims to enhance user experience in daily activities, potentially changing how people interact with AI at work and in life.
Updated Features in OpenAI's o3-mini: OpenAI shared updates on the chain of thought processes integrated into the o3-mini, available for users, enhancing capabilities for both free and paid subscribers.
These enhancements aim to improve performance and user experience, demonstrating OpenAI's commitment to evolving its services.
Advancements in PDF Parsing Technology: @deedydas remarked that PDF parsing is effectively solved at scale, noting that Gemini 2 Flash offers parsing abilities for large documents at a minimal cost of $1 per 6000 tokens.
This breakthrough illustrates the growing efficiency in processing complex documents, opening new avenues for applications requiring high-quality text extraction.
Links mentioned:
Tweet from kyutai (@kyutai_labs): Meet Hibiki, our simultaneous speech-to-speech translation model, currently supporting ğŸ‡«ğŸ‡·â¡ï¸ğŸ‡¬ğŸ‡§.Hibiki produces spoken and text translations of the input speech in real-time, while preserving the sp...
Tweet from Mistral AI (@MistralAI): Introducing the all new Le Chat: your ultimate AI sidekick for life and work! Now live on web and mobile!
AI: A Guide for Thinking Humans | Melanie Mitchell | Substack: I write about interesting new developments in AI. Click to read AI: A Guide for Thinking Humans, by Melanie Mitchell, a Substack publication with tens of thousands of subscribers.
On the â€œARC-AGIâ€ $1 Million Reasoning Challenge: In this post Iâ€™m going to go into the weeds, describing how some people are trying to win a big $$$ prize for solving a still-wide-open AI challenge, the â€œAbstraction and Reasoning Corpus,â€ and what i...
Tweet from MMitchell (@mmitchell_ai): New piece out!We explain why Fully Autonomous Agents Should Not be Developed, breaking â€œAI Agentâ€ down into its components & examining through ethical values.https://huggingface.co/papers/2502.02649Wi...
Tweet from Neil Zeghidour (@neilzegh): Today we release Hibiki, real-time speech translation that runs on your phone. Adaptive flow without fancy policy, simple temperature sampling of a multistream audio-text LM. Very proud of @tom_labiau...
Tweet from Deedy (@deedydas): PDF parsing is pretty much solved at scale now.Gemini 2 Flash's $0.40/M tokens and 1M token context means you can now parse 6000 long PDFs at near perfect quality for $1
The LLM Reasoning Debate Heats Up : Three recent papers examine the robustness of reasoning and problem-solving in large language models
Tweet from OpenAI (@OpenAI): Updated chain of thought in OpenAI o3-mini for free and paid users, and in o3-mini-high for paid users.
LlamaIndex â–· #blog (2 messages):
Gemini 2.0 availability, LlamaParse for financial documents

Meet Hibikiï¼šå®æ—¶ç¿»è¯‘å† å†›ï¼šHibikiï¼Œæ¥è‡ª kyutai çš„æœ€æ–°åŒå£°è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘æ¨¡å‹ï¼Œæ”¯æŒä»ğŸ‡«ğŸ‡·åˆ°ğŸ‡¬ğŸ‡§çš„å®æ—¶ç¿»è¯‘ï¼Œä¿ç•™è¯´è¯è€…çš„å£°éŸ³å¹¶æ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´èŠ‚å¥ã€‚
æŠ¥å‘Šè¡¨æ˜ï¼ŒHibiki åœ¨è´¨é‡ã€è‡ªç„¶åº¦å’Œè¯´è¯è€…ç›¸ä¼¼æ€§æ–¹é¢ä¼˜äºå…ˆå‰çš„ç³»ç»Ÿï¼Œæ¥è¿‘äººç±»å£è¯‘å‘˜çš„èƒ½åŠ›ã€‚
Melanie Mitchell è¯„è®º AIï¼š@mmitchell_ai å‘å¸ƒäº†ä¸€ç¯‡é‡è¦çš„æ–‡ç« ï¼Œè®¨è®ºäº†ä¸ºä»€ä¹ˆä¸åº”è¯¥å¼€å‘å®Œå…¨è‡ªä¸»çš„ AI æ™ºèƒ½ä½“ï¼ˆAI Agentï¼‰ï¼Œå¼ºè°ƒäº†ä¼¦ç†æ–¹é¢çš„è€ƒè™‘ï¼Œå¹¶å‰–æäº† AI æ™ºèƒ½ä½“çš„æ¦‚å¿µï¼ˆè®ºæ–‡ï¼‰ã€‚
å¯¹è¯åæ˜ äº†å¯¹å¥¹çš„å·¥ä½œçš„ä¸åŒçœ‹æ³•ï¼Œä¸€äº›äººæ³¨æ„åˆ°å¥¹åœ¨ AI ç¤¾åŒºæŒç»­è¾©è®ºä¸­çš„å¹³è¡¡è§†è§’ã€‚
æ­å¹• Mistral AI çš„ Le Chatï¼šMistral AI å®£å¸ƒæ¨å‡º Le Chatï¼Œè¢«æè¿°ä¸ºä¸ªäººå’Œä¸“ä¸šä»»åŠ¡çš„ç»ˆæ AI åŠ©æ‰‹ï¼Œç°åœ¨å¯åœ¨ç½‘é¡µå’Œç§»åŠ¨è®¾å¤‡ä¸Šä½¿ç”¨ã€‚
è¿™ä¸ªæ–°å·¥å…·æ—¨åœ¨æå‡ç”¨æˆ·åœ¨æ—¥å¸¸æ´»åŠ¨ä¸­çš„ä½“éªŒï¼Œå¯èƒ½ä¼šæ”¹å˜äººä»¬åœ¨å·¥ä½œå’Œç”Ÿæ´»ä¸­ä¸ AI äº¤äº’çš„æ–¹å¼ã€‚
OpenAI çš„ o3-mini ä¸­çš„æ›´æ–°åŠŸèƒ½ï¼šOpenAI åˆ†äº«äº†é›†æˆåˆ° o3-mini ä¸­çš„æ€ç»´é“¾ï¼ˆChain of Thoughtï¼‰çš„æ›´æ–°ï¼Œä¾›ç”¨æˆ·ä½¿ç”¨ï¼Œå¢å¼ºäº†å…è´¹å’Œä»˜è´¹ç”¨æˆ·çš„èƒ½åŠ›ã€‚
è¿™äº›å¢å¼ºæ—¨åœ¨æé«˜æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒï¼Œå±•ç¤ºäº† OpenAI å¯¹å‘å±•å…¶æœåŠ¡çš„æ‰¿è¯ºã€‚
PDF è§£ææŠ€æœ¯æ–¹é¢çš„è¿›æ­¥ï¼š@deedydas è¯„è®ºè¯´ï¼ŒPDF è§£æåœ¨è§„æ¨¡ä¸Šå¾—åˆ°äº†æœ‰æ•ˆçš„è§£å†³ï¼Œå¹¶æŒ‡å‡º Gemini 2 Flash ä»¥æ¯ 6000 ä¸ª Tokenï¼ˆTokenï¼‰1 ç¾å…ƒçš„æœ€ä½æˆæœ¬æä¾›å¤§å‹æ–‡æ¡£çš„è§£æèƒ½åŠ›ã€‚
è¿™ä¸€çªç ´è¯´æ˜äº†å¤„ç†å¤æ‚æ–‡æ¡£çš„æ•ˆç‡æ—¥ç›Šæé«˜ï¼Œä¸ºéœ€è¦é«˜è´¨é‡æ–‡æœ¬æå–çš„åº”ç”¨ç¨‹åºå¼€è¾Ÿäº†æ–°é€”å¾„ã€‚
æåˆ°çš„é“¾æ¥ï¼š
æ¥è‡ª kyutaiï¼ˆ@kyutai_labsï¼‰çš„æ¨æ–‡ï¼šè®¤è¯† Hibikiï¼Œæˆ‘ä»¬çš„åŒå£°è¯­éŸ³åˆ°è¯­éŸ³ç¿»è¯‘æ¨¡å‹ï¼Œç›®å‰æ”¯æŒ ğŸ‡«ğŸ‡·â¡ï¸ğŸ‡¬ğŸ‡§ã€‚Hibiki å®æ—¶ç”Ÿæˆè¾“å…¥è¯­éŸ³çš„å£è¯­å’Œæ–‡æœ¬ç¿»è¯‘ï¼ŒåŒæ—¶ä¿ç•™ sp...
æ¥è‡ª Mistral AIï¼ˆ@MistralAIï¼‰çš„æ¨æ–‡ï¼šä»‹ç»å…¨æ–°çš„ Le Chatï¼šæ‚¨ç”Ÿæ´»å’Œå·¥ä½œçš„ç»ˆæ AI åŠ©æ‰‹ï¼ç°åœ¨åœ¨ç½‘é¡µå’Œç§»åŠ¨è®¾å¤‡ä¸Šç›´æ’­ï¼
AIï¼šä¸ºæ€è€ƒè€…å‡†å¤‡çš„ AI æŒ‡å— | Melanie Mitchell | Substackï¼šæˆ‘å†™å…³äº AI ä¸­æœ‰è¶£çš„æ–°å‘å±•ã€‚ç‚¹å‡»é˜…è¯» AIï¼šä¸ºæ€è€ƒè€…å‡†å¤‡çš„ AI æŒ‡å—ï¼Œä½œè€… Melanie Mitchellï¼Œä¸€ä»½æ‹¥æœ‰æ•°ä¸‡è®¢é˜…è€…çš„ Substack å‡ºç‰ˆç‰©ã€‚
å…³äºã€ŒARC-AGIã€100 ä¸‡ç¾å…ƒæ¨ç†æŒ‘æˆ˜èµ›ï¼šåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†æ·±å…¥æ¢è®¨ï¼Œæè¿°ä¸€äº›äººå¦‚ä½•è¯•å›¾èµ¢å¾—å·¨é¢ $$$ å¥–é‡‘ï¼Œä»¥è§£å†³ä¸€ä¸ªä»ç„¶å¼€æ”¾çš„ AI æŒ‘æˆ˜ï¼Œã€ŒæŠ½è±¡å’Œæ¨ç†è¯­æ–™åº“ã€ï¼Œä»¥åŠæˆ‘...
æ¥è‡ª MMitchellï¼ˆ@mmitchell_aiï¼‰çš„æ¨æ–‡ï¼šæ–°æ–‡ç« å‘å¸ƒï¼æˆ‘ä»¬è§£é‡Šäº†ä¸ºä»€ä¹ˆä¸åº”è¯¥å¼€å‘å®Œå…¨è‡ªä¸»çš„ AI æ™ºèƒ½ä½“ï¼Œå°†ã€ŒAI æ™ºèƒ½ä½“ã€åˆ†è§£ä¸ºå„ä¸ªç»„æˆéƒ¨åˆ†ï¼Œå¹¶é€šè¿‡ä¼¦ç†ä»·å€¼è§‚è¿›è¡Œæ£€éªŒã€‚https://huggingface.co/papers/2502.02649Wi...
æ¥è‡ª Neil Zeghidourï¼ˆ@neilzeghï¼‰çš„æ¨æ–‡ï¼šä»Šå¤©æˆ‘ä»¬å‘å¸ƒ Hibikiï¼Œå¯åœ¨æ‚¨çš„æ‰‹æœºä¸Šè¿è¡Œçš„å®æ—¶è¯­éŸ³ç¿»è¯‘ã€‚æ— éœ€èŠ±å“¨ç­–ç•¥çš„è‡ªé€‚åº”æµç¨‹ï¼Œå¤šæµéŸ³é¢‘æ–‡æœ¬ LM çš„ç®€å•æ¸©åº¦é‡‡æ ·ã€‚ä¸º @tom_labiau æ„Ÿåˆ°éå¸¸è‡ªè±ª...
æ¥è‡ª Deedyï¼ˆ@deedydasï¼‰çš„æ¨æ–‡ï¼šPDF è§£æç°åœ¨å‡ ä¹å¯ä»¥åœ¨è§„æ¨¡ä¸Šè§£å†³ã€‚Gemini 2 Flash çš„ 0.40 ç¾å…ƒ / ç™¾ä¸‡ Token å’Œ 100 ä¸‡ Token ä¸Šä¸‹æ–‡æ„å‘³ç€æ‚¨ç°åœ¨å¯ä»¥ä»¥æ¥è¿‘å®Œç¾çš„è´¨é‡è§£æ 6000 ä¸ªé•¿ PDFï¼Œä»·æ ¼ä¸º 1 ç¾å…ƒå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM/Large Language Modelï¼‰çš„æ¨ç†èƒ½åŠ›è¾©è®ºæ„ˆæ¼”æ„ˆçƒˆï¼šä¸‰ç¯‡æœ€æ–°è®ºæ–‡æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­æ¨ç†å’Œé—®é¢˜è§£å†³çš„ç¨³å¥æ€§æ¥è‡ª OpenAIï¼ˆ@OpenAIï¼‰çš„æ¨æ–‡ï¼šæ›´æ–°äº† OpenAI o3-mini ä¸­å…è´¹å’Œä»˜è´¹ç”¨æˆ·çš„æ€ç»´é“¾ï¼Œä»¥åŠä»˜è´¹ç”¨æˆ·çš„ o3-mini-high ä¸­çš„æ€ç»´é“¾ã€‚
LlamaIndex â–· #blogï¼ˆ2 æ¡æ¶ˆæ¯)ï¼š
Gemini 2.0 å¯ç”¨æ€§ï¼ŒLlamaParse ç”¨äºé‡‘èæ–‡æ¡£

Gemini 2.0 Launches with Day 0 Support: Gemini 2.0 from @google is now generally available, providing day 0 support and impressive benchmarks. Developers can install the latest integration package with pip install llama-index-llms-gemini and check out the announcement blog post.
The updated 2.0 Flash is available to all users in the Gemini app on desktop and mobile, promoting collaboration with Gemini's enhanced features and low latency capabilities.
LlamaParse Simplifies Financial Document Parsing: Hanane D showcased how to tackle parsing complex financial documents accurately and cost-effectively using LlamaParse's 'Auto' mode on LinkedIn. She leverages @OpenAI embeddings and advanced LlamaParse capabilities for effective parsing of charts and tables, as shared in this link.
Her demonstration highlights the advancements in parsing technology, making it easier for users to extract relevant insights from intricate data.
Links mentioned:
Gemini 2.0 is now available to everyone: Weâ€™re announcing new updates to Gemini 2.0 Flash, plus introducing Gemini 2.0 Flash-Lite and Gemini 2.0 Pro Experimental.
no title found: no description found
LlamaIndex â–· #general (4 messages):
Embedding Print Removal, Pull Request Suggestion, Documentation Clarity

Gemini 2.0 å‘å¸ƒï¼Œå…·å¤‡é¦–æ—¥æ”¯æŒï¼šæ¥è‡ª @google çš„ Gemini 2.0 ç°å·²å…¨é¢æ¨å‡ºï¼Œæä¾›é¦–æ—¥æ”¯æŒå’Œä»¤äººå°è±¡æ·±åˆ»çš„æ€§èƒ½åŸºå‡†ã€‚å¼€å‘è€…å¯ä»¥é€šè¿‡ `pip install llama-index-llms-gemini` å‘½ä»¤å®‰è£…æœ€æ–°çš„é›†æˆåŒ…ï¼ˆintegration packageï¼‰ï¼Œå¹¶æŸ¥çœ‹ç›¸å…³å…¬å‘Šåšå®¢æ–‡ç« ã€‚
æ›´æ–°åçš„ 2.0 Flash ç‰ˆæœ¬ç°å·²åœ¨æ¡Œé¢å’Œç§»åŠ¨è®¾å¤‡ä¸Šçš„ Gemini åº”ç”¨ä¸­å‘æ‰€æœ‰ç”¨æˆ·å¼€æ”¾ï¼Œå®ƒå‡­å€Ÿ Gemini å¢å¼ºçš„åŠŸèƒ½å’Œä½å»¶è¿Ÿæ€§èƒ½ï¼Œèƒ½å¤Ÿæœ‰æ•ˆä¿ƒè¿›åä½œã€‚
LlamaParse ç®€åŒ–è´¢åŠ¡æ–‡æ¡£è§£æï¼šHanane D åœ¨ LinkedIn ä¸Šå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ LlamaParse çš„ã€Œè‡ªåŠ¨ã€(Autoï¼‰æ¨¡å¼ï¼Œç»æµé«˜æ•ˆä¸”å‡†ç¡®åœ°è§£æå¤æ‚çš„è´¢åŠ¡æ–‡æ¡£ã€‚å¥¹åˆ©ç”¨ @OpenAI çš„åµŒå…¥ï¼ˆembeddingï¼‰å’Œ LlamaParse çš„é«˜çº§åŠŸèƒ½ï¼Œæœ‰æ•ˆåœ°è§£æå›¾è¡¨ï¼ˆchartï¼‰å’Œè¡¨æ ¼ï¼ˆtableï¼‰ï¼Œå…·ä½“ä¿¡æ¯è¯·å‚è€ƒä»¥ä¸‹é“¾æ¥ã€‚
å¥¹çš„æ¼”ç¤ºç€é‡å±•ç¤ºäº†è§£ææŠ€æœ¯çš„è¿›æ­¥ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿæ›´è½»æ¾åœ°ä»å¤æ‚æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„è§è§£ã€‚

ç›¸å…³é“¾æ¥ï¼š
Gemini 2.0 ç°å·²å‘æ‰€æœ‰äººå¼€æ”¾ï¼šæˆ‘ä»¬åœ¨æ­¤å®£å¸ƒ Gemini 2.0 Flash çš„æœ€æ–°æ›´æ–°ï¼ŒåŒæ—¶æ¨å‡º Gemini 2.0 Flash-Lite å’Œ Gemini 2.0 Pro Experimentalã€‚
æœªæ‰¾åˆ°æ ‡é¢˜ï¼šæš‚æ— ç›¸å…³æè¿°
LlamaIndex â–· #generalï¼ˆ4 æ¡æ¶ˆæ¯)ï¼š
ç§»é™¤åµŒå…¥æ‰“å°ï¼Œæ‹‰å–è¯·æ±‚å»ºè®®ï¼Œæ–‡æ¡£ä¼˜åŒ–

Request to Delete Embedding Print: A member requested to delete the embedding print from the documentation as it takes up excessive space and affects readability.
They linked to a GitHub issue highlighting the documentation issue and suggested that it should be removed for better clarity.
Suggestion for Pull Request: Another member acknowledged the request and offered to create a Pull Request (PR) to address the embedding print removal.
They indicated willingness to handle it if the original requester did not want to proceed with the PR.
Links mentioned:
[Documentation]: Postgres Vector Store Â· Issue #17735 Â· run-llama/llama_index: Documentation Issue Description Embeddings print takes up excessive space and affects readability. To improve clarity and docs usability, the embedding print should be removed Documentation Link ht...
Postgres Vector Store - LlamaIndex: no description found
MLOps @Chipro â–· #general-ml (6 messages):
LLMs in Classification, Latency Requirements in ML, Composite Pipeline for Noisy Data

è¯·æ±‚ç§»é™¤ Embedding çš„æ‰“å°ä¿¡æ¯ï¼šä¸€ä½æˆå‘˜è¯·æ±‚ä»æ–‡æ¡£ä¸­ç§»é™¤ Embedding çš„æ‰“å°ä¿¡æ¯ï¼Œå› ä¸ºè¿™äº›ä¿¡æ¯å ç”¨äº†è¿‡å¤šç©ºé—´ï¼Œå½±å“äº†æ–‡æ¡£çš„å¯è¯»æ€§ã€‚
è¯¥æˆå‘˜æä¾›äº†ä¸€ä¸ª GitHub issue çš„é“¾æ¥ï¼Œå…¶ä¸­è¯¦ç»†æè¿°äº†æ–‡æ¡£å­˜åœ¨çš„é—®é¢˜ï¼Œå¹¶å»ºè®®ç§»é™¤ Embedding çš„æ‰“å°ä¿¡æ¯ï¼Œä»¥æé«˜æ–‡æ¡£çš„æ¸…æ™°åº¦ã€‚
å…³äºæäº¤ Pull Request çš„å»ºè®®ï¼šå¦ä¸€ä½æˆå‘˜è¡¨ç¤ºå·²ç»äº†è§£è¯¥è¯·æ±‚ï¼Œå¹¶æå‡ºåˆ›å»ºä¸€ä¸ª Pull Requestï¼ˆPRï¼‰æ¥è§£å†³ç§»é™¤ Embedding æ‰“å°ä¿¡æ¯çš„é—®é¢˜ã€‚
ä»–ä»¬è¡¨ç¤ºï¼Œå¦‚æœæœ€åˆæå‡ºè¯·æ±‚çš„æˆå‘˜ä¸å¸Œæœ›äº²è‡ªæäº¤ PRï¼Œä»–ä»¬æ„¿æ„ä»£ä¸ºå¤„ç†ã€‚
ç›¸å…³é“¾æ¥ï¼š
[æ–‡æ¡£]ï¼šPostgres Vector StoreÂ·Issue #17735Â·run-llama/llama_indexï¼šæ–‡æ¡£é—®é¢˜æè¿°ï¼šEmbedding çš„æ‰“å°ä¿¡æ¯å ç”¨äº†è¿‡å¤šç©ºé—´ï¼Œå½±å“äº†å¯è¯»æ€§ã€‚ä¸ºäº†æé«˜æ–‡æ¡£çš„æ¸…æ™°åº¦å’Œå¯ç”¨æ€§ï¼Œåº”è¯¥ç§»é™¤ Embedding çš„æ‰“å°ä¿¡æ¯ã€‚æ–‡æ¡£é“¾æ¥ ht...
Postgres Vector Store - LlamaIndexï¼šæœªæ‰¾åˆ°æè¿°
MLOps @Chipro â–· #general-mlï¼ˆ6 messages)ï¼š
åˆ†ç±»ä¸­çš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLM/Large Language Modelï¼‰ï¼ŒML ä¸­çš„å»¶è¿Ÿè¦æ±‚ï¼Œå™ªå£°æ•°æ®çš„å¤åˆç®¡é“

LLMs excel in classification but struggle with noise: A member emphasized that while LLMs are effective for classification, noisy data demands additional techniques like dense embeddings and autoencoder rerankers to enhance performance.
This indicates that a more complex approach may be necessary when dealing with challenging data environments.
Latency concerns when using LLMs: Discussion revealed that while LLMs can classify well, it may be impractical to use them in scenarios with strict latency requirements due to their processing limits.
The conversation concluded that the suitability of LLMs really hinges on the specific latency constraints of a given application.
Reframing business requirements for ML solutions: A member mentioned that there was a missed opportunity in properly framing the business requirements when transitioning to an ML problem.
They noted that it should have been apparent from the start that if low-latency is critical, traditional LLMs might not be the best fit.
Cohere â–· #api-discussions (6 messages):
Fine-tuning Error, System Design Interview Questions

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ“…é•¿åˆ†ç±»ï¼Œä½†å®¹æ˜“å—åˆ°å™ªå£°æ•°æ®çš„å½±å“ï¼šä¸€ä½æˆå‘˜å¼ºè°ƒï¼Œè™½ç„¶å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åˆ†ç±»æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†å™ªå£°æ•°æ®éœ€è¦é¢å¤–çš„æŠ€æœ¯ï¼Œä¾‹å¦‚å¯†é›†åµŒå…¥ï¼ˆdense embeddingsï¼‰å’Œè‡ªç¼–ç å™¨é‡æ’åºå™¨ï¼ˆautoencoder rerankersï¼‰ç­‰ï¼Œæ¥æé«˜æ€§èƒ½ã€‚
ä¾‹å¦‚ï¼Œå¯ä»¥å…ˆé€šè¿‡å¯†é›†å‘é‡è¡¨å¾æ•°æ®ï¼Œç„¶åä½¿ç”¨è‡ªç¼–ç å™¨å¯¹ç»“æœè¿›è¡Œé‡æ’åºï¼Œä»¥è¿‡æ»¤æ‰å™ªå£°çš„å¹²æ‰°ã€‚
è¿™æ„å‘³ç€åœ¨å¤æ‚çš„æ•°æ®ç¯å¢ƒä¸‹ï¼Œå¯èƒ½éœ€è¦é‡‡ç”¨æ›´å¤æ‚çš„æ–¹æ³•ã€‚
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å»¶è¿Ÿé—®é¢˜ï¼šè®¨è®ºè¡¨æ˜ï¼Œè™½ç„¶å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯ä»¥å¾ˆå¥½åœ°è¿›è¡Œåˆ†ç±»ï¼Œä½†ç”±äºå…¶å¤„ç†é€Ÿåº¦çš„é™åˆ¶ï¼Œåœ¨å¯¹å»¶è¿Ÿæœ‰ä¸¥æ ¼è¦æ±‚çš„åœºæ™¯ä¸­ï¼Œä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯èƒ½ä¸å¤ªç°å®ã€‚
æ€»çš„æ¥è¯´ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦é€‚ç”¨ï¼Œä¸»è¦å–å†³äºå…·ä½“åº”ç”¨çš„å»¶è¿Ÿè¦æ±‚ã€‚
é‡æ–°å®šä¹‰æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰è§£å†³æ–¹æ¡ˆçš„ä¸šåŠ¡éœ€æ±‚ï¼šä¸€ä½æˆå‘˜æåˆ°ï¼Œåœ¨å°†é—®é¢˜è½¬åŒ–ä¸ºæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰è§£å†³æ–¹æ¡ˆæ—¶ï¼Œé”™å¤±äº†ä¸€ä¸ªæ°å½“å®šä¹‰ä¸šåŠ¡éœ€æ±‚çš„æœºä¼šã€‚
ä»–ä»¬æŒ‡å‡ºï¼Œä¸€å¼€å§‹å°±åº”è¯¥æ„è¯†åˆ°ï¼Œå¦‚æœå¯¹å»¶è¿Ÿæœ‰ä¸¥æ ¼è¦æ±‚ï¼Œä¼ ç»Ÿçš„å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å¯èƒ½å¹¶éæœ€ä½³æ–¹æ¡ˆã€‚
Cohere â–· #api-discussionsï¼ˆ6 messages)ï¼š
å¾®è°ƒé”™è¯¯ã€ç³»ç»Ÿè®¾è®¡é¢è¯•é—®é¢˜

Fine-tuning Error and Batch Size Limits: A user reported a BadRequestError (Status code: 400) indicating that the current training configuration exceeds the maximum of 250 training steps, with a limit on batch size set to 16.
Concerns were raised about whether this means a limit of 4000 examples for fine-tuning, as a member noted this limitation wasn't present before.
Inquiry on AIML System Design Questions: A member asked if anyone has system design interview questions specific to AI/ML.
Another member acknowledged the inquiry and directed it for collection, signaling collaboration amongst teams.
Gorilla LLM (Berkeley Function Calling) â–· #discussion (4 messages):
Tool-using model system prompts, Hugging Face dataset transformation issues, Dataset file format mismatch

å¾®è°ƒé”™è¯¯ä¸æ‰¹æ¬¡å¤§å°é™åˆ¶ï¼š æœ‰ç”¨æˆ·æŠ¥å‘Šäº†ä¸€ä¸ª `BadRequestError` é”™è¯¯ï¼ˆçŠ¶æ€ç ï¼š400ï¼‰ï¼Œé”™è¯¯ä¿¡æ¯è¡¨æ˜å½“å‰çš„è®­ç»ƒé…ç½®è¶…è¿‡äº†æœ€å¤§è®­ç»ƒæ­¥æ•°é™åˆ¶ï¼Œæœ€å¤šå…è®¸ 250 æ­¥ï¼ŒåŒæ—¶æ‰¹æ¬¡å¤§å°è¢«é™åˆ¶ä¸º 16ã€‚
æœ‰ç”¨æˆ·æ‹…å¿ƒè¿™æ˜¯å¦æ„å‘³ç€å¾®è°ƒçš„è®­ç»ƒæ ·æœ¬æ•°é‡è¢«é™åˆ¶ä¸º 4000 ä¸ªï¼Œå› ä¸ºä¹‹å‰å¹¶æ²¡æœ‰è¿™ä¸ªé™åˆ¶ã€‚
å…³äº AI/ML ç³»ç»Ÿè®¾è®¡é—®é¢˜çš„å’¨è¯¢ï¼š æœ‰æˆå‘˜å’¨è¯¢æ˜¯å¦æœ‰é’ˆå¯¹äººå·¥æ™ºèƒ½ / æœºå™¨å­¦ä¹ ï¼ˆAI/MLï¼‰çš„ç³»ç»Ÿè®¾è®¡é¢è¯•é¢˜ã€‚
å¦ä¸€ä½æˆå‘˜ç¡®è®¤æ”¶åˆ°äº†è¯¥é—®é¢˜ï¼Œå¹¶å°†å…¶è½¬äº¤ç»™ç›¸å…³å›¢é˜Ÿè¿›è¡Œæ•´ç†ï¼Œè¿™è¡¨æ˜å›¢é˜Ÿä¹‹é—´æ­£åœ¨è¿›è¡Œåä½œã€‚
Gorilla å¤§è¯­è¨€æ¨¡å‹ï¼ˆGorilla LLMï¼‰(Berkeley Function Callingï¼‰â–· #discussionï¼ˆ4 messages):
å·¥å…·ä½¿ç”¨æ¨¡å‹çš„ç³»ç»Ÿæç¤ºï¼ŒHugging Face æ•°æ®é›†è½¬æ¢é—®é¢˜ï¼Œä»¥åŠæ•°æ®é›†æ–‡ä»¶æ ¼å¼ä¸åŒ¹é…ç­‰é—®é¢˜ã€‚

Need for canonical system prompts: A member inquired about the canonical system prompts for fine-tuned tool-using models to ensure they return responses or JSON for function calls.
They noted that the Gorilla paper did not include the system prompt used, creating a gap in available resources.
Experimenting with Hugging Face datasets: A member expressed a desire to run experiments more easily by transforming data and utilizing datasets.map on Hugging Face.
This indicates a push towards enhancing the functionality and accessibility of datasets for experimentation.
Dataset format issue with Hugging Face: A member pointed out that the dataset is in .json format, but its content is actually in jsonl format, which has caused issues with Hugging Face.
They suggested changing the file suffix to .jsonl and modifying the dataset config files to potentially resolve the issue.
DSPy â–· #papers (1 messages):
batmanosama: https://arxiv.org/abs/2502.02508

å¯¹äºè§„èŒƒç³»ç»Ÿæç¤ºçš„éœ€æ±‚ï¼šä¸€ä½æˆå‘˜è¯¢é—®äº†å…³äºå¾®è°ƒåçš„å·¥å…·è°ƒç”¨æ¨¡å‹çš„è§„èŒƒç³»ç»Ÿæç¤ºï¼Œä»¥ç¡®ä¿å®ƒä»¬è¿”å›å“åº”æˆ–ç”¨äºå‡½æ•°è°ƒç”¨çš„ JSON æ ¼å¼æ•°æ®ã€‚
ä»–ä»¬æŒ‡å‡ºï¼ŒGorilla è®ºæ–‡æ²¡æœ‰åŒ…å«æ‰€ä½¿ç”¨çš„ç³»ç»Ÿæç¤ºï¼Œè¿™å¯¼è‡´äº†ç¼ºå°‘å¯ç”¨çš„èµ„æºã€‚
åˆ©ç”¨ Hugging Face æ•°æ®é›†è¿›è¡Œå®éªŒï¼šä¸€ä½æˆå‘˜è¡¨ç¤ºå¸Œæœ›é€šè¿‡è½¬æ¢æ•°æ®å¹¶åœ¨ Hugging Face ä¸Šä½¿ç”¨ datasets.mapï¼Œæ¥æ›´è½»æ¾åœ°è¿è¡Œå®éªŒã€‚
è¿™è¡¨æ˜å¤§å®¶å¸Œæœ›å¢å¼ºæ•°æ®é›†çš„åŠŸèƒ½å’Œå¯è®¿é—®æ€§ã€‚è¿™æ ·å¯ä»¥æ›´æ–¹ä¾¿åœ°è¿›è¡Œå®éªŒã€‚
Hugging Face çš„æ•°æ®é›†æ ¼å¼é—®é¢˜ï¼šä¸€ä½æˆå‘˜æŒ‡å‡ºï¼Œæ•°æ®é›†æ˜¯ .json æ ¼å¼ï¼Œä½†å…¶å†…å®¹å®é™…ä¸Šæ˜¯ jsonl æ ¼å¼ï¼Œè¿™å¯¼è‡´ Hugging Face å‡ºç°äº†é—®é¢˜ã€‚
ä»–ä»¬å»ºè®®å°†æ–‡ä»¶åç¼€æ›´æ”¹ä¸º .jsonl å¹¶ä¿®æ”¹æ•°æ®é›†é…ç½®æ–‡ä»¶ï¼Œä»¥æœŸè§£å†³è¯¥é—®é¢˜ã€‚
DSPy â–· #papersï¼ˆ1 æ¡æ¶ˆæ¯)ï¼š
batmanosamaï¼šhttps://arxiv.org/abs/2502.02508

DSPy â–· #examples (2 messages):
Git Repository, Colab Notebook

Inquiry about Git Repo: A member asked if there is a Git repo available for their work.
The inquiry suggests interest in accessing code or resources related to the project.
Colab Notebook Shared: A member provided a link to a Colab notebook in response to the Git repo query.
This notebook is likely related to the discussion and can be accessed by signing in.
Link mentioned: Google Colab: no description found

DSPy â–· #examplesï¼ˆ2 æ¡æ¶ˆæ¯):
Git ä»“åº“ï¼ŒColab Notebook

å…³äº Git ä»“åº“çš„è¯¢é—®ï¼šä¸€ä½æˆå‘˜è¯¢é—®æ˜¯å¦æœ‰å¯ç”¨äºå…¶å·¥ä½œçš„ Git ä»“åº“ã€‚
è¯¥è¯¢é—®è¡¨æ˜å…¶å¯¹è®¿é—®ä¸è¯¥é¡¹ç›®ç›¸å…³çš„ä»£ç æˆ–èµ„æºæ„Ÿå…´è¶£ã€‚
Colab Notebook åˆ†äº«ï¼šä¸€ä½æˆå‘˜åˆ†äº«äº†ä¸€ä¸ª Colab notebook çš„é“¾æ¥ï¼Œä»¥å›åº”å…³äº Git ä»“åº“çš„è¯¢é—®ã€‚
è¿™ä¸ª notebook å¾ˆå¯èƒ½ä¸è®¨è®ºå†…å®¹ç›¸å…³ï¼Œç”¨æˆ·éœ€è¦ç™»å½•åæ‰èƒ½è®¿é—®ã€‚
é“¾æ¥æåˆ°ï¼šGoogle Colabï¼šæœªæ‰¾åˆ°ç›¸å…³æè¿°ã€‚