## 20250319Inside-Googles-Co-Scientist-Copyright-Office-Weighs-Generated-Works

[Inside Google’s Co-Scientist, Copyright Office Weighs Generated Works, Multilingual (and Good at All of Them), Diffusion for Materials Design](https://info.deeplearning.ai/inside-googles-co-scientist-copyright-office-weighs-generated-works-multilingual-and-good-at-all-of-them-diffusion-for-materials-design-1?ecid=ACsprvvFjTnC8PmyWg1ra5vJonYqme6y6MuIAP_pQadCwNYeYmzH0_aAgde-cZGiJgD4q0ZYz4tQ&utm_campaign=The%20Batch&utm_medium=email&_hsenc=p2ANqtz--647tA4LppfK-hx2FneOK6bqpqq_lYiETGBYDnN4PoKASXOOljcJ4fJqYtD4xtbMPjW7EUeVJC9WawKuj-xOYXgbHGuQ&_hsmi=352686041&utm_content=352667058&utm_source=hs_email)

Dear friends,

Last Friday on Pi Day，we held AI Dev 25，a new conference for AI Developers. Tickets had（unfortunately）sold out days after we announced their availability，but I came away energized by the day of coding and technical discussions with fellow AI Builders! Let me share here my observations from the event.

上周五，也就是圆周率日（Pi Day，3 月 14 日），我们举办了 AI Dev 25 大会，这是一个专门为 AI 开发者们准备的全新会议。门票在发布后几天内就销售一空（非常遗憾），但我和其他 AI 开发者们一起度过了充满代码和技术讨论的一天，这让我备受鼓舞！在此，我想和大家分享我对本次活动的一些观察。

What a great group of people at AI Dev 25. Also… look what my fortune cookie from the event said!

AI Dev 25 的与会者们都非常出色。另外…… 看看我在活动中拿到的幸运饼干上写了什么！

I'd decided to start AI Dev because while there're great academic AI conferences that disseminate research work（such as NeurIPS，ICML and ICLR）and also great meetings held by individual companies，often focused on each company's product offerings，there were few vendor-neutral conferences for AI developers. With the wide range of AI tools now available，there is a rich set of opportunities for developers to build new things（and to share ideas on how to build things!），but also a need for a neutral forum that helps developers do so.

我决定创办 AI Dev，是因为目前已经有一些优秀的学术 AI 会议用于传播研究成果，例如 NeurIPS、ICML 和 ICLR 等。也有不少公司会举办会议，但通常聚焦于自家公司的产品。鲜有面向 AI 开发者的、并且保持供应商中立的会议。如今，各种各样的 AI 工具层出不穷，这为开发者们创造了大量机会来构建新应用，并分享相关的经验和想法。因此，我们需要一个中立的平台来帮助开发者们实现这些目标。

Based on an informal poll，about half the attendees had traveled to San Francisco from outside the Bay Area for this meeting，including many who had come from overseas. I was thrilled by the enthusiasm to be part of this AI Builder community.To everyone who came，thank you!

根据一项非正式的调查，大约一半的参会者为了本次会议，从湾区以外，甚至是海外远道而来旧金山。能够亲身参与到 AI Builder 社区（AI Builder community）中，我感到非常激动！感谢每一位的光临！

Other aspects of the event that struck me:

关于本次活动，其他让我印象深刻的方面：

1 First，agentic AI continues to be a strong theme. The topic attendees most wanted to hear about（based on free text responses to our in-person survey at the start of the event）was agents!

首先，智能体持续成为一个强有力的主题。根据活动开始时我们现场调查的自由文本回复，智能体是与会者最感兴趣的话题！

2 Google's Paige Bailey talked about embedding AI in everything and using a wide range of models to do so. I also particularly enjoyed her demos of Astra and Deep Research agents.

谷歌的 Paige Bailey 谈到了将 AI 融入到各种应用中，并利用丰富的模型来实现这一目标。我还特别喜欢她对 Astra 和 Deep Research AI 智能体（AI Agent）的演示。

3 Meta's Amit Sangani talked compellingly as usual about open models. Specifically，he described developers fine-tuning smaller models on specific data，resulting in superior performance than with large general purpose models. While there're still many companies using fine-tuning that should really just be prompting，I'm also seeing continued growth of fine-tuning in applications that are reaching scale and that are becoming valuable.

Meta 的 Amit Sangani 像往常一样，生动地阐述了开放模型的重要性。他提到，开发者通过在特定数据上微调较小模型，可以获得比大型通用模型更优秀的性能。虽然现在还有不少公司在使用微调，但实际上他们更应该直接使用提示（prompting）。同时，我也观察到，在那些已经达到一定规模并具有商业价值的应用中，微调技术正在持续发展。

4 Many speakers also spoke about the importance of being pragmatic about what problems we are solving，as opposed to buying into the AGI hype. For example，Nebius' Rowan Charmin put it simply：Focusing on solving real problems is important!

许多发言者还强调了务实的重要性，即关注我们实际要解决的问题，而不是被通用人工智能（AGI）的炒作所迷惑。例如，Nebius 的 Rowan Charmin 简洁地指出：专注于解决实际问题，这才是最重要的！

5 Lastly，I was excited to hear continued enthusiasm for the Voice Stack. Justin Uberti gave a talk about OpenAI's realtime audio API to a packed room，with many people pulling out laptops to try things out themselves in code!

最后，我很高兴听到大家对语音栈（Voice Stack）持续的热情。Justin Uberti 在一个座无虚席的房间里，发表了关于 OpenAI 的实时音频 API 的演讲，现场很多人都迫不及待地拿出笔记本电脑，希望通过代码亲自体验这项技术！

DeepLearning.AI has a strong「Learner First」mentality; our foremost goal is always to help learners. I was thrilled that a few attendees told me they enjoyed how technical the sessions were，and said they learned many things that they're sure they will use.（In fact，I，too，came away with a few ideas from the sessions!）I was also struck that，both during the talks and at the technical demo booths，the rooms were packed with attendees who were highly engaged throughout the whole day. I'm glad that we were able to have a meeting filled with technical and engineering discussions.

DeepLearning.AI 秉持着「学习者至上」的理念；我们始终将帮助学习者作为首要目标。让我感到欣慰的是，一些参会者表示他们非常喜欢本次大会的技术深度，并表示从中学习到了许多他们确信能够运用的知识（事实上，我也从大会中获得了一些新的启发！）。演讲环节和技术演示区域都挤满了参会者，他们全天都积极参与其中，这让我印象深刻。很高兴我们能举办这样一场充满技术和工程交流的盛会。

I'm delighted that AI Dev 25 went off so well，and am grateful to all the attendees，volunteers，speakers，sponsors，partners，and team members that made the event possible. I regretted only that the physical size of the event space prevented us from admitting more attendees this time. There is something magical about bringing people together physically to share ideas，make friends，and to learn from and help each other. I hope we'll be able to bring even more people together in the future.

AI Dev 25 圆满落幕，我真是太高兴了！ 感谢每一位参与者，包括志愿者、演讲嘉宾、赞助商、合作伙伴，以及我们团队的成员，因为有了大家，这次活动才能如此成功。唯一有点遗憾的是，场地大小限制了参会人数，没能让更多人参与。但将大家聚在一起，面对面地交流想法、结交朋友、互相学习和帮助，这种感觉真的很棒，仿佛有一种魔力。希望未来能有机会让更多人加入我们！

Keep building!

Andrew

保持构建！

Andrew

### News

#### Equally Fluent in Many Languages

精通多国语言

Multilingual AI models often suffer uneven performance across languages，especially in multimodal tasks. A pair of lean models counters this trend with consistent understanding of text and images across major languages.

多语言 AI 模型（Multilingual AI models）在处理不同语言时，性能往往参差不齐，尤其是在多模态任务中。而一种精简的双子模型，凭借其对主流语言文本和图像的稳定理解能力，有效缓解了这一问题。

What's new：A team at Cohere led by Saurabh Dash released Aya Vision，a family of multilingual vision-language models with downloadable weights in 8 billion- and 32-billion-parameter sizes.

最新进展：Cohere 公司的 Saurabh Dash 带领的团队发布了 Aya Vision，这是一个多语种视觉语言模型家族，其模型权重已经开放下载，拥有 80 亿和 320 亿两种参数规模。

[Aya Vision: Expanding the worlds AI can see](https://cohere.com/blog/aya-vision?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=352686041&_hsenc=p2ANqtz-_FlXKJY5AlDtqjQ2VumVNfNz8brnGODpHz72xcJyu9JTL_v-y7x_k1sFJL_T4sUNgSgpvnwPiq6wRnA-fH2Sh-vrcAyA)

[C4AI Aya Vision - a CohereForAI Collection](https://huggingface.co/collections/CohereForAI/c4ai-aya-vision-67c4ccd395ca064308ee1484?ref=cohere-ai.ghost.io)

[unsloth/aya-vision-32b-unsloth-bnb-4bit at main](https://huggingface.co/unsloth/aya-vision-32b-unsloth-bnb-4bit/tree/main)

1 Input/output：Text and images in（up to 2,197 image tokens，up to 16,000 tokens total），text out（up to 4,000 tokens).

输入 / 输出：支持文本和图像输入（图像最多 2,197 个图像 Token（Image Token），总计最多 16,000 个 Token），并输出文本（最多 4,000 个 Token）。

2 Availability：Free via WhatsApp or Cohere Playground. Weights available to download，but licensed only for noncommercial uses.

可用性：可以通过 WhatsApp 或 Cohere Playground 免费使用。模型权重可以下载，但仅限于非商业用途。

3 Features：Multilingual input and output in 23 languages.

特性：支持 23 种语言的多语言输入和输出。

4 Undisclosed：Knowledge cutoff，training datasets，adapter architecture.

未公开信息：知识截止时间、训练数据集、以及适配器架构等信息。

How it works：Each model comprises a pretrained large language model（Aya Expanse for the 32B model，C4AI Command R7B for the 8B version），a pretrained vision encoder（SigLIP 2），and a vision-language adapter（「connector」）of unspecified architecture.

工作原理：每个模型都包含一个预训练的大语言模型（LLM/Large Language Model)（32B 模型使用 Aya Expanse，8B 版本使用 C4AI Command R7B）、一个预训练的视觉编码器（SigLIP 2）以及一个架构未知的视觉 - 语言适配器（连接器）。

1 To establish basic vision-language understanding，the team froze the vision encoder and language model and trained the vision-language connector.

为了让模型具备基本的视觉 - 语言理解能力，研究团队「冻结」了视觉编码器和语言模型（Language Model），也就是固定住它们的参数不再更新，然后训练视觉 - 语言连接器。

2 They fine-tuned the vision-language connector and language model on multimodal tasks. To build the fine-tuning dataset，they generated synthetic annotations for various English-language datasets and translated a large amount of data into a variety of languages. They rephrased the translations to add fluency and variety，particularly for languages with little real-world data，by matching generated pairs with the original synthetic samples.

接着，他们在多模态任务上对视觉 - 语言连接器和语言模型进行微调。为了构建用于微调的数据集，他们先为各种英文数据集生成了「合成注释」，就像是给图片加上一些自动生成的描述。然后，他们将大量数据翻译成多种语言。为了提高翻译质量，特别是对于那些缺乏真实世界数据的语言，他们会通过将生成的翻译与原始的英文合成样本进行比对，来润色这些翻译，使其更流畅、更自然。

3 They merged the language model with the fine-tuned vision-language model using an undisclosed method that preserved text capabilities while adding vision understanding.

研究者们采用了一种未公开的方法，将语言模型与经过微调的视觉 - 语言模型融合。这种方法在保留原有文本处理能力的同时，也赋予了模型理解图像信息的能力。

4 After proving this method for 8 billion parameters，they scaled up the recipe to 32 billion parameters.

在验证了该方法适用于 80 亿参数的模型后，他们将其扩展到了 320 亿参数。

Performance：To test the model，the team built and released two benchmarks：m-WildVision，a multilingual version of Wild Vision Bench's arena-style competition for discussion of images，and AyaVisionBench，135 image-question pairs in each language that cover nine tasks including captioning images，understanding charts，recognizing characters in images，visual reasoning，and converting screenshots to code. On these two benchmarks，Aya Vision 8B and 32B outperformed larger competitors，as judged by Claude 3.7 Sonnet.

性能：为了测试模型，研究团队构建并发布了两个基准测试：m-WildVision，这是一个多语言版本的 Wild Vision Bench，用于进行竞技场式的图像讨论；以及 AyaVisionBench，它包含了每种语言 135 个图像 - 问题对，涵盖了九个任务，包括图像描述（image captioning）、理解图表、识别图像中的字符、视觉推理以及将屏幕截图转换为代码。在这两个基准测试中，根据 Claude 3.7 Sonnet 的评估，Aya Vision 8B 和 32B 的表现优于更大的竞争对手。

1 In head-to-head competitions on AyaVisionBench，Aya Vision 8B won up to 79 percent of the time against six competitors of similar size. On m-WildVision，it achieved 81 percent when compared to vision-language models of similar size including Qwen2.5-VL 7B，Pixtral 12B，Gemini Flash 1.5 8B，and Llama-3.2 11B Vision. Aya Vision 8B won 63 percent of the time against Llama-3.2 90B Vision，a model more than 10 times its size.

在 AyaVisionBench 评测中，Aya Vision 8B 在与六个规模相近的模型的直接对比中，胜率高达 79%。在 m-WildVision 评测中，与规模相近的视觉 - 语言模型（Vision-Language Model）相比，例如 Qwen2.5-VL 7B、Pixtral 12B、Gemini Flash 1.5 8B 和 Llama-3.2 11B Vision，Aya Vision 8B 的胜率达到了 81%。更有甚者，Aya Vision 8B 在 63% 的情况下战胜了 Llama-3.2 90B Vision，这是一个参数规模是其 10 倍以上的模型。

2 On both benchmarks，Aya Vision 32B outperformed vision-language models more than twice its size including Llama-3.2 90B Vision，Molmo 72B，and Qwen2.5-VL 72B. On AyaVisionBench，it won between 50 and 64 percent of the time. On WildVision，it achieved win rates between 52 percent and 72 percent across all languages.

在两个基准测试中，Aya Vision 32B 的表现都超过了比它大两倍以上的视觉语言模型（vision-language models），包括 Llama-3.2 90B Vision、Molmo 72B 和 Qwen2.5-VL 72B。在 AyaVisionBench 上，Aya Vision 32B 大约能赢得 50% 到 64% 的测试。在 WildVision 上，它在所有语言中都取得了 52% 到 72% 左右的胜率。

Behind the news：Aya Vision builds on the Cohere-led Aya initiative，a noncommercial effort to build models that perform consistently well in all languages，especially languages that lack high-quality training data. The project started with a multilingual text model（Aya Expanse），added vision（Aya Vision），and plans to eventually add video and audio.

新闻解读：Aya Vision 的构建基于由 Cohere 牵头的 Aya 计划。这是一项非商业项目，旨在开发在各种语言中都能稳定运行的模型，特别是那些缺乏高质量训练数据的语言。该项目最初的多语言文本模型为 Aya Expanse，之后加入了视觉能力（Aya Vision），并计划最终支持视频和音频。

Why it matters：Multilingual vision-language models often perform less well in low-resource languages，and the gap widens when they process media other than text. Aya Vision's recipe for augmenting synthetic data with successively refined translations may contribute to more universally capable models. Aya Vision is available on the global messaging platform WhatsApp，where it can be used to translate text and images in all 23 of its current languages.

Why it matters：多语言视觉 - 语言模型在处理低资源语言时，性能通常会下降；如果处理的不是文本，而是图像等其他媒体，性能差距会进一步扩大。Aya Vision 采用了一种方案：通过不断改进的翻译来扩充合成数据，这可能有助于构建能力更强的通用模型。Aya Vision 现已上线全球通讯平台 WhatsApp，支持所有 23 种语言的文本和图像翻译。

We're thinking：Multilingual vision models could soon help non-native speakers decipher Turkish road signs，Finnish legal contracts，and Korean receipts. We look forward to a world in which understanding any scene or document is as effortless in Swahili as it is in English.

我们设想：多语言视觉模型将在不久的将来，帮助母语非土耳其语的人士看懂土耳其语的路标，帮助理解芬兰语的法律合同，以及识别韩语的收据。我们期待这样一个未来：理解任何场景或文档，无论使用斯瓦希里语还是英语，都能一样轻松自如。

#### Science Research Proposals Made to Order

科学研究提案

An AI agent synthesizes novel scientific research hypotheses. It's already making an impact in biomedicine.

按需生成一个 AI 智能体（AI Agent）能够创造全新的科学研究假设，并且已经在生物医学领域崭露头角。

What's new：Google introduced AI co-scientist，a general multi-agent system designed to generate in-depth research proposals within constraints specified by the user. The team generated and evaluated proposals for repurposing drugs，identifying drug targets，and explaining antimicrobial resistance in real-world laboratories. It's available to research organizations on a limited basis.

新进展：Google 推出了 AI 协同科学家，这是一种通用型多 AI 智能体系统，旨在用户指定的约束条件下，生成深入的研究提案。该团队在真实实验室中生成并评估了关于药物再利用、药物靶点识别以及抗菌素耐药性解释的提案。目前，该系统以有限的方式向研究机构提供。

[Accelerating scientific breakthroughs with an AI co-scientist](https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=352686041&_hsenc=p2ANqtz-_FlXKJY5AlDtqjQ2VumVNfNz8brnGODpHz72xcJyu9JTL_v-y7x_k1sFJL_T4sUNgSgpvnwPiq6wRnA-fH2Sh-vrcAyA)

How it works：AI co-scientist accepts a text description of a research goal，including relevant constraints or ideas. In response，it generates research proposals and reviews，ranks，and improves them using seven agents based on Google's Gemini 2.0 family of large language models. The completed proposals include sections that explain background，unmet needs，a proposed solution，goals，hypotheses，reasoning，study steps，and relevant articles. The agents take feedback and outputs from other agents to perform their prompted task simultaneously.

它是如何工作的：AI 辅助科研工具接受包含相关约束或想法的研究目标的文本描述。作为回应，该 AI 系统会生成研究提案和评审意见，并利用七个基于 Google Gemini 2.0 系列大语言模型的 AI 智能体对提案进行排序和改进。完整的提案包含多个部分，分别阐述背景、未被满足的需求、建议的解决方案、目标、假设、推理、研究步骤以及相关文献。这些 AI 智能体接收来自其他智能体的反馈和输出，从而并行执行各自的任务。

1 The supervisor agent periodically determines how often to run the other six agents，how important their output is，and whether the system is finished. To accomplish this，it computes statistics that represent the number of proposals generated so far，how many have been reviewed，and so on.

监督者智能体（Supervisor agent）周期性地决定其他六个智能体运行的频率、它们输出的重要性，以及系统是否完成任务。为了实现这一点，它会计算一些统计数据，比如目前生成的提议数量、已审核的提议数量等等。

2 The generation agent generates a list of proposals. It searches the web for relevant research articles，identifies testable assumptions，and debates with itself to improve ambiguous statements and adhere to constraints.

生成式 AI 智能体（generation agent）生成一份提议列表。它会在网络上搜索相关的研究论文，识别其中可以验证的假设，并通过自我辩论来改进含义模糊的语句，并确保符合既定约束。

3 The reflection agent filters the generated proposals according to correctness，quality，safety，and novelty. First，it reviews a proposal without web search and discards obviously bad proposals. Then it reviews each proposal against literature it finds online. It breaks down and checks the proposal's assumptions，checks whether the proposal might explain some observations in previous work，and simulates the proposed experiment（via text generation，similar to how a person performs a thought experiment).

反思智能体（reflection agent）会根据正确性、质量、新颖性和安全性等标准，对生成的提议进行筛选。首先，它会在不进行网络搜索的情况下初步审查提议，并剔除那些明显不合理的提议。然后，它会结合在线检索到的文献，进一步审查每个提议，具体包括：剖析并检验提议中的各项假设；验证该提议是否能够解释先前研究中的某些观测结果；以及模拟提出的实验（通过生成文本的方式，类似于人们进行思维实验）。

4 The proximity agents compute similarity between proposals to avoid redundancy.

为了避免提案内容重复，相似度 AI 智能体会计算提案之间的相似性。

5 The ranking agent determines the best proposals according to a tournament. It examines one pair of proposals at a time（including reviews from the reflection agent）and debates itself to pick the better one. To save computation，it prioritizes comparing similar proposals，new proposals，and highest-ranking proposals.

排序 AI 智能体会通过类似竞赛的方式来确定最佳提案。它每次会审查两个提案（包括来自反思 AI 智能体的评价），然后通过内部评估来选择更优的提案。为了节省计算资源，排序 AI 智能体会优先比较以下几种提案：相似的提案、新提交的提案以及排名靠前的提案。

6 The evolution agent generates new proposals by improving existing ones. It does this in several different ways，including simplifying current ideas，combining top-ranking ideas，and generating proposals that are very different from current ones.

进化智能体通过优化已有的方案来产生新的方案。它采用多种方法，包括简化现有的思路、融合表现最好的思路，以及创造与现有思路截然不同的方案。

7 The meta-review agent identifies common patterns in the reflection agent's reviews and the ranking agent's debates. Its feedback goes to the reflection and generation agents，which use it to address common factors in future reviews and avoid generating similar proposals，respectively.

综合审查智能体（Meta-review Agent）识别反思智能体（Reflection Agent）的评论和排序智能体（Ranking Agent）的辩论中存在的常见模式。它将反馈提供给反思智能体和生成智能体（Generation Agent）。反思智能体利用这些反馈来改进未来评论中常见的不足；而生成智能体则利用这些反馈来避免生成类似的提案。

Results：AI co-scientist achieved a number of impressive biomedical results in tests.

结果：AI 协同研究在测试中取得了一些令人印象深刻的生物医学成果。

1 Google researchers generated proposals for experiments that would repurpose drugs to treat acute myeloid leukemia. They shared the 30 highest-ranked proposals with human experts，who chose five for lab tests. Of the five drugs tested，three killed acute myeloid leukemia cells.

谷歌的研究人员提出了若干利用现有药物治疗急性髓系白血病的实验方案。他们与人类专家分享了排名最高的 30 个方案，专家们从中选择了 5 种进行实验室测试。结果显示，在测试的 5 种药物中，有 3 种能够杀死急性髓系白血病细胞。

2 Experts selected three among 15 top-ranked generated proposals that proposed repurposing existing drugs to treat liver fibrosis. Two significantly inhibited liver fibrosis without being toxic to general cells.（Prior to this research，one of the drugs was approved by the United States Food and Drug Administration for a different illness，which may lead to a new treatment for liver fibrosis.)

专家们从 15 份高质量生成提案中遴选出 3 份，这些提案都建议重新利用现有药物来治疗肝纤维化。其中两种药物表现出显著的肝纤维化抑制效果，并且对普通细胞没有毒性。(在此项研究之前，其中一种药物已经获得美国食品药品监督管理局（FDA）的批准，用于治疗其他疾病，这有望为肝纤维化带来新的治疗方案。)

3 AI co-scientist invented a hypothesis to explain how microbes become resistant to antibiotics. Human researchers had proposed and experimentally validated the same hypothesis，but their work had not yet been published at the time，and AI co-scientist did not have access to it.

AI 协同科研人员提出了一个假设，用来解释微生物如何产生抗生素耐药性。人类研究人员也提出了相同的假设，并通过实验验证了它，但当时他们的研究尚未发表，AI 协同科研人员也无法获取相关信息。

Behind the news：A few AI systems have begun to produce original scientific work. For instance，a model generated research proposals that human judges deemed more novel than proposals written by flesh-and-blood scientists，and an agentic workflow produced research papers that met standards for acceptance by top conferences.

最新进展：一些 AI 系统已经开始产出原创性的科学成果。例如，一个模型生成的科研提案，经人类评审员评估，其新颖性甚至超过了人类科学家撰写的提案；此外，一个 AI 智能体（AI Agent）工作流程所产出的研究论文，也达到了顶级会议的录用标准。

Why it matters：While previous work used agentic workflows to propose research ideas on a general topic，this work generates proposals for specific ideas according to a researcher's constraints（for example，a researcher could specify that a novel medical treatment for a specific disease only consider drugs already approved for human trials for other uses）and further instructions. AI co-scientist can take feedback at any point，allowing humans to collaborate with the machine：People provide ideas，feedback，and guidance for the model，and the model researches and proposes ideas in return.

为什么重要：之前的研究工作使用 AI 智能体工作流来产生宽泛主题的研究方向，而本文的工作则更进一步，它能根据研究人员的具体限制（例如，研究人员可以指定：针对某种疾病的新型疗法，仅限于已获批准用于其他疾病人体试验的药物）和其他指令，为特定的研究方向生成建议。AI 辅助科研可以在任何阶段接受反馈，从而实现人机协作：人类为模型提供想法、反馈和指导，模型则负责研究并提出新的研究方向。

We're thinking：I asked my AI system to propose a new chemical experiment. But there was no reaction!

一个例子：我让我的 AI 系统（AI system）提出一个新的化学实验。但是没有反应！

#### Some AI-Generated Works Are Copyrightable

一些 AI 生成的作品可以获得版权

The United States Copyright Office determined that existing laws are sufficient to decide whether a given AI-generated work is protected by copyright，making additional legislation unnecessary.

美国版权局（United States Copyright Office）认为，现行法律足以判断某个 AI 生成的作品是否应受到版权保护，因此无需为此制定新的法律。

What's new：AI-generated works qualify for copyright if a human being contributed enough creative input，according to the second part of what will be a three-part report on artificial intelligence and copyright law.

最新进展：根据一份关于人工智能和版权法的三部分报告的第二部分，如果人类贡献了足够的创造性投入，人工智能（AI）生成的作品在满足一定条件下可以获得版权。

How it works：The report states that「the outputs of generative AI can be protected by copyright only where a human author has determined sufficient expressive elements.」In other words，humans and AI can collaborate on creative works，but copyright protection applies only if a human shapes the AI-generated material beyond simply supplying a prompt.

工作原理：该报告指出，「只有当人类作者为生成式 AI（Generative AI）的输出确定了充分的表达性元素时，该输出才能受到版权保护。」换句话说，人类和 AI 可以在创意作品上协作，但版权保护仅适用于人类对 AI 生成的内容进行了塑造，而不仅仅是提供指令的情形。

1 The report rejects the argument that protecting AI-generated works requires a new legal framework. Instead，it argues that copyright law already establishes clear standards of authorship and originality.

该报告驳斥了「保护人工智能（AI）生成作品需要建立新的法律框架」这一观点。相反，报告认为现行的版权法已经确立了明确的作者身份和原创性标准。

2 Human authors or artists retain copyright over creative contributions in the form of selection，coordination，and modification of generated outputs. Selection refers to curating AI-generated elements. Coordination involves organizing multiple generated outputs into a cohesive work. Modification is altering generated material in a way that makes it original. They retain copyright even if AI processes their creative work. They lose it only if the generated output is genuinely transformative.

人类作者或艺术家对其创意贡献保留版权，这些贡献包括对生成式 AI（Generative AI）输出结果的选择、协调和修改。其中，选择指的是对 AI 生成元素的精选。协调指的是将多个生成结果组织成一个连贯的作品。修改指的是以某种方式改变生成的内容，使其具有原创性。即使 AI 处理了他们的创意作品，他们仍然拥有版权。只有当生成的输出具有真正的变革性时，他们才会失去版权。

3 The report emphasizes continuity with past decisions regarding computer-assisted works. It cites a February 2022 ruling in which the Copyright Office rejected a work that had no human involvement. However，in 2023，the office granted a copyright to a comic book that incorporated AI-generated images because a human created original elements such as text，arrangement，and modifications. The report argues this approach aligns with prior treatment of technologies like photography：Copyright protection depends on identifiable human creative input，and that input merits protection even if technology assists in producing it.

该报告强调了在计算机辅助创作作品方面，与以往判决保持一致性。报告援引了 2022 年 2 月的一项裁定，指出如果作品完全由电脑生成，没有任何人类参与，则不具备版权。但是，在 2023 年，美国版权局批准了一本包含生成式 AI 生成图像的漫画书的版权，原因是其中存在人类的原创性投入，例如文本撰写、页面排版和图像修改。报告认为，这一原则与此前对摄影等技术的版权处理方式类似：版权保护的关键在于可识别的人类创造性贡献，即使创作过程中使用了技术辅助手段，这些人类的贡献仍然受到保护。

Behind the news：The first part of the Copyright Office's report on digital replicas，or generated likenesses of a person's appearance and voice. It found that existing laws don't provide sufficient protection against unauthorized digital replicas and recommended federal legislation to address the gap. Its findings influenced ongoing discussions in Congress，where proposed bills like the No AI FRAUD Act and the NO FAKES Act aim to regulate impersonation via AI. Additionally，industry groups such as the Authors Guild and entertainment unions have pursued their own agreements with studios and publishers to safeguard performers，artists，and authors from unauthorized digital reproduction. However，no federal law currently defines whether copyright can protect a person's likeness or performance.

新闻背景：版权局发布了关于数字复制品，或对个人外貌和声音的生成式复制的首份报告。报告发现，现有法律未能充分保护未经授权的数字复制品，因此建议通过联邦立法来填补这一法律空白。调查结果影响了国会正在进行的讨论。《禁止人工智能欺诈法案》（No AI FRAUD Act）和《禁止伪造法案》（NO FAKES Act）等拟议法案旨在通过 AI 来监管冒名顶替行为。此外，行业组织，例如作家协会（Authors Guild）和娱乐工会，也纷纷与制片厂和出版商达成协议，以保护表演者、艺术家和作家免受未经授权的数字复制。然而，目前还没有联邦法律明确规定版权是否能够保护个人的肖像或表演。

Why it matters：The Copyright Office deliberately avoided prescribing rigid criteria for the types or degrees of human input that are sufficient for copyright. Such determinations require nuanced evaluation case by case. This flexible approach accommodates the diverse ways creative people use AI as well as unforeseen creative possibilities of emerging technology.

Why it matters： 重要性在于，版权局特意避免制定关于版权所需人工参与的类型或程度的严格标准。因为，这些判断需要根据具体情况进行详细评估。这种灵活的方式既能适应创意人员使用 AI 的多种途径，也能适应新兴技术可能带来的意想不到的创意可能性。

We're thinking：Does copyright bar the use of protected works to train AI systems? The third part of the Copyright Office's report — no indication yet as to when to expect it — will address this question. The answer could have important effects on both the arts and AI development.

我们正在思考：利用受版权保护的作品来训练 AI 系统是否会被版权法禁止？版权局的报告将包含解决这个问题的第三部分，但目前尚不清楚何时发布。这个问题的答案可能会对艺术领域和 AI（人工智能）开发产生深远的影响。

#### Designer Materials

定制材料

Materials that have specific properties are essential to progress in critical technologies like solar cells and batteries. A machine learning model designs new materials to order.

拥有特定属性的材料，对于像太阳能电池和电池这样的关键技术的发展至关重要。现在，机器学习模型可以根据需求，设计出全新的材料。

What's new: Researchers at Microsoft and Shenzhen Institute of Advanced Technology proposed MatterGen，a diffusion model that generates a material's chemical composition and structure from a prompt that specifies a desired property. The model and code are available under a license that allows commercial as well as noncommercial uses without limitation. The training data also is noncommercially available.

最新进展：Microsoft 和深圳先进技术研究院的研究人员提出了 MatterGen，这是一种扩散模型，能够根据描述所需属性的提示，生成材料的化学成分和结构。该模型和代码均已开源，允许商业和非商业用途，没有任何限制。训练数据也以非商业用途提供。

[MatterGen: A new paradigm of materials design with generative AI  - Microsoft Research](https://www.microsoft.com/en-us/research/blog/mattergen-a-new-paradigm-of-materials-design-with-generative-ai/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=352686041&_hsenc=p2ANqtz-_FlXKJY5AlDtqjQ2VumVNfNz8brnGODpHz72xcJyu9JTL_v-y7x_k1sFJL_T4sUNgSgpvnwPiq6wRnA-fH2Sh-vrcAyA)

How it works：MatterGen's training followed a two-stage process. In the first stage，it learned to generate materials（specifically crystals — no liquids，gasses，or amorphous solids like glass). In the second，it learned to generate materials given a target mechanical，electronic，magnetic，or chemical property such as magnetic density or bulk modulus（the material's resistance to compression).

它是如何工作的：MatterGen 的训练分为两个阶段。第一阶段，它学习生成各种材料，具体来说是晶体 —— 不包括液体、气体以及像玻璃这样的非晶态固体。第二阶段，它学习根据给定的目标属性来生成材料，这些目标属性可以是机械、电子、磁性或化学性质，例如磁密度或体积模量（衡量材料抗压缩能力的指标）。

1 MatterGen first learned to remove noise that had been added to 600,000 examples drawn from two datasets. Specifically，it learned to remove noise from three noisy matrices that represented a crystal's shape（parallelepiped），the type of each atom，and the coordinates of each atom.

MatterGen 首先学习去除噪声，它使用了从两个数据集中提取的 60 万个例子进行训练。更具体地说，它学习去除三种带噪声的矩阵中的噪声，这三种矩阵分别代表晶体的形状（平行六面体）、每个原子的类型以及每个原子的坐标。

2 To incorporate information about properties，the authors added to the diffusion model four vanilla neural networks，each of which took an embedding of the target property. The diffusion model added the output of these networks to its intermediate embeddings at different layers.

为了加入关于属性的信息，作者在扩散模型中增加了四个简单的神经网络，每个网络都接收目标属性的嵌入（embedding，一种将属性信息转换为向量表示的方法）。扩散模型将这些网络的输出，在不同的层级添加到它的中间嵌入（intermediate embedding）中。这样做相当于在扩散模型的处理过程中，根据属性信息对中间结果进行调整。

3 Then the authors fine-tuned the system to remove added noise from materials that contained property information in their original dataset.

接着，作者对该系统进行了精细调整（fine-tuned），目的是去除那些原始数据集中，带有属性信息的材料在生成过程中引入的噪声。

4 At inference，given three matrices of pure noise representing crystal shape，atom types，and atom coordinates，and a prompt specifying the desired property，the diffusion model iteratively removed the noise from all three matrices.

在实际推理（inference）阶段，系统会接收三个矩阵，它们最初都充满了代表噪声的数据，分别对应晶体的形状、原子类型和原子坐标。同时，系统还会接收一个提示（prompt），用以指定期望获得的晶体属性。之后，扩散模型会逐步地从这三个矩阵中去除噪声，最终生成符合要求的晶体结构。

Results：The authors generated a variety of materials，and they synthesized one to test whether it had a target property. Specifically，they generated over 8,000 candidates with the target bulk modulus of 200 gigapascals（a measure of resistance to uniform compression），then automatically filtered them based on a number of factors to eliminate material in their dataset and unstable materials. Of the remaining candidates，they chose four manually and successfully synthesized one. The resulting crystal had a measured bulk modulus of 158 gigapascals.（Most materials in the dataset had a bulk modulus of between 0 and 400 gigapascals.)

结果：作者们生成了各种各样的材料，并合成其中一种，以验证其是否具备目标属性。具体来说，他们生成了超过 8000 种候选材料，这些材料的体积模量目标值为 200 吉帕斯卡（gigapascals）（体积模量是衡量材料抵抗均匀压缩能力的指标）。随后，他们基于多种因素自动筛选这些候选材料，剔除了数据集中已有的以及不稳定的材料。在剩余的候选材料中，他们手动选择了四种，并成功合成了一种。最终获得的晶体，其测量体积模量为 158 吉帕斯卡。（该数据集中，大多数材料的体积模量介于 0 到 400 吉帕斯卡之间。）

Behind the news：Published in 2023，DiffCSP also uses a diffusion model to generate the structures of new materials. However，it does so without considering their desired properties.

深度剖析：DiffCSP 发表于 2023 年，它同样采用扩散模型（diffusion model）来生成新材料的结构。不过，该方法在生成材料结构时，并没有考虑材料本身所应具备的特定属性。

Why it matters：Discovering materials relies mostly on searching large databases of existing materials for those with desired properties or synthesizing new materials and testing their properties by trial and error. Designing new crystals with desired properties at the click of a button accelerates the process dramatically.

为什么重要：目前，发现新材料主要有两种方式：一是搜索大型数据库，查找具有特定性能的现有材料；二是合成新材料，并通过反复试验来测试其性能。而现在，只需轻轻一点，就能设计出具有所需属性的新晶体，这将极大地加速材料发现的进程。

We're thinking：While using AI to design materials accelerates an important step，determining whether a hypothesized material can be  manufactured efficiently at scale is still challenging. We look forward to research into AI models that also take into account ease of manufacturing.

我们正在思考：虽然使用 AI 设计材料加速了一个重要环节，但要确定一种被预测的新材料是否能够高效地进行大规模生产，仍然是一个挑战。我们期待未来能出现更多 AI 模型方面的研究，这些模型能够将制造的难易程度也纳入考虑。