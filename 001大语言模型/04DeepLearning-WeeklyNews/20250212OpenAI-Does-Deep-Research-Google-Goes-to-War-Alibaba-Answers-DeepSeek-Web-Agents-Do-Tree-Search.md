## 20250212OpenAI-Does-Deep-Research-Google-Goes-to-War-Alibaba-Answers-DeepSeek-Web-Agents-Do-Tree-Search

[OpenAI Does Deep Research, Google Goes to War, Alibaba Answers DeepSeek, Web Agents Do Tree Search](https://www.deeplearning.ai/the-batch/issue-288/)

Dear friends,

At the Artificial Intelligence Action Summit in Paris this week，U.S. Vice President J.D. Vance said,「I'm not here to talk about AI safety. ... I'm here to talk about AI opportunity.」I'm thrilled to see the U.S. government focus on opportunities in AI. Further，while it is important to use AI responsibly and try to stamp out harmful applications，I feel「AI safety」is not the right terminology for addressing this important problem. Language shapes thought，so using the right words is important. I'd rather talk about「responsible AI」than「AI safety.」Let me explain.

在美国副总统 J.D. Vance 本周于巴黎人工智能行动峰会上表示：「我此行并非为了探讨人工智能安全，而是着眼于人工智能所带来的机遇。」我很高兴看到美国政府将重心放在人工智能的机遇之上。此外，尽管以负责任的态度使用人工智能，并努力消除其潜在的危害应用至关重要，但我认为使用「人工智能安全」一词来定义和解决这个问题并不恰当。语言会影响我们的思考方式，因此选择正确的措辞至关重要。我更倾向于使用「负责任的人工智能」而非「人工智能安全」来描述。下面我将对此进行阐述。

First，there are clearly harmful applications of AI，such as non-consensual deepfake porn（which creates sexually explicit images of real people without their consent），the use of AI in misinformation，potentially unsafe medical diagnoses，addictive applications，and so on. We definitely want to stamp these out! There are many ways to apply AI in harmful or irresponsible ways，and we should discourage and prevent such uses.

首先，人工智能显然有一些有害的应用，例如未经同意的深度伪造（deepfake）色情内容（即在未经他人允许的情况下，利用 AI 生成性暴露图像），在虚假信息中使用人工智能，潜在的不安全医疗诊断，以及令人上瘾的应用程序等。我们当然希望消除这些负面影响！有很多种以有害或不负责任的方式应用人工智能的途径，我们应该阻止并防止此类行为。

However，the concept of「AI safety」tries to make AI — as a technology — safe，rather than making safe applications of it. Consider the similar，obviously flawed notion of「laptop safety.」There are great ways to use a laptop and many irresponsible ways，but I don't consider laptops to be intrinsically either safe or unsafe. It is the application，or usage，that determines if a laptop is safe. Similarly，AI，a general-purpose technology with numerous applications，is neither safe nor unsafe. How someone chooses to use it determines whether it is harmful or beneficial.

然而，「AI 安全」的概念试图让 AI —— 作为一种技术本身 —— 变得安全，而不是仅仅关注如何安全地应用它。想想类似的，显然存在缺陷的「笔记本电脑安全」概念。使用笔记本电脑有很多好的方法，也有很多不负责任的方法，但我并不认为笔记本电脑本身是绝对安全或绝对不安全的。真正决定笔记本电脑是否安全的，是它的应用方式或者使用场景。类似地，AI 作为一种具有广泛用途的技术，拥有无数的应用，它本身既不是绝对安全的，也不是绝对不安全的。如何选择使用 AI，才决定了它会带来危害还是益处。

Now，safety isn't always a function only of how something is used. An unsafe airplane is one that，even in the hands of an attentive and skilled pilot，has a large chance of mishap. So we definitely should strive to build safe airplanes（and make sure they are operated responsibly)! The risk factors are associated with the construction of the aircraft rather than merely its application. Similarly，we want safe automobiles，blenders，dialysis machines，food，buildings，power plants，and much more.

现在，安全性不仅仅取决于如何使用某物。一架不安全的飞机，即使由细致且技术精湛的飞行员驾驶，也可能发生事故。因此，我们必须努力制造安全的飞机（并确保以负责任的方式使用它们)！ 这里的风险更多地与飞机的设计和制造相关，而不仅仅是使用方式。类似地，我们希望拥有安全的汽车、搅拌机、透析机、食品、建筑物和发电厂等。

「AI safety」presupposes that AI，the underlying technology，can be unsafe. I find it more useful to think about how applications of AI can be unsafe.

「AI 安全」这个概念， подразумевает 底层技术 AI 本身可能存在不安全性。但我个人认为，思考「AI 的应用在哪些方面可能造成不安全」更有实际意义。

Further，the term「responsible AI」emphasizes that it is our responsibility to avoid building applications that are unsafe or harmful and to discourage people from using even beneficial products in harmful ways.

更进一步，「负责任的 AI」强调了我们的责任：一方面，要避免构建不安全或有害的应用程序；另一方面，也要劝阻人们以不恰当的方式使用那些即便本身有益的产品。

If we shift the terminology for AI risks from「AI safety」to「responsible AI,」we can have more thoughtful conversations about what to do and what not to do.

如果我们把关于 AI 风险的说法从「AI 安全（AI Safety）」转向「负责任的 AI」，就能更深入地探讨哪些事该做，哪些事不该做。

I believe the 2023 Bletchley AI Safety Summit slowed down European AI development — without making anyone safer — by wasting time considering science-fiction AI fears rather than focusing on opportunities. Last month，at Davos，business and policy leaders also had strong concerns about whether Europe can dig itself out of the current regulatory morass and focus on building with AI. I am hopeful that the Paris meeting，unlike the one at Bletchley，will result in acceleration rather than deceleration.

我认为，2023 年的布莱切利 AI 安全峰会拖慢了欧洲 AI 的发展 —— 而且并没有提升安全性 —— 因为它把时间浪费在考虑科幻小说里的 AI 恐慌上，而忽略了发展机遇。上个月在达沃斯，商业和政策领袖们也表达了强烈的担忧，他们担心欧洲是否能摆脱当前的监管困境，转而专注于 AI 技术的应用。我希望在巴黎举行的会议，能一改布莱切利会议的风格，推动 AI 发展，而不是阻碍它。

In a world where AI is becoming pervasive，if we can shift the conversation away from「AI safety」toward responsible [use of] AI，we will speed up AI's benefits and do a better job of addressing actual problems. That will actually make people safer.

在一个人工智能日益普及的世界中，如果我们能将讨论的焦点从「人工智能安全」转变为负责任地 [使用] AI，我们将加速人工智能带来的益处，并更好地解决实际问题。这实际上会使人们更安全。

Keep building!

Andrew

### News

#### Agents Go Deep

AI 智能体迎来新突破

OpenAI introduced a state-of-the-art agent that produces research reports by scouring the web and reasoning over what it finds.

OpenAI 推出了一款先进的 AI 智能体，它通过搜索网络，并对发现的内容进行推理来生成研究报告。

What's new: OpenAI's deep research responds to users' requests by generating a detailed report based on hundreds of online sources. The system generates text output，with images and other media expected soon. Currently the agent is available only to subscribers to ChatGPT Pro，but the company plans to roll it out to users of ChatGPT Plus，Team，and Enterprise.

最新内容：OpenAI 的一项深度研究，通过生成一份基于数百个在线资源的详细报告来响应用户的请求。该系统目前生成文本报告，未来预计将支持图像和其他媒体。目前，该 AI 智能体仅对 ChatGPT Pro 订阅者开放，但 OpenAI 计划将其推广到 ChatGPT Plus、Team 和 Enterprise 用户。

How it works: Deep research is an agent that uses OpenAI's o3 model，which is not yet publicly available. The model was trained via reinforcement learning to use a browser and Python tools，similar to the way o1 learned to reason from reinforcement learning. OpenAI has not yet released detailed information about how it built the system.

它是如何工作的：深度研究是一个 AI 智能体，它使用了 OpenAI 的 o3 模型，但该模型目前尚未公开。这个模型通过强化学习进行训练，从而学会使用浏览器和 Python 工具，其训练方式类似于 o1 通过强化学习学会推理。关于 OpenAI 如何构建这个系统的详细信息，目前尚未公布。

1 The system responds best to detailed prompts that specify the desired output（such as the desired information，comparisons，and format），the team said in its announcement video (which features Mark Chen，Josh Tobin，Neel Ajjarapu，and Isa Fulford，co-instructor of our short courses「ChatGPT Prompt Engineering for Developers」and「Building Systems with the ChatGPT API」).

该系统对详细的提示反应最佳，这些提示应明确指定所需的输出，例如所需的信息、比较以及输出格式。这是该团队在公告视频中提到的（视频中出现了 Mark Chen、Josh Tobin、Neel Ajjarapu 和 Isa Fulford，他们也是我们的短期课程「面向开发者的 ChatGPT 提示工程（ChatGPT Prompt Engineering for Developers）」和「使用 ChatGPT API（ChatGPT API）构建系统」的联合讲师）。

2 Before answering，Deep research asks clarifying questions about the task.

在回答之前，为了更好地完成任务，需要首先进行深入研究，明确任务的具体要求。

3 In the process of answering，the system presents a sidebar that summarizes the model's chain of thought，terms it searched，websites it visited，and so on.

在回答过程中，系统会显示一个侧边栏，总结模型的思维链（Chain of Thought），包括模型搜索的关键词、访问的网站等信息。

4 The system can take as long as 30 minutes to provide output.

某些情况下，系统可能需要长达 30 分钟才能给出结果。

Result：On a benchmark of 3,000 multiple-choice and short-answer questions that cover subjects from ecology to rocket science，OpenAI deep research achieved 26.6 percent accuracy. In comparison，DeepSeek-R1（without web browsing or other tool use）achieved 9.4 percent accuracy and o1（also without tool use）achieved 9.1 percent accuracy. On GAIA，questions that are designed to be difficult for large language models without access to additional tools，OpenAI deep research achieved 67.36 percent accuracy，exceeding the previous state of the art of 63.64 percent accuracy.

结果：在一项基准测试中，研究人员使用了 3000 道多项选择和简答题，题目范围涵盖了从生态学到火箭科学等多个领域。OpenAI 的深度研究团队在这些问题上取得了 26.6% 的正确率。相比之下，DeepSeek-R1（未启用网页浏览或其他工具）的正确率为 9.4%，而 o1（同样未启用任何工具）的正确率为 9.1%。在 GAIA 基准测试中，该测试专门设计了一些对未使用额外工具的大语言模型来说非常困难的问题。OpenAI 的深度研究团队在该测试中取得了 67.36% 的正确率，超越了此前 63.64% 的最佳成绩。

Behind the news: OpenAI's deep research follows a similar offering of the same name by Google in December. A number of open source teams have built research agents that work in similar ways. Notable releases include a Hugging Face project that attempted to replicate OpenAI's work（not including training）in 24 hours（which achieved 55.15 percent accuracy on GAIA）and gpt-researcher，which implemented agentic web search in 2023，long before Google and OpenAI launched their agentic research systems.

新闻幕后：OpenAI 的深入研究，与 Google 在去年 12 月推出的同名产品殊途同归。许多开源团队也构建了类似的研究 AI 智能体。其中值得关注的项目包括：Hugging Face 团队尝试在 24 小时内复现 OpenAI 的研究成果（不包括训练），并在 GAIA（一个测试 AI 基本能力的基准）上达到了 55.15% 的准确率；以及 gpt-researcher，它早在 2023 年就实现了基于 AI 智能体的网络搜索，这比 Google 和 OpenAI 发布他们的 AI 智能体研究系统要早得多。

Why it matters: Reasoning models like o1 or o3 made a splash not just because they delivered superior results but also because of the impressive reasoning steps the model took to produce the results. Combining that ability with web search and tool use enables large language models to formulate better answers to difficult questions，including those whose answers aren't in the training data or whose answers change over time.

为什么重要：像 o1 或 o3 这样的推理模型之所以引人注目，不仅仅是因为它们提供了卓越的结果，更在于模型在生成结果时所展现出的卓越推理能力。将这种能力与网络搜索和工具的使用相结合，使得大语言模型能够更好地解答难题，这些难题的答案可能不在训练数据中，或者会随着时间而变化。

We're thinking: Taking as much as 30 minutes of processing to render a response，OpenAI's deep research clearly illustrates why we need more compute for inference.

我们认为：需要耗费长达 30 分钟的处理时间才能生成一个回复，OpenAI 的深入研究清楚地说明了为什么我们需要更多的算力来进行推理。

#### Google Joins AI Peers In Military Work

Google 加入人工智能同行，开展军事工作

Google revised its AI principles，reversing previous commitments to avoid work on weapons，surveillance，and other military applications beyond non-lethal uses like communications，logistics，and medicine.

Google 修改了其 AI 原则，不再坚持之前避免从事武器、监视等军事应用的承诺。此前，Google 承诺只将 AI 技术应用于通信、后勤、医疗等非致命性用途。

What's new: Along with releasing its latest Responsible AI Progress Report and an updated AI safety framework，Google removed key restrictions from its AI principles. The new version omits a section in the previous document titled「Applications we will not pursue.」The deleted text pledged to avoid「technologies that cause or are likely to cause overall harm」and，where the technology risks doing harm，to「proceed only where we believe that the benefits substantially outweigh the risks」with「appropriate safety constraints.」

最新进展：Google 在发布最新的《人工智能责任进展报告》和更新的人工智能安全体系的同时，取消了对其人工智能原则的重要限制。新版本省略了先前文档中标题为「我们不会追求的应用」的部分。删除的文本曾承诺避免「造成或可能造成总体损害的技术」，并且，如果技术存在造成损害的风险，则仅在「我们认为其收益远大于风险」，并具有「适当安全约束」的情况下才会「继续推进」。

How it works: Google's AI principles no longer prohibit specific applications but promote developing the technology to improve scientific inquiry，national security，and the economy.

它是如何运作的：Google 的人工智能（Artificial Intelligence，AI）原则不再禁止特定的应用，而是提倡开发这项技术，以促进科学研究、国家安全和经济发展。

1 The revised principles state that AI development should be led by democracies. The company argues that such leadership is needed given growing global competition in AI from countries that are not widely considered liberal democracies.

修改后的原则指出，AI 开发应该由民主国家引领。该公司认为，在全球 AI 领域的竞争日趋激烈，特别是来自一些非传统意义上的自由民主国家的挑战，因此（民主国家）需要在此领域发挥领导作用。

2 The new principles stress「responsible development and deployment」to manage AI's complexities and risks. They state that AI must be developed with safeguards at every stage，from design and testing to deployment and iteration，and those safeguards must adapt as technology and applications evolve.

新的原则强调「负责任的开发和部署」(responsible development and deployment），旨在应对人工智能（AI）带来的复杂性和风险。这些原则指出，AI 的开发必须在各个阶段都配备安全措施，从设计、测试到部署和迭代，并且这些安全措施必须随着技术和应用的演进而不断调整。

3 The revised principles also emphasize collaborative progress，stating that Google aims to learn from others and build AI that's broadly useful across industries and society.

修订后的原则还强调协作进步，声明 Google 的目标是向他人学习，并构建在各行各业和社会中广泛适用的 AI（Artificial Intelligence）。

4 Google emphasizes the need for「bold innovation,」stating that AI should be developed to assist，empower，and inspire people; drive economic progress; enable scientific breakthroughs; and help address global challenges. Examples include AlphaFold 3，which figures out how biological molecules interact，a key factor in designing chemical processes that affect them.

Google 强调需要「大胆创新」，声明人工智能（Artificial Intelligence）的开发应该旨在帮助、增强和激励人们；推动经济进步；实现科学突破；并帮助应对全球挑战。例子包括 AlphaFold 3，它可以解析生物分子之间的相互作用方式，这对于设计能够影响这些生物分子的化学过程至关重要。

5 The revised principles are buttressed by the 2025 Responsible AI Progress Report. This document outlines the company's efforts to evaluate risks through measures that align with the NIST AI Risk Management Framework including red teaming，automated assessments，and input from independent experts.

修订后的原则由《2025 年负责任的 AI 进展报告》提供支持。这份报告概述了 Microsoft 公司在评估风险方面所做的努力，这些努力与 NIST AI 风险管理框架保持一致，具体措施包括：红队演练（red teaming，一种通过模拟攻击来发现系统漏洞的安全测试方法）、自动化评估以及来自独立专家的意见。

Behind the news: Google's new stance reverses a commitment it made in 2018 after employees protested its involvement in Project Maven，a Pentagon AI program for drone surveillance，from which Google ultimately withdrew. At the time，Google pledged not to develop AI applications for weapons or surveillance，which set it apart from Amazon and Microsoft. Since then，the company has expanded its work in defense, building on a $1.3 billion contract with Israel. In 2024, Anthropic，Meta，and OpenAI removed their restrictions on military and defense applications，and Anthropic and OpenAI strengthened their ties with defense contractors such as Anduril and Palantir.

背景信息：Google 的新立场，实际上推翻了其 2018 年做出的承诺。此前，由于员工抗议 Google 参与了「Maven 项目」（Project Maven），这是一个五角大楼的 AI 项目，旨在利用无人机进行监控，Google 最终退出了该项目。当时，Google 承诺不开发用于武器或监视目的的 AI 应用，这使得它与 Amazon 和 Microsoft 形成鲜明对比。此后，Google 扩大了其在国防领域的工作，并与以色列签订了一项价值 13 亿美元的合同。2024 年，Anthropic、Meta 和 OpenAI 取消了对军事和国防应用的限制，并且 Anthropic 和 OpenAI 深化了与 Anduril 和 Palantir 等国防承包商的合作。

Why it matters: Google's shift in policy comes as AI is playing an increasing role in conflicts in Israel, Ukraine，and elsewhere，and while global geopolitical tensions are on the rise. While Google's previous position kept it out of military AI development，defense contractors like Anduril，Northrop Grumman，and Palantir — not to mention AI-giant peers — stepped in. The new principles recognize the need for democratic countries to take the lead in developing technology and standards for its use as well as the massive business opportunity in military AI as governments worldwide seek new defense capabilities. Still，no widely accepted global framework governs uses of AI in combat.

这项政策转变的意义在于：人工智能（AI）在以色列、乌克兰等地的冲突中扮演着日益重要的角色，全球地缘政治局势也日趋紧张。此前，Google 的政策使其避免直接参与军事 AI 的开发，但像 Anduril、Northrop Grumman 和 Palantir 这样的国防承包商，以及其他人工智能巨头公司，已经纷纷加入这一领域。新的原则表明，民主国家有必要率先开发相关技术和使用标准。同时，各国政府对新型防御能力的需求，也使得军事 AI 蕴藏着巨大的商业机会。然而，目前在国际上，仍然缺乏被广泛认可的、用于规范 AI 在战争中使用的框架。

We're thinking: Knowing how and when to employ AI in warfare is one of the most difficult ethical questions of our time. Democratic nations have a right to defend themselves，and those of us who live in democracies have a responsibility to support fellow citizens who would put themselves in harm's way to protect us. AI is transforming military strategy，and refusing to engage with it doesn't make the risks go away.

在战争中如何以及何时运用人工智能（AI），是当今时代最严峻的伦理挑战之一。民主国家拥有自卫的权利，而生活在民主制度下的人民，有责任支持那些挺身而出、保护我们免受伤害的同胞。人工智能（AI）正在变革军事战略，回避这一趋势并不能规避潜在的风险。

#### Alibaba's Answer to DeepSeek

阿里巴巴对 DeepSeek 的回应

While Hangzhou's DeepSeek flexed its muscles，Chinese tech giant Alibaba vied for the spotlight with new open vision-language models.

当杭州的 DeepSeek 展示实力时，中国科技巨头阿里巴巴也发布了新的开放视觉 - 语言模型，力图成为焦点。

What's new: Alibaba announced Qwen2.5-VL，a family of vision-language models（images and text in，text out）in sizes of 3 billion，7 billion，and 72 billion parameters. The weights for all three models are available for download on Hugging Face，each under a different license: Qwen2.5-VL-3B is free for non-commercial uses，Qwen2.5-VL-7B is free for commercial and noncommercial uses under the Apache 2.0 license，and Qwen2.5-VL-72B is free to developers that have less than 100 million monthly active users. You can try them out for free for a limited time in Alibaba Model Studio，and Qwen2.5-VL-72B is available via the model selector in Qwen Chat.

最新进展：阿里巴巴发布了 Qwen2.5-VL，这是一个视觉 - 语言模型（vision-language model）系列，它可以接收图像和文本作为输入，并输出文本。该系列包含三种不同规模的模型，参数量分别为 30 亿、70 亿和 720 亿。这三个模型的权重都可以在 Hugging Face 上下载，但各自采用不同的许可协议：Qwen2.5-VL-3B 免费用于非商业用途；Qwen2.5-VL-7B 在 Apache 2.0 许可下，可以免费用于商业和非商业用途；Qwen2.5-VL-72B 则面向月活用户少于 1 亿的开发者免费开放。你可以在阿里巴巴 Model Studio 中免费体验这些模型（限时），并且可以通过 Qwen Chat 中的模型选择器来使用 Qwen2.5-VL-72B。

How it works: Qwen2.5-VL models accept up to 129,024 tokens of input according to the developer reference (other sources provide conflicting numbers）and generate up to 8,192 tokens of output. Alibaba has not released details about how it trained them.

工作原理：根据开发者的参考信息，Qwen2.5-VL 模型最多可接受 129,024 个 Token 的输入（其他来源的数据可能存在差异），并生成最多 8,192 个 Token 的输出。阿里巴巴尚未公开关于这些模型训练方式的详细信息。

1 Qwen2.5-VL comprises a vision encoder and large language model. It can parse videos，images，text，and is capable of computer use（desktop and mobile).

Qwen2.5-VL 包含一个视觉编码器和大语言模型（Large Language Model），能够处理视频、图像和文本等多种信息，并具备操作计算机的能力（包括桌面和移动设备）。

2 The vision encoder accepts images of different sizes and represents them with different numbers of tokens depending on the size. For instance，one image might be 8 tokens and another 1125 tokens. This enabled the model to learn about the scale of images and to estimate the coordinates of objects in an image without rescaling.

视觉编码器可以接收不同尺寸的图像，并根据图像大小，将其表示为不同数量的 Token（Token）。例如，一张图像可能被编码为 8 个 Token，而另一张图像则可能被编码为 1125 个 Token。这种设计使得模型能够学习图像的比例信息，从而在无需重新缩放图像的前提下，估计图像中物体的坐标。

3 To reduce computation incurred by the vision encoder，the team replaced attention（which considers the entire input context）with windowed attention（which limits the input context to a window around a given token）and used full attention only in four layers. The resulting efficiency improves training and inference speeds.

为了降低视觉编码器带来的计算负担，研究团队使用窗口注意力（windowed attention），代替了原本需要考虑完整输入上下文的注意力机制（attention），窗口注意力将计算范围限制在特定 Token（Token）周围的窗口内。此外，他们仅在四个层中使用了完全注意力。这种优化显著提升了训练和推理速度。

Results：Alibaba reports Qwen2.5-VL-72B's performance on measures that span image and text problems，parsing documents，understanding videos，and interacting with computer programs. Across 21 benchmarks，it beat Microsoft Gemini 2.0 Flash，OpenAI GPT-4o，Anthropic Claude 3.5 Sonnet，and open competitors on 13 of them（where comparisons are  relevant and available).

结果：阿里巴巴公布了 Qwen2.5-VL-72B 在多个任务上的表现，这些任务涵盖了图像和文本处理、文档解析、视频理解以及与计算机程序互动。在 21 项基准测试中，Qwen2.5-VL-72B 在其中 13 项上超越了 Microsoft Gemini 2.0 Flash、OpenAI GPT-4o、Anthropic Claude 3.5 Sonnet 以及其他开源模型（在有可比数据且比较有意义的情况下）。

1 For example，on answering math questions about images in MathVista，Qwen2.5-VL-72B achieved 74.8 percent，while the closest competing model（Gemini 2.0 Flash）achieved 73.1 percent.

例如，在回答 MathVista 数据集中关于图像的数学问题时，Qwen2.5-VL-72B 取得了 74.8% 的准确率，而最接近的竞争模型（Gemini 2.0 Flash）的准确率则为 73.1%。

2 In Video-MME，which evaluates a model's ability to answer questions about videos，Qwen 2.5 VL achieved 73.3 percent. GPT-4o achieved 71.9 percent and InternVL2.5，the next-best open competitor，achieved 72.1 percent.

在 Video-MME 基准测试中（该测试评估模型回答视频相关问题的能力），Qwen 2.5 VL 取得了 73.3% 的成绩。GPT-4o 取得了 71.9% 的成绩，而 InternVL2.5，作为表现第二好的开源模型，取得了 72.1% 的成绩。

3 Used in an agentic workflow，Qwen2.5-VL-72B outperformed Claude 3.5 Sonnet when controlling Android devices and navigating desktop user interfaces. However，it finished second to other open vision-language models in several tests.

在 AI 智能体（AI Agent）工作流程中，Qwen2.5-VL-72B 在控制 Android 设备和导航桌面用户界面方面的表现优于 Claude 3.5 Sonnet。然而，在一些测试中，它的性能不如其他开源视觉语言模型。

More models: Alibaba also introduced competition for DeepSeek and a family of small models.

更多模型：阿里巴巴也推出了与 DeepSeek 竞争的模型，以及一系列小型模型。

1 Qwen2.5-Max is a mixture-of-experts model that outperforms GPT-4o and DeepSeek-V3 on graduate-level science questions in GPQA-Diamond and regularly updated benchmarks like Arena-Hard, LiveBench，and LiveCodeBench. However，Qwen2.5-Max performed worse than o1 and DeepSeek-R1.

Qwen2.5-Max 是一种混合专家模型（Mixture-of-Experts model），在 GPQA-Diamond 数据集中的研究生水平科学问题上，以及 Arena-Hard、LiveBench 和 LiveCodeBench 等定期更新的评测基准中，性能优于 GPT-4o 和 DeepSeek-V3。不过，Qwen2.5-Max 的性能略逊于 o1 公司和 DeepSeek-R1。

2 Qwen2.5-1M is a family of smaller language models（7 billion and 14 billion parameters）that accept up to 1 million tokens of input context.

Qwen2.5-1M 是一系列小型语言模型 （70 亿和 140 亿参数） ，它们可以接受高达 100 万个 Token 的输入上下文。

Why it matters: Vision-language models are getting more powerful and versatile. Not long ago，it was an impressive feat simply to answer questions about a chart or diagram that mixed graphics with text. Now such models are paired with an agent to control computers and smartphones. Broadly speaking，the Qwen2.5-VL models outperform open and closed competitors and they're open to varying degrees（though the data is not available），giving developers a range of highly capable choices.

为什么重要：多模态模型正变得越来越强大和通用。不久前，能够回答关于混合了图形和文本的图表的问题还是一项令人印象深刻的成就。而现在，这些模型已经与 AI 智能体结合，用于控制计算机和智能手机。总的来说，Qwen2.5-VL 模型在性能上优于其他开放和封闭的模型，并且它们在不同程度上对外开放 （但其训练数据尚未公开） ，为开发者提供了多种选择。

We're thinking: We're happy Alibaba released a vision-language model that is broadly permissive with respect to commercial use（although we'd prefer that all sizes were available under a standard open weights license). We hope to see technical reports that illuminate Alibaba's training and fine-tuning recipes.

我们的想法是：我们很高兴阿里巴巴发布了一个在商业用途上具有广泛授权的视觉语言模型（vision-language model）（尽管我们更希望所有尺寸的模型都能以标准的开放权重许可形式提供）。我们期待看到相关的技术报告，详细介绍阿里巴巴的训练和微调方法。

#### Tree Search for Web Agents

Web 智能体的树搜索方法

Browsing the web to achieve a specific goal can be challenging for agents based on large language models and even for vision-language models that can process onscreen images of a browser. While some approaches address this difficulty in training the underlying model，the agent architecture can also make a difference.

对于基于大语言模型（Large Language Model）的智能体来说，浏览网页以达成特定目标可能充满挑战。即使是能够处理浏览器屏幕图像的视觉 - 语言模型，也面临同样的难题。一些研究致力于训练底层的大语言模型来解决这个问题，但智能体架构的设计也会对性能产生影响。

What's new: Jing Yu Koh and colleagues at Carnegie Mellon University introduced tree search for language model agents，a method that allows agents to treat web interactions like tree searches. In this way，agents can explore possible chains of actions and avoid repeating mistakes.

最新进展：Jing Yu Koh 和卡内基梅隆大学的同事们提出了一种用于 AI 智能体（AI Agent）的树搜索方法。该方法使 AI 智能体能够像进行树搜索一样处理 Web 交互。这样，AI 智能体就可以探索各种可能的行动方案，避免重蹈覆辙。

Key insight: Some web tasks，for instance finding a price of a particular item，require a chain of intermediate actions：navigating to the right page，scrolling to find the item，matching an image of the item to the image on the page，and so on. If an agent clicks the wrong link during this process，it might lose its way. The ability to evaluate possible actions and remember previous states of web pages can help an agent correct its mistakes and choose a chain of actions that achieves its goal.

核心观点：某些网络任务，比如查找特定商品的价格，需要执行一系列中间步骤：导航至正确的页面，滚动页面找到目标商品，将商品的图片与网页上的图片进行比对，等等。如果 AI 智能体（AI Agent）在此过程中点击了错误的链接，就可能导致任务失败。因此，评估潜在操作并记住网页之前的状态，有助于 AI 智能体纠正错误，并选择一系列能达成目标的动作。

How it works: An agent based on GPT-4o attempted 200 tasks using website mockups that mimicked an online retail store，Reddit-like forum，and directory of classified ads. The tasks included ordering an item to be delivered to a given address，finding specific images on the forum，and posting an ad. The authors annotated each web page using the method called Set of Mark，which identifies every visual element capable of interaction with a bounding box and a numerical ID.

工作原理：一个基于 GPT-4o 的 AI 智能体尝试了 200 个任务，这些任务使用了网站模型，模拟了在线零售商店、类似 Reddit 的论坛和分类广告目录。任务内容包括：订购一件商品并将其送到指定地址、在论坛上查找特定图片、以及发布广告。作者使用了一种名为 Set of Mark 的方法对每个网页进行标注。该方法通过边界框和数字 ID 来识别每个可交互的视觉元素。

1 The agent started with a web page and an instruction such as,「Tell me the number of reviews our store received that mention the term ‘not useful.'」It passed an image of the page to the LLM，which predicted five actions that could make progress toward completing the task such as scrolling up or down，hovering over an element，clicking，typing in a text field，or opening a new URL.

AI 智能体从一个网页和一条指令开始，例如：「请告诉我，在所有关于我们店铺的评价中，有多少条提到了 ‘没用' 这个词」。它会将网页的图片传递给大语言模型，然后大语言模型会预测出五个有助于完成任务的操作，包括：向上或向下滚动页面、将鼠标悬停在某个元素之上、点击、在文本框中输入文字，以及打开一个新的网址。

2 The agent executed the five actions. After each one，the LLM assessed the current state of the page using the previous states as context. The assessment assigned a value between 0 and 1（meaning the task was complete). The agent kept a list of page states and their values.

AI 智能体执行了五个动作。每次执行动作后，大语言模型（LLM/Large Language Model）都会参考之前的页面状态，评估当前页面的状态，并给出一个 0 到 1 之间的评分（1 代表任务完成）。AI 智能体会保存一个包含所有页面状态及其对应评分的列表。

3 The agent selected the web page state with the highest value after executing the five actions，and repeated the process，making a new set of five predictions based on the highest-value state.

智能体在执行五个动作后，会选择价值最高的网页状态。然后，它会重复这个过程，基于该状态做出新的一组五个预测。

4 This process is a search：The agent executed a chain of actions until the value of the new states dropped below the values of other states. If all new states had lower values，the agent backtracked to a previous state with a higher value and asked the LLM for five more actions. The search stopped when the agent had completed the task or explored 20 possible states.

这个过程可以看作一次搜索：AI 智能体（AI Agent）执行一系列动作，直到新的状态值低于其他状态的值。如果所有新状态的值都较低，AI 智能体（AI Agent）会回溯到之前状态值较高的状态，并要求大语言模型（LLM/Large Language Model）提供另外五个动作。当 AI 智能体（AI Agent）完成任务或探索了 20 个可能的状态时，搜索就会停止。

Results: The authors compared two agents，one that followed their search method and another that started at the same page and received the same instruction but took one action per state and never backtracked. The agents attempted 100 shopping tasks，50 forum tasks，and 50 classified-ads tasks. The one equipped to search successfully completed 26.4 percent of the tasks，while the other agent completed 18.9 percent of the tasks.

结果：作者比较了两个 AI 智能体，一个采用了他们提出的搜索方法，另一个从相同页面开始，并接收到相同的指令，但每个状态仅执行一个动作且不进行回溯。这两个 AI 智能体分别尝试了 100 个购物任务、50 个论坛任务和 50 个分类广告任务。结果显示，具备搜索能力的 AI 智能体成功完成了 26.4% 的任务，而另一个 AI 智能体完成了 18.9% 的任务。

Why it matters: Search joins reflection，planning，tool use，and multi-agent collaboration as an emerging agentic design pattern. Following many branching paths of actions enables an agent to determine the most effective set of actions to accomplish a task.

为什么重要：搜索与反思、规划、工具使用和多智能体协作一起，构成了一种新兴的自主设计模式。通过探索多种行动方案，AI 智能体能够确定完成任务的最有效行动方案。

We're thinking: Agentic design patterns are progressing quickly! In combination with computer use，this sort of search method may enable agents to execute a wide variety of desktop tasks.

我们的想法是：AI 智能体设计模式（Agentic design patterns）正在快速发展！结合使用计算机，这种搜索机制可能让 AI 智能体执行各种桌面任务。