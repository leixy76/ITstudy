## 20250205o3-mini-Puts-Reasoning-in-High-Gear-How-to-Train-for-Computer-Use-Gemini-2.0-Thinks-Faster-and-more

[o3-mini Puts Reasoning in High Gear, How to Train for Computer Use, Gemini 2.0 Thinks Faster, and more...](https://www.deeplearning.ai/the-batch/issue-287/)

Feb 05, 2025

亲爱的朋友们，

Dear friends,

A “10x engineer” — a widely accepted concept in tech — purportedly has 10 times the impact of the average engineer. But we don’t seem to have 10x marketers, 10x recruiters, or 10x financial analysts. As more jobs become AI enabled, I think this will change, and there will be a lot more “10x professionals.”

「10 倍工程师」—— 科技界普遍认可的一种说法 —— 指的是其影响力是普通工程师的 10 倍。然而，我们似乎很少听到「10 倍营销人员」、「10 倍招聘人员」或是「10 倍金融分析师」的说法。随着越来越多的工作被人工智能（AI）赋能，我认为这种情况将会改变，并且会出现更多「10 倍专业人士」。

There aren’t already more 10x professionals because, in many roles, the gap between the best and the average worker has a ceiling. No matter how athletic a supermarket checkout clerk is, they’re not likely to scan groceries so fast that customers get out of the store 10x faster. Similarly, even the best doctor is unlikely to make patients heal 10x faster than an average one (but to a sick patient, even a small difference is worth a lot). In many jobs, the laws of physics place a limit on what any human or AI can do (unless we completely reimagine that job).

目前没有更多效率提升十倍的专业人士，是因为在许多岗位中，顶尖人才和普通员工之间的差距存在上限。无论一个超市收银员多么敏捷，他们都不太可能以十倍的速度扫描商品，从而使顾客能更快离开商店。同样，即使是最优秀的医生，也不太可能让病人比普通医生康复速度快十倍（但对于生病的病人来说，即使是很小的进步也意义重大）。在许多工作中，物理定律限制了人类或 AI（人工智能）所能达到的程度（除非我们彻底重新定义这项工作）。

But for many jobs that primarily involve applying knowledge or processing information, AI will be transformative. In a few roles, I’m starting to see tech-savvy individuals coordinate a suite of technology tools to do things differently and start to have, if not yet 10x impact, then easily 2x impact. I expect this gap to grow.

但对于许多主要涉及应用知识或处理信息的工作而言，人工智能（AI）将带来变革。在某些岗位上，我已观察到一些技术娴熟的人士正在协调运用一系列技术工具来创新工作方式，即使尚未实现 10 倍的影响，也能轻松达到 2 倍的影响。我预计这种差距会持续扩大。

10x engineers don’t write code 10 times faster. Instead, they make technical architecture decisions that result in dramatically better downstream impact, they spot problems and prioritize tasks more effectively, and instead of rewriting 10,000 lines of code (or labeling 10,000 training examples) they might figure out how to write just 100 lines (or collect 100 examples) to get the job done.

顶尖工程师写代码的速度并不是 10 倍于常人。相反，他们通过制定合理的技术架构，从而对后续工作产生显著的积极影响；他们能够发现问题，并更有效地安排任务优先级；他们不会重写 10,000 行代码（或标记 10,000 个训练示例），而是另辟蹊径，可能只需要编写 100 行代码（或收集 100 个示例）即可完成任务。

I think 10x marketers, recruiters, and analysts will, similarly, do things differently. For example, perhaps traditional marketers repeatedly write social media posts. 10x marketers might use AI to help write, but the transformation will go deeper than that. If they are deeply sophisticated in how to apply AI — ideally able to write code themselves to test ideas, automate tasks, or analyze data — they might end up running a lot more experiments, get better insights about what customers want, and generate much more precise or personalized messages than a traditional marketer, and thereby end up making 10x impact.

我认为顶尖的营销人员、招聘人员和分析师也会以类似的方式行事。例如，传统的营销人员可能需要反复撰写社交媒体帖子。顶尖的营销人员可能会利用 AI 来辅助写作，但这种转变将会更加彻底。如果他们能够精通 AI 的应用，理想情况下，能够亲自编写代码来验证想法、实现任务自动化或分析数据，他们最终可能会进行更多的实验，更深入地了解客户的需求，并生成比传统营销人员更精准或更个性化的信息，从而最终实现十倍的效益。

Fig: Comic-style illustration of a confident woman and man standing beside bold ‘10X’ text on a bright background.

漫画风格的插图，描绘了一位自信的女性和一位自信的男性，他们站在明亮背景上粗体的「10X」文字旁边。

Similarly, 10x recruiters won’t just use generative AI to help write emails to candidates or summarize interviews. (This level of use of prompting-based AI will soon become table stakes for many knowledge roles.) They might coordinate a suite of AI tools to efficiently identify and carry out research on a large set of candidates, enabling them to have dramatically greater impact than the average recruiter. And 10x analysts won’t just use generative AI to edit their reports. They might write code to orchestrate a suite of AI agents to do deep research into the products, markets, and companies, and thereby derive far more valuable conclusions than someone who does research the traditional way.

同样，顶尖的招聘人员不仅仅会使用生成式 AI（Generative AI）来帮助撰写给候选人的电子邮件或总结面试内容（基于提示的 AI 使用水平很快将成为许多知识型岗位的标配）。他们可能会协调一系列 AI 工具，以有效地识别和对大量候选人进行研究，使他们能够比普通招聘人员产生更大的影响。而且，顶尖的分析师不仅仅会使用生成式 AI 来编辑他们的报告。他们可能会编写代码来协调一系列 AI 智能体（AI Agent），以深入研究产品、市场和公司，从而得出比传统的分析研究方法更有价值的结论。

A 2023 Harvard/BCG study estimated that, provided with GPT-4, consultants could complete 12% more tasks, and completed tasks 25% more quickly. This was just the average, using 2023 technology. The maximum advantage to be gained by using AI in a sophisticated way will be much bigger, and will only grow as technology improves.

2023 年哈佛 / BCG 的一项研究估计，借助 GPT-4，顾问们可以多完成 12% 的任务，并且完成任务的速度提升 25%。而这仅仅是基于 2023 年技术得出的平均结果。如果以更精细的方式运用 AI，所能获得的最大优势将会远超于此，并且还将随着技术的进步持续扩大。

Here in Silicon Valley, I see more and more AI-native teams reinvent workflows and do things very differently. In software engineering, we've venerated the best engineers because they can have a really massive impact. This has motivated many generations of engineers to keep learning and working hard, because doing those things increases the odds of doing high-impact work. As AI becomes more helpful in many more job roles, I believe we will open up similar paths to a lot more people becoming a “10x professional.”

在硅谷，我看到越来越多的团队利用 AI 技术重塑工作流程，他们的工作方式也变得越来越不一样。在软件工程领域，我们一直非常看重那些能够产生巨大影响的优秀工程师。这种认可激励了一代又一代的工程师不断学习和努力，因为他们相信这样做能让他们更有可能做出更有价值的工作。随着 AI 在越来越多的岗位上发挥作用，我相信会有更多人有机会成为「10 倍效能的专业人才」(指工作效率极高的人才）。

Keep learning!

保持学习的热情！

Andrew

### News

#### Reasoning in High Gear

推理引擎，火力全开

OpenAI 推出了他们 o1 模型的迭代版本，新模型速度更快、成本更低，尤其擅长编程、数学和科学。

What’s new: o3-mini is a large language model that offers selectable low, medium, and high levels of reasoning “effort.” These levels consume progressively higher numbers of reasoning tokens (specific numbers and methods are undisclosed), and thus greater time and cost, to generate a chain of thought. It’s available to subscribers to ChatGPT Plus, Team, and Pro, as well as to higher-volume users of the API (tiers 3 through 5). Registered users can try it via the free ChatGPT service by selecting “reason” in the message composer or selecting o3-mini before regenerating a response.

最新进展：o3-mini 是一种大语言模型（Large Language Model，LLM），它提供了低、中、高三种可选的推理「强度」级别。这些级别会消耗递增的推理 Token（Token）数量（具体数字和方法未公开），因此生成思维链需要更多时间和成本。ChatGPT Plus、Team 和 Pro 的订阅用户，以及 API 的高级别用户（第 3 级到第 5 级）可以使用此模型。注册用户可以通过免费的 ChatGPT 服务体验它，只需在消息输入框中选择「reason」，或者在重新生成回复前选择 o3-mini 即可。

How it works: o3-mini’s training set emphasized structured problem-solving in science and technology fields, and fine-tuning used reinforcement learning on chain-of-thought (CoT) data. Like the o1 family, it charges for tokens that are processed during reasoning operations and hides them from the user. (Competing reasoning models DeepSeek-R1, Gemini 2.0 Flash Thinking, and QwQ-32B-Preview make these tokens available to users.) o3-mini has a maximum input of 200,000 tokens and a maximum output of 100,000 tokens. Its knowledge cutoff is October 2023.

工作原理：o3-mini 的训练集侧重于科学技术领域中结构化的问题解决，并通过在思维链（CoT）数据上使用强化学习进行微调。与 o1 系列一样，它对推理运算期间处理的 Token 收费，并且这些 Token 对用户不可见（竞争推理模型包括 DeepSeek-R1、Gemini 2.0 Flash Thinking 和 QwQ-32B-Preview，它们会将这些 Token 提供给用户）。o3-mini 的最大输入为 200,000 个 Token，最大输出为 100,000 个 Token。它的知识截止日期是 2023 年 10 月。

1 In OpenAI’s tests, o3-mini beat o1 and o1-mini on multiple benchmarks including math (AIME 2024), science (GPQA Diamond), and coding (Codeforces and LiveBench). It outperformed o1 by 1 to 4 percentage points when set at high or medium effort, and it outperformed o1-mini when set at low effort. It did significantly less well on tests of general knowledge, even with high effort. On MMLU (multiple-choice questions in many fields) and SimpleQA (questions about basic facts), o3-mini with high effort (which achieved 86.9 percent and 13.8 percent respectively) underperformed o1 (92.3 percent and 47 percent) and GPT-4o (88.7 percent and 39 percent).

在 OpenAI 的测试中，o3-mini 在多项基准测试中击败了 o1 和 o1-mini，包括数学（AIME 2024）、科学（GPQA Diamond）和编码（Codeforces 和 LiveBench）。在高或中等算力下，它的表现比 o1 高出 1 到 4 个百分点；在低算力下，它的表现优于 o1-mini。即使在高算力下，它在常识测试中的表现也明显较差。在 MMLU（多个领域的选择题）和 SimpleQA（关于基本事实的问题）上，高算力下的 o3-mini（分别达到 86.9% 和 13.8%）的表现不如 o1（92.3% 和 47%）和 GPT-4o（88.7% 和 39%）。

2 Unlike o1-mini, o3-mini supports function calling, structured outputs (JSON format), developer messages (system prompts that specify the model’s context or persona separately from user input), and streaming (delivering responses token-by-token in real time).

与 o1-mini 不同，o3-mini 支持函数调用、结构化输出（JSON 格式）、开发者消息（独立于用户输入，用于指定模型上下文或角色的系统提示）和流式传输（实时地逐个 Token 交付响应）。

3 API access costs $1.10/$4.40 per million input/output tokens with a discounted rate of $0.55 per million cached input tokens. OpenAI’s Batch API, which processes high-volume requests asynchronously, costs half as much. In comparison, access to o1 costs $15/$60 per million input/output tokens and o1-mini costs $3/$12 per million input/output tokens. (OpenAI recently removed API pricing for o1-mini and, in the ChatGPT model picker, replaced it with o3-mini, which suggests that o1-mini is being phased out.)

API 访问费用为：每百万输入 Token 1.10 美元 / 每百万输出 Token 4.40 美元；缓存的输入 Token 享有折扣，为每百万 0.55 美元。OpenAI 的批量 API 异步处理大量请求，费用减半。相比之下，访问 o1 的费用为每百万输入 / 输出 Token 15 美元 / 60 美元，而 o1-mini 的费用为每百万输入 / 输出 Token 3 美元 / 12 美元。(OpenAI 最近取消了 o1-mini 的 API 定价，并在 ChatGPT 模型选择器中将其替换为 o3-mini，这表明 o1-mini 正在逐步淘汰。)

4 OpenAI limits the number API calls users can make per minute and per day depending on how frequently they use the API and how much money they’ve spent. Rate limits range from 5,000/4 million requests/tokens per per minute (Tier 3) to 30,000/150 million requests/tokens per minute (Tier 5), with higher limits for batch requests.

OpenAI 根据用户使用 API 的频率和消费金额，限制他们每分钟和每天可以发起的 API 调用数量。速率限制根据用户等级而不同，从 Tier 3 用户的每分钟 5,000 个请求 / 400 万个 Token，到 Tier 5 用户的每分钟 30,000 个请求 / 1.5 亿个 Token。批量请求的限制更高。

5 o3-mini’s system card highlights safety measures taken during the model’s training. OpenAI notes that o3-mini’s improved coding ability puts it at a medium risk for autonomous misuse, the first OpenAI model to be so flagged.

o3-mini 的系统卡突出了模型训练期间采取的安全措施。OpenAI 指出，o3-mini 改进的编码能力使其面临自主滥用的中等风险，这是第一个被如此标记的 OpenAI 模型。

What they’re saying: Users praised o3-mini for its speed, reasoning, and coding abilities. They noted that it responds best to “chunkier” prompts with lots of context. However, due to its smaller size, it lacks extensive real-world knowledge and struggles to recall facts.

用户反馈：用户称赞 o3-mini 的速度、推理和编码能力。他们指出，它更擅长处理包含大量上下文信息的提示。然而，由于其较小的尺寸，它缺乏广泛的现实世界知识，并且难以回忆事实。

Behind the news: Days after releasing o3-mini, OpenAI launched deep research, a ChatGPT research agent based on o3. OpenAI had announced the o3 model family in December, positioning it as an evolution of its chain-of-thought approach. The release followed hard upon that of DeepSeek-R1, an open weights model that captivated the AI community with its high performance and low training cost, but OpenAI maintained that the debut took place on its original schedule.

最新进展：在发布 o3-mini 几天后，OpenAI 推出了 deep research，这是一个基于 o3 的 ChatGPT 研究智能体（AI Agent）。OpenAI 曾在 12 月份宣布了 o3 模型系列，并将其定位为链式思考方法的演进。在此之前不久，DeepSeek-R1 刚刚发布，这是一种以高性能和低训练成本吸引了 AI 社区的开放权重模型。但 OpenAI 方面表示，deep research 的发布仍然按照原定计划进行。

Why it matters: o3-mini continues OpenAI’s leadership in language models and further refines the reasoning capabilities introduced with the o1 family. In focusing on coding, math, and science tasks, it takes advantage of the strengths of reasoning models and raises the bar for other model builders. In practical terms, it pushes AI toward applications in which it’s a reliable professional partner rather than a smart intern.

为什么重要：o3-mini 的发布，延续了 OpenAI 在大语言模型领域的领先地位，并进一步提升了 o1 系列所具备的推理能力。通过专注于代码、数学和科学等任务，它充分发挥了推理模型的优势，也为其他模型开发者设立了更高的标杆。从实际应用角度来看，它推动着 AI 向更可靠的专业伙伴方向发展，而不仅仅是一个聪明的实习生。

We’re thinking: We’re glad that o3-mini is available to users of ChatGPT’s free tier as well as paid subscribers and API users. The more users become familiar with how to prompt reasoning models, the more value they’ll deliver.

我们的想法是：我们很高兴 o3-mini 能够提供给 ChatGPT 免费用户、付费订阅者和 API 用户。用户越熟悉如何引导推理模型，它所能创造的价值就越大。

#### Training for Computer Use

计算机应用训练

Fig: Flowchart illustrating the automation of opening, editing, and saving a Word document using PyAutoGUI.

流程图，展示了如何使用 PyAutoGUI 自动打开、编辑和保存 Word 文档。

As Anthropic, Google, OpenAI, and others roll out agents that are capable of computer use, new work shows how underlying models can be trained to do this.

随着 Anthropic、Google、OpenAI 等公司相继推出具备计算机使用能力的 AI 智能体（AI Agent），最新的研究表明，可以通过训练底层模型来实现这一目标。

What’s new: Yujian Qin and colleagues at ByteDance and Tsinghua University introduced UI-TARS, a fine-tuned version of the vision-language model Qwen2-VL that uses lines of reasoning to decide which mouse clicks, keyboard presses, and other actions to take in desktop and mobile apps. The model’s weights are licensed freely for commercial and noncommercial uses via Apache 2.0. You can download them here.

最新进展：字节跳动和清华大学的秦玉健及其同事推出了 UI-TARS，它是视觉 - 语言模型 Qwen2-VL 的一个精调版本。UI-TARS 通过推理过程，决定在桌面和移动应用中执行哪些鼠标点击、键盘输入等操作。该模型的权重采用 Apache 2.0 协议开源，允许商业和非商业用途。您可以在这里下载：[下载链接]。

[bytedance-research/UI-TARS-72B-DPO · Hugging Face](https://huggingface.co/bytedance-research/UI-TARS-72B-DPO?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8zUg78jpYxLSTEFg2x489XvzCThJaiNi9sPaI3tjrsEhPQ73-1Wngmw912raeA2_ZO_dwP)

1 The authors added chains of thought (CoTs) to their training set of screenshots and actions by prompting an unspecified vision-language model to explain the current action given previous screenshots, actions, and generated CoTs. Sometimes that process led to bad explanations, so they also generated multiple CoTs and actions for a given screenshot and selected the CoT that led to the correct action.

作者们将思维链（Chain of Thought，CoT）添加到他们的屏幕截图和动作的训练集中。他们通过提示一个未指明的视觉 - 语言模型，解释在先前的屏幕截图、动作和生成的思维链（CoT）的情况下，当前的动作应该是什么。有时，这个过程会导致错误的解释，因此他们也为一个给定的屏幕截图生成多个思维链（CoT）和动作，并选择能够引导至正确动作的思维链（CoT）。

2 They fine-tuned UI-TARS to generate a CoT and action from an instruction (such as “Open the document and add the text ‘hello’”) plus the screenshots, CoTs, and actions so far.

他们对 UI-TARS 进行了微调，使其能够从指令（例如「打开文档并添加文本 ‘hello'」），以及到目前为止的屏幕截图、思维链（CoT）和动作中生成一个思维链（CoT）和动作。

3 They ran UI-TARS within a virtual PC, generating a large number of screenshots, CoTs, and actions so far. They filtered out erroneous CoTs and actions using rules (such as removing those that included redundant actions), scoring outputs automatically and removing those with low scores, and reviewing them manually. They fine-tuned the model on the remaining outputs and repeatedly generated, filtered, and fine-tuned.

他们在虚拟 PC 中运行 UI-TARS，生成大量的屏幕截图、思维链（CoT）和到目前为止的动作。他们使用规则过滤掉错误的思维链（CoT）和动作（例如，删除包含冗余动作的那些），自动对输出进行评分并删除那些得分低的，以及手动审查它们。他们根据剩余的输出对模型进行微调，并重复进行生成、过滤和微调。

4 They also fine-tuned the model on corrected examples of erroneous CoTs and actions. Human annotators corrected the CoT and action to (a) avoid the error and (b) fix the error after it occurred.

他们还在纠正后的 CoT 和动作的例子上对模型进行了微调。人工标注员纠正 CoT 和动作，以（a）避免错误，以及（b）在错误发生后修复错误。

5 Finally, they fine-tuned the model using Direct Preference Optimization (DPO) to prefer generating the corrected examples over the erroneous examples from the previous step.

最后，他们使用直接偏好优化（Direct Preference Optimization，DPO）对模型进行微调，使其倾向于生成来自前一步骤的纠正后的例子，而不是错误的例子。

6 At inference, given a screenshot, an instruction, and potential actions (as is typical with open computer use models; the authors provide a handy list in a sample prompt), UI-TARS generated a CoT and an action to take. After taking that action (via PyAutoGUI, a Python module that controls computers), the model received a new screenshot and generated another chain of thought and action, and so on. At each step, the model produced a new chain of thought and action, taking into account the instruction and all CoTs, actions, and screenshots so far.

在推理时，给定一个屏幕截图、一个指令和潜在的动作（这在开放式计算机使用模型中很常见；作者在示例提示中提供了一个方便的列表），UI-TARS 会生成一个思维链（CoT）和要采取的动作。在采取该动作之后（通过 PyAutoGUI，一个控制计算机的 Python 模块），该模型接收到一个新的屏幕截图并生成另一个思维链（CoT）和动作，等等。在每一步，该模型都会产生一个新的思维链（CoT）和动作，同时考虑到指令和到目前为止的所有思维链（CoT）、动作和屏幕截图。

Behind the news: Adept touted computer use in early 2022, and OmniParser Aguvis soon followed with practical implementations. In October 2024, Anthropic set off the current wave of model/app interaction with its announcement of computer use for Claude 3.5 Sonnet. OpenAI recently responded with Operator, its own foray into using vision and language models to control computers.

背景信息：Adept 在 2022 年初就宣传了计算机的使用，OmniParser Aguvis 紧随其后进行了实际的实施。在 2024 年 10 月，Anthropic 通过宣布 Claude 3.5 Sonnet 可以控制电脑，掀起了当前的模型 / 应用交互浪潮。OpenAI 最近也推出了 Operator，尝试使用视觉和语言模型控制电脑。

Results: UI-TARS matched or outperformed Claude 3.5 Sonnet with computer use, GPT-4o with various computer use frameworks, and the Aguvis framework with its native model on 11 benchmarks. On OSWorld, which asks models to perform tasks using a variety of real-world applications and operating systems, UI-TARS successfully completed 22.7 percent of the tasks in 15 steps, whereas Claude 3.5 Sonnet with computer use completed 14.9 percent, GPT-4o with Aguvis 17 percent, and Aguvis with its native model 10.3 percent.

结果：在 11 项基准测试中，UI-TARS 在使用计算机的情况下，性能与 Claude 3.5 Sonnet 相当甚至更优，并且在使用不同计算机框架的 GPT-4o，以及使用其原生模型的 Aguvis 框架时，也表现出相似或更优的性能。在 OSWorld 测试中，该测试要求模型利用各种真实世界的应用程序和操作系统来完成任务，UI-TARS 成功完成了 15 个步骤中的 22.7% 的任务。相比之下，使用计算机的 Claude 3.5 Sonnet 完成了 14.9%，使用 Aguvis 框架的 GPT-4o 完成了 17%，而 Aguvis 框架使用其原生模型完成了 10.3%。

Why it matters: Training a model to take good actions enables it to perform well. Training it to correct its mistakes after making them enables it to recover from unexpected issues that may occur in the real world.

为什么重要：训练模型采取正确的行动能使其表现出色。训练模型在犯错后及时纠正，能使其从现实世界中可能发生的意外问题中快速恢复。

We’re thinking: Since computer use can be simulated in a virtual machine, it’s possible to generate massive amounts of training data automatically. This is bound to spur rapid progress in computer use by large language models.

我们是这样想的：既然计算机的使用可以在虚拟机中模拟，那么就能自动生成海量的训练数据。这肯定会大大推动大语言模型（Large Language Model，LLM）在计算机使用方面的快速发展。

#### Gemini Thinks Faster

Gemini 思考速度再创新高

Line charts showing performance improvements in math and science with 2.0 Flash Thinking models.

图表显示，采用 2.0 Flash Thinking 模型后，数学和科学方面的性能显著提升。

Google updated the December-vintage reasoning model Gemini 2.0 Flash Thinking and other Flash models, gaining ground on OpenAI o1 and DeepSeek-R1.

Google 近期升级了 12 月份推出的推理模型 Gemini 2.0 Flash Thinking 及其他 Flash 模型，在性能上超越了 OpenAI o1 和 DeepSeek-R1。

What’s new: Gemini 2.0 Flash Thinking Experimental 1-21 is a vision-language model (images and text in, text out) that’s trained to generate a structured reasoning process or chain of thought. The new version improves on its predecessor’s reasoning capability and extends its context window. It's free to access via API while it remains designated “experimental” and available to paid users of the Gemini app, along with Gemini 2.0 Flash (fresh out of experimental mode) and the newly released Gemini 2.0 Pro Experimental. The company also launched a preview of Gemini 2.0 Flash Lite, a vision-language model (images and text in, text out) that outperforms Gemini 1.5 Flash at the same price.

最新动态：Gemini 2.0 Flash Thinking Experimental 1-21 是一个视觉语言模型（Vision-Language Model），可以接收图像和文本输入，并输出文本。它经过专门训练，能够生成结构化的推理过程，也就是「思维链（Chain of Thought）」。新版本在其前代产品的基础上，提升了推理能力，并扩大了上下文窗口。目前，该模型仍处于「实验性」阶段，可以通过 API 免费访问。Gemini 应用的付费用户也可以使用该模型，以及刚刚结束实验阶段的 Gemini 2.0 Flash 和新发布的 Gemini 2.0 Pro Experimental。此外，该公司还发布了 Gemini 2.0 Flash Lite 的预览版，这同样是一个视觉语言模型，在相同价格下，其性能优于 Gemini 1.5 Flash。

How it works: Gemini 2.0 Flash Thinking Experimental 1-21 is based on Gemini 2.0 Flash Experimental (parameter count undisclosed). It processes up to 1 million tokens of input context, compared to its predecessor’s 32,000 and o1’s 128,000.

工作原理：Gemini 2.0 Flash Thinking Experimental 1-21 基于 Gemini 2.0 Flash Experimental 构建（参数数量未披露）。与前代模型的 32,000 个 Token 和 o1 模型的 128,000 个 Token 相比，它能够处理高达 100 万个 Token 的输入上下文。

1 Unlike o1, which hides its chain of thought, but like DeepSeek-R1 and Qwen QwQ, Gemini 2.0 Flash Thinking Experimental 1-21 includes its reasoning in its output.

与隐藏其思维链的 o1 不同，但与 DeepSeek-R1 和 Qwen QwQ 类似，Gemini 2.0 Flash Thinking Experimental 1-21 在其输出中包含了它的推理过程。

2 On the graduate-level science exam GPQA-Diamond, it achieved 74.2 percent compared to the earlier version’s 58.6 percent, surpassing DeepSeek-R1 (71.5 percent) but behind o1 (77.3 percent).

在研究生级别的科学考试 GPQA-Diamond 上，它的成绩为 74.2%，高于早期版本的 58.6%，超过了 DeepSeek-R1（71.5%），但低于 o1（77.3%）。

3 On the advanced math benchmark AIME 2024, it achieved 73.3 percent compared to the previous version’s 35.5 percent, but it trails behind DeepSeek-R1 (79.8 percent) and o1 (74.4 percent).

在高级数学基准 AIME 2024 上，它的成绩为 73.3%，高于之前版本的 35.5%，但落后于 DeepSeek-R1（79.8%）和 o1（74.4%）。

4 On the visual and multimedia understanding test MMMU, it achieved 75.4 percent to outperform the previous version (70.7 percent) but fell short of o1 (78.2 percent).

在视觉和多媒体理解测试 MMMU 上，它的成绩为 75.4%，优于之前版本的 70.7%，但低于 o1（78.2%）。

5 Developers can integrate Python code execution via the API, with support for data analysis and visualization through pre-installed libraries.

开发者可以通过 API 集成 Python 代码执行，并通过预安装的库支持数据分析和可视化。

Speed bumps: Large language models that are trained to generate a chain of thought (CoT) are boosting accuracy even as the additional processing increases inference costs and latency. Reliable measures of Gemini 2.0 Flash Thinking Experimental 1-21’s speed are not yet available, but its base model runs faster (168.8 tokens per second with 0.46 seconds of latency to the first token, according to Artificial Analysis) than all models in its class except o1-mini (which outputs 200 tokens per second with 10.59 seconds of latency to the first token).

速度瓶颈：经过训练以生成思维链（Chain of Thought，CoT）的大语言模型（LLM/Large Language Model）正在提高准确性，但额外的处理也增加了推理成本和延迟。Gemini 2.0 Flash Thinking Experimental 1-21 的速度的可靠指标尚未公布，但根据 Artificial Analysis 的数据，其基础模型的运行速度更快（每秒 168.8 个 Token（Token），首次 Token 的延迟为 0.46 秒），仅次于 o1-mini（每秒输出 200 个 Token，首次 Token 的延迟为 10.59 秒），在同类模型中排名第二。

Why it matters: The combination of CoT reasoning and long context — assuming the new model can take advantage of its 1 million-token context window, as measured by a benchmark such as RULER — could open up valuable applications. Imagine a reasoning model that can take an entire codebase as input and analyze it without breaking it into smaller chunks.

重要性：如果「思维链（Chain-of-Thought，CoT）推理」与长上下文相结合 —— 假设新模型能够充分利用其高达 100 万 Token 的上下文窗口（通过 RULER 等基准测试衡量）—— 将会开启极具价值的应用场景。试想一下，一个推理模型可以直接将整个代码库作为输入进行分析，而无需将其拆分成更小的代码片段。

We’re thinking: Regardless of benchmark performance, this model topped the Chatbot Arena leaderboard at the time of writing. This suggests that users preferred it over o1 and DeepSeek-R1 — at least for common, everyday prompts.

我们的思考：撇开基准测试的性能不谈，这款模型在本文撰写之时，一直稳居 Chatbot Arena 排行榜的首位。这表明，至少在处理常见的日常问题时，用户更倾向于选择这款模型，而不是 o1 和 DeepSeek-R1。

#### Okay, But Please Don’t Stop Talking

好的，请继续即使是像 ChatGPT

Diagram illustrating Moshi’s use of an LLM to process user audio input, inner monologue, and output.

图表展示了 Moshi 如何使用大语言模型（LLM/Large Language Model）处理用户音频输入、内心独白并生成输出。

Even cutting-edge, end-to-end, speech-to-speech systems like ChatGPT’s Advanced Voice Mode tend to get interrupted by interjections like “I see” and “uh-huh” that keep human conversations going. Researchers built an open alternative that’s designed to go with the flow of overlapping speech.

即使高级语音模式这样先进的端到端语音转语音系统，也常常会被「我明白了」和「嗯哼」这样的插话打断，而这些插话能维持人类对话的进行。研究人员开发了一种开放的替代方案，旨在更好地处理重叠语音。

What’s new: Alexandre Défossez, Laurent Mazaré, and colleagues at Kyutai, a nonprofit research lab in Paris, released Moshi, an end-to-end, speech-to-speech system that’s always listening and always responding. The weights and code are free for noncommercial and commercial uses under CC-BY 4.0, Apache 2.0, and MIT licenses. You can try a web demo here.

新发现：位于巴黎的非营利研究实验室 Kyutai 的 Alexandre Défossez、Laurent Mazaré 及其同事发布了 Moshi，这是一个端到端的语音交互系统，能够持续监听并响应。该项目的权重和代码基于 CC-BY 4.0、Apache 2.0 和 MIT 许可，可免费用于非商业和商业用途。您可以在以下链接体验网络演示。

Key insight: Up to 20 percent of spoken conversation consists of overlapping speech, including interjections like “okay” and “I see.”

核心发现：在日常口语对话中，高达 20% 的部分包含语音重叠现象，例如穿插着「好的」、「我明白了」之类的词语。

1 To respond appropriately despite such overlaps, a system must both listen and generate sound continuously — although much of what it will generate is silence.

为了在这种语音重叠的情况下做出合适的反应，系统必须持续地监听并生成声音 —— 即便生成的大部分内容是静默。

2 To respond without delay, it must keep latency to a minimum. This goal requires an end-to-end design rather than a pipeline of stand-alone models to perform voice detection, speech-to-text, text processing, and text-to-speech in turn.

为了实现无延迟响应，系统必须最大限度地降低延迟。为了达成这个目标，需要采用端到端的设计，而不是使用独立的模型管线来依次完成语音检测、语音转文本、文本处理和文本转语音等任务。

How it works: The authors combined an encoder-decoder called Mimi and an RQ-Transformer, which is made up of the Helium transformer-based large language model (LLM) plus another transformer.

工作原理：研究者们将一个名为 Mimi 的编码器 - 解码器与一个 RQ-Transformer 结合使用。RQ-Transformer 由 Helium Transformer 架构的大语言模型（LLM）以及另一个 Transformer 模型构成。

1 Mimi’s encoder embedded spoken input using 8 audio tokens per timestep (80 milliseconds). The authors trained Mimi on 7 million hours of mostly English speech from undisclosed sources. The training involved two loss terms. (i) The first loss term encouraged Mimi, given one audio timestep, to produce audio that fooled a pretrained MS-STFT discriminator into thinking it was human speech. The second loss term distilled knowledge from a pretrained WavLM, an audio embedding model. It encouraged Mimi’s encoder, when Mimi and WavLM received the same audio timestep, to produce one audio token (of its 8 audio tokens per timestep) whose embedding was similar to the corresponding embedding produced by WavLM.

Mimi 的编码器将语音输入嵌入到每时间步 8 个音频 Token（Token）中，每个时间步长为 80 毫秒。作者使用了来自未公开来源的 700 万小时的语音数据来训练 Mimi，这些数据主要为英语语音。训练涉及两个损失项。(i）第一个损失项旨在鼓励 Mimi 在给定一个音频时间步时，生成尽可能逼真，能够使预训练的 MS-STFT 判别器误以为是人类语音的音频。(ii）第二个损失项利用知识蒸馏技术，从预训练的 WavLM（一种音频嵌入模型）中提取知识。具体来说，当 Mimi 和 WavLM 接收到相同的音频时间步时，该损失项会鼓励 Mimi 的编码器生成一个音频 Token（在每时间步的 8 个音频 Token 中选择一个），使其嵌入向量与 WavLM 产生的相应嵌入向量尽可能相似。

2 Given the audio tokens, the Helium LLM produced text tokens that were used internally to help the additional transformer predict the next audio token (the idea being that the LLM’s skill with words would inform which audio token to generate next). The authors trained Helium to predict the next text token in 2.1 trillion tokens of English text (12.5 percent from Wikipedia and Stack Exchange, and the remaining 87.5 percent from Common Crawl).

给定音频 Token，Helium 大语言模型（LLM/Large Language Model）生成文本 Token，这些 Token 在内部被用于辅助额外的 Transformer 预测下一个音频 Token（Transformer 期望借助 LLM 在文本方面的能力，来辅助判断接下来应该生成哪个音频 Token）。作者使用包含 2.1 万亿个 Token 的英语文本数据集训练 Helium，使其能够预测下一个文本 Token（数据集中，12.5% 来自 Wikipedia 和 Stack Exchange，剩余 87.5% 来自 Common Crawl）。

3 RQ-Transformer received many sets of 17 tokens per time step: 8 audio tokens encoded by Mimi from the audio input, 8 audio tokens from Moshi’s previously generated audio output, and 1 text token produced by Helium. RQ-Transformer learned to predict the next set of 17 tokens in 7 million hours of audio and transcribed text.

RQ-Transformer 接收到的输入是多组每时间步 17 个 Token 的组合：其中包括 Mimi 从音频输入编码得到的 8 个音频 Token，来自 Moshi 先前生成的音频输出的 8 个音频 Token，以及 Helium 产生的 1 个文本 Token。RQ-Transformer 学会了预测 700 万小时的音频和转录文本中的下一组 17 个 Token。

4 To train the system specifically on conversational interaction, the authors further trained it to predict the next token in 2,000 hours of recorded phone conversations between randomly paired participants.

为了使系统更好地处理会话交互，作者还使用了 2,000 小时的电话录音数据对其进行训练，这些录音数据来自随机配对的参与者之间的对话，训练目标是预测对话中的下一个 Token。

5 At inference, given a user's speech, Mimi turned it into audio tokens. Given the audio tokens and RQ-Transformer’s previously generated audio and text tokens, RQ-Transformer generated new audio and text tokens. From the generated audio tokens, Mimi produced synthetic speech.

在推理阶段，给定用户的语音，Mimi 首先将其转换为音频 Token。然后，RQ-Transformer 结合 Mimi 产生的音频 Token 以及自身先前生成的音频和文本 Token，来生成新的音频和文本 Token。最后，Mimi 将 RQ-Transformer 生成的音频 Token 转换为合成语音。

Results: In tests, Moshi proved fast and relatively accurate.

实验结果表明，Moshi 具有较高的语音合成速度和相对准确性。

1 Moshi (7 billion parameters) took around 200 milliseconds to respond to user input. In comparison, GPT-4o, which also produces speech output directly from speech input, took 232 milliseconds minimum (320 milliseconds average). Prior to GPT-4o, ChatGPT Voice Mode (a pipeline of speech-to-text, text-to-text, and text-to-speech models) took an average of 5.4 seconds.

Moshi（拥有 70 亿参数）响应用户输入大约需要 200 毫秒。相比之下，GPT-4o 同样可以直接从语音输入生成语音输出，但至少需要 232 毫秒（平均 320 毫秒）。在 GPT-4o 之前，ChatGPT 语音模式（采用语音转文本、文本转文本和文本转语音模型的流水线）平均需要 5.4 秒。

2 Moshi achieved 26.6 percent accuracy on Web Questions, higher than the speech-to-text-to-speech models tested by the authors: Spectron (1 billion parameters) achieved 6.1 percent accuracy and SpeechGPT (7 billion parameters) achieved 6.5 percent accuracy. The authors didn’t provide comparable results for GPT-4o or ChatGPT Voice.

Moshi 在 Web Questions 数据集上取得了 26.6% 的准确率，高于作者测试的语音转文本转语音模型：Spectron（拥有 10 亿参数）的准确率为 6.1%，而 SpeechGPT（拥有 70 亿参数）的准确率为 6.5%。作者没有提供 GPT-4o 或 ChatGPT 语音的可比结果。

Why it matters: While a turn-based approach may suffice for text input, voice-to-voice interactions benefit from a system that processes both input and output quickly and continuously. Previous systems process input and output separately, making users wait. Moshi delivers seamless interactivity.

重要性：虽然基于轮询的方法可能足以应付文本输入，但语音交互需要一个能够快速、持续处理输入和输出的系统。以往的系统分别处理输入和输出，用户不得不等待结果。而 Moshi 提供了无缝的交互体验。

We’re thinking: Generating silence is golden!

我们的想法是：此时无声胜有声！